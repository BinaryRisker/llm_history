---
concept_id: positional-encoding
concept_name_zh: 位置编码
concept_name_en: Positional Encoding
category: architecture
difficulty: intermediate
introduced: 2017
paper: "Attention is All You Need (Vaswani et al., 2017)"
related_concepts: [self-attention, transformer, sinusoidal-encoding]
---

# 位置编码 (Positional Encoding)

## 什么是位置编码？

**位置编码 (Positional Encoding)** 是一种给序列中每个位置添加"位置信息"的技术，让模型知道词语在句子中的顺序。

**核心问题**: Self-Attention本身没有位置概念，"我爱你"和"你爱我"看起来一样。

### 类比理解

想象一个失忆的读者阅读打乱顺序的句子：

**没有位置编码**:
```
单词袋: [猫, 吃, 老鼠]
问题: 是"猫吃老鼠"还是"老鼠吃猫"？无法区分！
```

**有位置编码**:
```
位置1: 猫
位置2: 吃
位置3: 老鼠
结果: 明确知道是"猫吃老鼠"
```

### 实际例子

**句子**: "我不喜欢这个电影"

**传统RNN处理**:
- 自带顺序: 我 → 不 → 喜欢 → 这个 → 电影
- 位置信息隐含在处理顺序中

**Self-Attention处理**:
- 同时看所有词，没有顺序
- **问题**: "不喜欢" vs "喜欢不" 无法区分
- **解决**: 给每个词加位置标记
  ```
  我[位置1] 不[位置2] 喜欢[位置3] 这个[位置4] 电影[位置5]
  ```

---

## 为什么需要位置编码？

### Self-Attention的"位置盲"问题

**核心矛盾**:
- Self-Attention的优势: 并行计算，任意位置直接关联
- Self-Attention的代价: 丢失了词语的顺序信息

**问题示例**:
```
原句: "狗咬了人"
打乱: "人咬了狗"

在纯Self-Attention看来:
- 两个句子包含相同的词: {狗, 咬, 了, 人}
- 注意力权重计算: Query·Key^T 只看内容，不看位置
- 结果: 无法区分谁咬谁！
```

### 位置为什么重要？

**语言中的位置敏感性**:

1. **语法结构**:
   - "我喜欢你" vs "你喜欢我" (主语和宾语)
   - "not good" vs "good not" (否定词位置)

2. **语义关系**:
   - "昨天的会议" vs "会议的昨天" (修饰关系)
   - "因为A所以B" vs "所以B因为A" (因果顺序)

3. **远距离依赖**:
   - "The keys **that** were on the table **are** gone"
   - **that** 和 **are** 相隔5个词，但语法关联强

---

## 如何工作？（Transformer中的方法）

### 方法1: 正弦余弦位置编码 (Sinusoidal Encoding)

Transformer原论文使用的方法，数学优雅且不需要学习。

**核心思想**: 用不同频率的波形表示不同位置

```
想象钟表:
- 秒针: 快速转动 (高频) → 表示细微位置差异
- 分针: 中速转动 (中频) → 表示中等位置差异
- 时针: 慢速转动 (低频) → 表示大范围位置差异

位置编码: 用多个频率的正弦/余弦波组合
→ 每个位置得到独一无二的"波形指纹"
```

**公式直觉** (无需深究数学):
```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

解读:
- pos: 位置 (0, 1, 2, 3, ...)
- i: 维度索引
- 不同维度使用不同频率的波
- 结果: 每个位置获得一个512维的向量
```

**优势**:
- **确定性**: 不需要训练，直接计算
- **外推性**: 可以处理比训练时更长的序列
- **相对位置**: 任意两个位置的编码差异反映距离

### 方法2: 可学习位置编码 (Learned Positional Embedding)

BERT等模型使用的方法，位置编码作为参数学习。

```
类比词嵌入 (Word Embedding):
- 每个词有一个可学习的向量
- 每个位置也有一个可学习的向量

位置0: [0.23, -0.45, 0.67, ...]
位置1: [-0.12, 0.34, -0.56, ...]
位置2: [0.78, 0.23, -0.34, ...]
...
```

**优势**:
- **灵活性**: 模型自己学习最优位置表示
- **任务适应**: 针对特定任务优化位置编码

**劣势**:
- **长度限制**: 只能处理训练时见过的最大长度
- **需要训练**: 增加参数量和训练成本

### 位置编码的使用方式

**加法融合** (Transformer使用):
```
输入词向量:     [0.5, 0.2, -0.3, 0.8]
位置编码:       [0.1, -0.1, 0.2, -0.2]
最终表示:       [0.6, 0.1, -0.1, 0.6]  (相加)

优势: 简单高效，保持维度不变
```

**拼接融合** (某些模型使用):
```
输入词向量:     [0.5, 0.2, -0.3, 0.8]
位置编码:       [0.1, -0.1]
最终表示:       [0.5, 0.2, -0.3, 0.8, 0.1, -0.1]  (拼接)

优势: 位置信息独立，不干扰词向量
劣势: 维度增加，计算成本上升
```

---

## 历史发展

### 2017年之前: 序列模型的天然位置

**RNN/LSTM时代**:
- 位置信息隐含在时间步顺序中
- 无需显式位置编码
- 问题: 顺序处理，无法并行

### 2017年: Transformer的位置编码创新

**2017-06**: "Attention is All You Need" 论文
- **挑战**: 完全抛弃RNN，如何保留位置信息？
- **解决**: 正弦余弦位置编码
- **创新**:
  - 确定性函数，无需学习
  - 支持外推到更长序列
  - 相对位置信息蕴含其中

### 2018-2019: 可学习位置编码流行

**BERT (2018-10)**:
- 使用可学习位置编码
- 最大长度512限制
- 任务特定优化

**GPT-2 (2019-02)**:
- 可学习位置编码
- 最大长度1024

### 2019-2020: 相对位置编码兴起

**问题**: 绝对位置编码的局限
- "第3个词"的表示在长短句子中含义不同
- 无法泛化到未见过的序列长度

**Transformer-XL (2019)**:
- 相对位置编码
- 只关心词与词之间的距离，不关心绝对位置
- 更好的长序列泛化

**T5 (2019)**:
- 相对位置偏置 (Relative Position Bias)
- 简化的相对位置表示

### 2020-至今: 长上下文与位置编码创新

**挑战**: 如何处理超长上下文（10K, 100K, 1M tokens）？

**ALiBi (2021)**:
- Attention with Linear Biases
- 不添加位置编码，而是在注意力计算中加入位置偏置
- 训练时见2K，推理时可用到11K

**RoPE (2021)**: Rotary Position Embedding
- 旋转位置编码
- GPT-NeoX, PaLM, LLaMA等模型采用
- 相对位置信息，外推性好

**2023-2024**: 极长上下文
- Claude 100K → 200K context
- Gemini 1.5 Pro 1M context
- 位置编码成为长上下文的关键瓶颈

---

## 优势与局限

### ✅ 优势

1. **恢复位置信息**: 让Self-Attention能区分词序
2. **并行计算**: 不像RNN需要顺序处理
3. **灵活性**: 多种方法适应不同需求
4. **可解释性**: 正弦编码的数学优雅性

### ⚠️ 局限

1. **长度限制** (可学习编码):
   - 只能处理训练时见过的长度
   - 外推到更长序列效果差

2. **绝对vs相对位置权衡**:
   - 绝对位置: 简单但泛化性差
   - 相对位置: 复杂但泛化性好

3. **计算开销**:
   - 增加模型参数量（可学习编码）
   - 增加计算复杂度（复杂相对编码）

4. **设计空间大**:
   - 没有"最优"方法，需要实验选择
   - 不同任务可能需要不同编码

---

## 位置编码方法对比

| 方法 | 代表模型 | 优势 | 劣势 |
|------|----------|------|------|
| **正弦余弦** | Transformer | 确定性，外推性好 | 表达能力有限 |
| **可学习绝对** | BERT, GPT-2 | 灵活，任务适应 | 长度限制，泛化差 |
| **相对位置** | Transformer-XL | 泛化性好 | 计算复杂度高 |
| **RoPE** | LLaMA, PaLM | 相对位置，外推好 | 实现复杂 |
| **ALiBi** | BLOOM | 超长外推 | 注意力计算修改 |

---

## 相关概念

1. **Self-Attention (自注意力)**: 需要位置编码来补充位置信息
2. **Transformer**: 首次系统性使用位置编码的架构
3. **Sinusoidal Encoding (正弦编码)**: 经典位置编码方法
4. **Relative Position (相对位置)**: 关注距离而非绝对位置
5. **Long Context (长上下文)**: 位置编码是长上下文的关键技术

---

## 实际应用

### 1. 机器翻译

**位置的重要性**:
```
英文: "I do not like this movie"
中文: "我不喜欢这个电影"

位置编码确保:
- "not" (否定词) 的位置正确理解
- 语序差异正确转换
```

### 2. 文本生成

**连贯性保证**:
- 生成时保持语法正确
- 代词指代关系正确
- 时间顺序逻辑合理

### 3. 长文档理解

**远距离依赖**:
```
文档开头: "2023年报告显示..."
文档末尾: "...相比去年增长50%"

位置编码帮助模型:
- 记住"2023年"在开头
- 理解"去年"指2022年
- 关联首尾信息
```

---

## 参考资料

**核心论文**:
- **[vaswani2017]** Vaswani, A., et al. (2017). Attention is All You Need. *NeurIPS 2017*. https://arxiv.org/abs/1706.03762
- **[devlin2018]** Devlin, J., et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers. https://arxiv.org/abs/1810.04805
- **[su2021]** Su, J., et al. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. https://arxiv.org/abs/2104.09864

**进阶阅读**:
- Press, O., et al. (2021). Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. https://arxiv.org/abs/2108.12409 (ALiBi)
- Dai, Z., et al. (2019). Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. https://arxiv.org/abs/1901.02860

---

**概念卡片版本**: 1.0
**创建日期**: 2025-10-17
**最后更新**: 2025-10-17
**维护者**: LLM History Chronicle Project
