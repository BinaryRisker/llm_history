---
event_id: transformer-2017-06
date: 2017-06-12
title: Transformerè®ºæ–‡å‘è¡¨ (Transformer Paper Publication)
title_en: "Attention is All You Need"
organization: Google Brain
event_type: research_publication
significance_level: critical
verification_status: verified
sources:
  - vaswani2017
tags:
  - transformer
  - architecture
  - foundation
  - attention-mechanism
causal_connections:
  - enables: [gpt1-2018-06, bert-2018-10]
  - foundation_for: all_subsequent_llms
technical_concepts:
  - self-attention
  - multi-head-attention
  - positional-encoding
  - transformer-architecture
chapter_id: 01-foundation
---

# Transformerè®ºæ–‡å‘è¡¨ (2017-06-12)

**ğŸ”´ å…³é”®é‡Œç¨‹ç¢‘** | **Critical Milestone**

## äº‹ä»¶æ¦‚è¿° (Event Overview)

2017å¹´6æœˆ12æ—¥ï¼ŒGoogle Brainå›¢é˜Ÿå‘è¡¨äº†é©å‘½æ€§è®ºæ–‡ã€ŠAttention is All You Needã€‹(Vaswani et al., 2017)ï¼Œæå‡ºäº†Transformeræ¶æ„ã€‚è¿™ä¸€åˆ›æ–°å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†(NLP)é¢†åŸŸï¼Œä¸ºæ‰€æœ‰åç»­å¤§è¯­è¨€æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚

On June 12, 2017, the Google Brain team published the revolutionary paper "Attention is All You Need" (Vaswani et al., 2017), introducing the Transformer architecture. This innovation fundamentally transformed the field of Natural Language Processing (NLP) and laid the foundation for all subsequent large language models.

## æŠ€æœ¯åˆ›æ–° (Technical Innovation)

### æ ¸å¿ƒçªç ´ (Core Breakthrough)

**Self-Attentionæœºåˆ¶ (Self-Attention Mechanism)**:
- å®Œå…¨æŠ›å¼ƒäº†ä¼ ç»Ÿçš„RNN/LSTMæ¶æ„
- é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶æ•æ‰åºåˆ—ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»
- å…è®¸å¹¶è¡ŒåŒ–è®­ç»ƒï¼Œå¤§å¹…æå‡è®­ç»ƒæ•ˆç‡

**å…³é”®æŠ€æœ¯ç»„ä»¶ (Key Technical Components)**:
1. **Multi-head Self-Attention (å¤šå¤´è‡ªæ³¨æ„åŠ›)**
   - å…è®¸æ¨¡å‹ä»å¤šä¸ªè¡¨ç¤ºå­ç©ºé—´å­¦ä¹ ä¿¡æ¯
   - æ•æ‰ä¸åŒç±»å‹çš„ä¾èµ–å…³ç³»

2. **Positional Encoding (ä½ç½®ç¼–ç )**
   - ä¸ºåºåˆ—ä¸­çš„ä½ç½®ä¿¡æ¯ç¼–ç 
   - å¼¥è¡¥Transformeræ— åºåˆ—ç»“æ„çš„ç¼ºé™·

3. **Layer Normalization (å±‚å½’ä¸€åŒ–)**
   - ç¨³å®šè®­ç»ƒè¿‡ç¨‹
   - åŠ é€Ÿæ”¶æ•›

4. **Residual Connections (æ®‹å·®è¿æ¥)**
   - ç¼“è§£æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
   - å…è®¸ä¿¡æ¯ç›´æ¥æµåŠ¨

### ä¸ºä»€ä¹ˆé©å‘½æ€§ (Why Revolutionary)

**ğŸš€ å¹¶è¡ŒåŒ–è®­ç»ƒèƒ½åŠ›**:
- vs RNNçš„åºåˆ—åŒ–å¤„ç†: Transformerå¯ä»¥å¹¶è¡Œå¤„ç†æ•´ä¸ªåºåˆ—
- è®­ç»ƒé€Ÿåº¦æå‡10-100å€
- ä½¿è¶…å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒæˆä¸ºå¯èƒ½

**ğŸ”— é•¿è·ç¦»ä¾èµ–å»ºæ¨¡**:
- ä¸å—RNNçš„æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é™åˆ¶
- ä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„è·¯å¾„é•¿åº¦ä¸ºå¸¸æ•°
- æ›´å¥½åœ°æ•æ‰æ–‡æœ¬ä¸­çš„é•¿è·ç¦»å…³ç³»

**ğŸ“ˆ å¯æ‰©å±•æ€§**:
- é€‚åˆè¶…å¤§è§„æ¨¡å‚æ•°æ‰©å±•
- ä¸ºGPT-3 (175B)ã€GPT-4ç­‰å¥ å®šåŸºç¡€
- è¯æ˜äº†"è§„æ¨¡åŒ–"è·¯å¾„çš„å¯è¡Œæ€§

## å†å²å½±å“ (Historical Impact)

### ç›´æ¥å½±å“ (Direct Impact)

**æ¶æ„åŸºç¡€ (Architecture Foundation)**:
- **GPTç³»åˆ—** (2018-): åŸºäºTransformer decoder
- **BERT** (2018): åŸºäºTransformer encoder
- **T5** (2019): å®Œæ•´çš„encoder-decoderæ¶æ„
- **æ‰€æœ‰ç°ä»£LLM**: éƒ½åŸºäºæˆ–è¡ç”Ÿè‡ªTransformer

### é•¿æœŸå½±å“ (Long-term Impact)

**å­¦æœ¯å½±å“ (Academic Impact)**:
- æˆªè‡³2025å¹´ï¼Œè®ºæ–‡å¼•ç”¨è¶…è¿‡10ä¸‡æ¬¡
- æˆä¸ºNLPé¢†åŸŸæœ€å…·å½±å“åŠ›çš„è®ºæ–‡ä¹‹ä¸€
- å¼•å‘äº†Transformerå˜ä½“çš„ç ”ç©¶æµªæ½® (Sparse Transformer, Reformer, etc.)

**äº§ä¸šå½±å“ (Industry Impact)**:
- æ¨åŠ¨äº†æ•´ä¸ªAIäº§ä¸šçš„å‘å±•
- ä½¿ChatGPTç­‰åº”ç”¨æˆä¸ºå¯èƒ½
- æ”¹å˜äº†ç§‘æŠ€å·¨å¤´çš„AIç ”å‘æ–¹å‘

## å› æœå…³ç³» (Causal Connections)

### å¯ç”¨çš„åç»­å‘å±• (Enabled Developments)

**2018å¹´**:
- **GPT-1** (2018-06): é¦–æ¬¡å°†Transformer decoderåº”ç”¨äºè¯­è¨€å»ºæ¨¡
- **BERT** (2018-10): é¦–æ¬¡å°†Transformer encoderåº”ç”¨äºåŒå‘é¢„è®­ç»ƒ

**2019-2020å¹´**:
- **GPT-2** (2019): è§„æ¨¡åŒ–éªŒè¯
- **GPT-3** (2020): 1750äº¿å‚æ•°ï¼ŒFew-shot learning
- **T5** (2019): ç»Ÿä¸€çš„text-to-textæ¡†æ¶

**2021-2025å¹´**:
- æ‰€æœ‰ä¸»æµLLMéƒ½åŸºäºTransformeræ¶æ„
- å¤šæ¨¡æ€æ¨¡å‹ (CLIP, GPT-4V, Gemini) æ‰©å±•äº†Transformeråˆ°è§†è§‰é¢†åŸŸ
- MoEæ¶æ„ (DeepSeek-V3) åœ¨TransformeråŸºç¡€ä¸Šå®ç°ç¨€ç–æ¿€æ´»

## æŠ€æœ¯ç»†èŠ‚ (Technical Details)

### è®ºæ–‡è´¡çŒ® (Paper Contributions)

**æ¨¡å‹è§„æ¨¡ (Model Size)**:
- Base model: 65Må‚æ•°
- Big model: 213Må‚æ•°
- åœ¨å½“æ—¶å±äºä¸­ç­‰è§„æ¨¡ï¼Œä½†éªŒè¯äº†æ¶æ„çš„æœ‰æ•ˆæ€§

**æ€§èƒ½çªç ´ (Performance Breakthrough)**:
- åœ¨WMT 2014 English-Germanç¿»è¯‘ä»»åŠ¡ä¸Šè¾¾åˆ°SOTA
- BLEU score: 28.4 (è¶…è¶Šä¹‹å‰çš„æœ€ä½³ç»“æœ)
- è®­ç»ƒæ—¶é—´å¤§å¹…å‡å°‘

### è®­ç»ƒæ•ˆç‡ (Training Efficiency)

**è®¡ç®—å¤æ‚åº¦**:
- Self-attention: O(nÂ²Â·d)ï¼Œå…¶ä¸­nä¸ºåºåˆ—é•¿åº¦ï¼Œdä¸ºç»´åº¦
- å¯¹äºçŸ­åºåˆ—ï¼Œæ¯”RNNçš„O(nÂ·dÂ²)æ›´é«˜æ•ˆ
- å¯ä»¥é€šè¿‡å¹¶è¡ŒåŒ–å……åˆ†åˆ©ç”¨GPU

## éªŒè¯æ¥æº (Verification Sources)

**å­¦æœ¯è®ºæ–‡**:
- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., & Polosukhin, I. (2017). Attention is All You Need. *Advances in Neural Information Processing Systems (NeurIPS) 30*. https://arxiv.org/abs/1706.03762

**å¼•ç”¨ç»Ÿè®¡**:
- Google Scholar: 100,000+ citations (æˆªè‡³2025)
- arXiv: 10,000+ citations

## ç›¸å…³æ¦‚å¿µ (Related Concepts)

- [Self-Attention](../../concepts/self-attention.md)
- [Multi-head Attention](../../concepts/multi-head-attention.md)
- [Positional Encoding](../../concepts/positional-encoding.md)
- [Transformer Architecture](../../concepts/transformer-architecture.md)

## ç›¸å…³ç« èŠ‚ (Related Chapters)

- [Chapter 1: Transformeré©å‘½](../../../manuscript/01-foundation/transformer-revolution.md)
- [Chapter 2: æ—©æœŸåº”ç”¨](../../../manuscript/01-foundation/early-applications.md)

## æ—¶é—´çº¿ä½ç½® (Timeline Position)

**å‰ç½®äº‹ä»¶**:
- æ—  (è¿™æ˜¯ç°ä»£LLMæ—¶ä»£çš„èµ·ç‚¹)

**åç»­äº‹ä»¶**:
- [GPT-1å‘å¸ƒ](gpt1-release-2018.md) (2018-06)
- [BERTå‘å¸ƒ](bert-release-2018.md) (2018-10)

---

**Event Card Version**: 1.0
**Created**: 2025-10-17
**Last Updated**: 2025-10-17
**Verification Status**: âœ… Verified from academic sources
