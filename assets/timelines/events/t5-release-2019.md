---
event_id: t5-2019-10
date: 2019-10-23
title: T5å‘å¸ƒ (T5 Release)
title_en: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
organization: Google
event_type: model_release
significance_level: major
verification_status: verified
sources:
  - raffel2020t5
tags:
  - t5
  - text-to-text
  - unified-framework
  - encoder-decoder
causal_connections:
  - enabled_by: [transformer-2017-06, bert-2018-10]
  - demonstrates: unified_text_to_text_framework
  - influences: [bart-2019, mT5-2020]
technical_concepts:
  - text-to-text-framework
  - encoder-decoder-transformer
  - transfer-learning
  - c4-dataset
chapter_id: 02-gpt-era
---

# T5å‘å¸ƒ (2019-10-23)

**ğŸ”µ é‡è¦é‡Œç¨‹ç¢‘** | **Major Milestone**

## äº‹ä»¶æ¦‚è¿° (Event Overview)

2019å¹´10æœˆ23æ—¥ï¼ŒGoogleå‘å¸ƒè®ºæ–‡ã€ŠExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformerã€‹(Raffel et al., 2020)ï¼Œæå‡ºT5 (Text-to-Text Transfer Transformer) æ¨¡å‹ã€‚è¿™æ˜¯é¦–æ¬¡æˆåŠŸå°†æ‰€æœ‰NLPä»»åŠ¡ç»Ÿä¸€ä¸ºtext-to-textæ ¼å¼ï¼Œç®€åŒ–äº†æ¨¡å‹è®¾è®¡ï¼Œä¸ºå¤šä»»åŠ¡å­¦ä¹ æä¾›äº†ç»Ÿä¸€æ¡†æ¶ã€‚

On October 23, 2019, Google published the paper "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" (Raffel et al., 2020), introducing T5. This was the first successful unification of all NLP tasks into a text-to-text format, simplifying model design and providing a unified framework for multi-task learning.

## æŠ€æœ¯åˆ›æ–° (Technical Innovation)

### ç»Ÿä¸€æ¡†æ¶ (Unified Framework)

**Text-to-Textæ ¸å¿ƒæ€æƒ³**:
- **æ‰€æœ‰ä»»åŠ¡éƒ½æ˜¯æ–‡æœ¬ç”Ÿæˆ**: è¾“å…¥æ–‡æœ¬ â†’ è¾“å‡ºæ–‡æœ¬
- **æ— éœ€ä»»åŠ¡ç‰¹å®šæ¶æ„**: åŒä¸€æ¨¡å‹å¤„ç†æ‰€æœ‰ä»»åŠ¡
- **ç®€åŒ–è®­ç»ƒå’Œæ¨ç†**: ç»Ÿä¸€çš„losså‡½æ•°å’Œè¾“å‡ºæ ¼å¼

**ä»»åŠ¡è½¬æ¢ç¤ºä¾‹**:

**ç¿»è¯‘ä»»åŠ¡**:
```
Input:  "translate English to German: That is good."
Output: "Das ist gut."
```

**åˆ†ç±»ä»»åŠ¡**:
```
Input:  "cola sentence: The course is jumping well."
Output: "not acceptable"
```

**æ‘˜è¦ä»»åŠ¡**:
```
Input:  "summarize: [long article text]"
Output: "key points of the article"
```

**é—®ç­”ä»»åŠ¡**:
```
Input:  "question: What is the capital of France? context: Paris is the capital..."
Output: "Paris"
```

### æ¨¡å‹æ¶æ„ (Model Architecture)

**Encoder-Decoder Transformer**:
- åŸºäºåŸå§‹Transformeræ¶æ„ (Vaswani et al., 2017)
- vs BERT (encoder-only): ä¿ç•™äº†decoder
- vs GPT (decoder-only): ä¿ç•™äº†encoder
- **ä¼˜åŠ¿**: åŒæ—¶å…·å¤‡ç†è§£å’Œç”Ÿæˆèƒ½åŠ›

**æ¨¡å‹è§„æ ¼ (T5-11B)**:
- **å‚æ•°é‡**: 11B (110äº¿å‚æ•°)
- **Encoder**: 24å±‚
- **Decoder**: 24å±‚
- **æ³¨æ„åŠ›å¤´**: 128ä¸ª
- **éšè—ç»´åº¦**: 1024
- **ä¸Šä¸‹æ–‡é•¿åº¦**: 512 tokens

**æ¨¡å‹å®¶æ—**:
- T5-Small: 60M
- T5-Base: 220M
- T5-Large: 770M
- T5-3B: 3B
- T5-11B: 11B

### C4æ•°æ®é›† (Colossal Clean Crawled Corpus)

**åˆ›æ–°çš„è®­ç»ƒæ•°æ®**:
- åŸºäºCommon Crawl
- å¤šæ­¥æ¸…æ´—å’Œè¿‡æ»¤
- çº¦750GBæ–‡æœ¬
- é«˜è´¨é‡ã€å¤šæ ·åŒ–å†…å®¹

**æ•°æ®æ¸…æ´—ç­–ç•¥**:
- è¯­è¨€è¿‡æ»¤ (ä»…è‹±è¯­)
- é‡å¤å†…å®¹å»é™¤
- ä½è´¨é‡é¡µé¢è¿‡æ»¤
- æœ‰å®³å†…å®¹ç§»é™¤

## æ€§èƒ½è¡¨ç° (Performance)

### Benchmarkç»“æœ

**GLUEåŸºå‡†**:
- T5-11B: 90.3 (vs BERT-Largeçš„80.5)
- æ¥è¿‘äººç±»æ°´å¹³ (87-89)

**SuperGLUE**:
- T5-11B: 89.3 (vs BERT-Largeçš„71.5)
- æ˜¾è‘—è¶…è¶Šä¹‹å‰çš„æœ€ä½³ç»“æœ

**SQuAD (é—®ç­”)**:
- T5-11B: 95.0 F1 (vs BERT-Largeçš„93.2)

**CNN/DM (æ‘˜è¦)**:
- T5-11B: 43.5 ROUGE-L
- æ¥è¿‘æŠ½å–å¼æ–¹æ³•ä¸Šç•Œ

### å¤šä»»åŠ¡å­¦ä¹ æ•ˆæœ (Multi-task Learning)

**å…³é”®å‘ç°**:
- å¤šä»»åŠ¡è®­ç»ƒæå‡å¹³å‡æ€§èƒ½
- ä»»åŠ¡é—´å­˜åœ¨æ­£è¿ç§»
- å¤§æ¨¡å‹æ›´èƒ½å—ç›Šäºå¤šä»»åŠ¡å­¦ä¹ 

## å†å²æ„ä¹‰ (Historical Significance)

### ç»Ÿä¸€æ¡†æ¶çš„ä»·å€¼ (Value of Unified Framework)

**ç®€åŒ–æ¨¡å‹è®¾è®¡**:
- **ä¹‹å‰**: æ¯ä¸ªä»»åŠ¡éœ€è¦ç‰¹å®šçš„è¾“å‡ºå±‚å’Œlosså‡½æ•°
- **T5ä¹‹å**: æ‰€æœ‰ä»»åŠ¡å…±äº«åŒä¸€æ¶æ„
- **æ„ä¹‰**: é™ä½å¼€å‘å’Œç»´æŠ¤æˆæœ¬

**ä¿ƒè¿›å¤šä»»åŠ¡å­¦ä¹ **:
- ä»»åŠ¡é—´çŸ¥è¯†å…±äº«
- æå‡æ³›åŒ–èƒ½åŠ›
- å‡å°‘æ¯ä¸ªä»»åŠ¡çš„æ•°æ®éœ€æ±‚

### å¯¹åç»­å·¥ä½œçš„å½±å“

**å¯å‘çš„æ¨¡å‹**:
- **BART** (Facebook, 2019): Denoising autoencoder for pretraining
- **mT5** (Google, 2020): å¤šè¯­è¨€ç‰ˆæœ¬T5
- **UL2** (Google, 2022): ç»Ÿä¸€çš„è¯­è¨€å­¦ä¹ æ¡†æ¶
- **Flan-T5** (Google, 2022): Instruction-tuned T5

**éªŒè¯çš„æ–¹å‘**:
- Text-to-textæ˜¯æœ‰æ•ˆçš„ç»Ÿä¸€èŒƒå¼
- Encoder-decoderæ¶æ„é€‚åˆå¤šä»»åŠ¡
- é«˜è´¨é‡é¢„è®­ç»ƒæ•°æ®è‡³å…³é‡è¦

## å½±å“åˆ†æ (Impact Analysis)

### å­¦æœ¯å½±å“ (Academic Impact)

**è®ºæ–‡å¼•ç”¨**:
- æˆªè‡³2025å¹´ï¼Œè¶…è¿‡15,000æ¬¡å¼•ç”¨
- æˆä¸ºNLPç»Ÿä¸€æ¡†æ¶çš„é‡è¦å‚è€ƒ

**ç ”ç©¶æ–¹å‘**:
- æ¢ç´¢ç»Ÿä¸€æ¡†æ¶çš„æé™
- å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ä¼˜åŒ–
- æç¤ºå­¦ä¹  (prompt learning) çš„å…´èµ·

### äº§ä¸šåº”ç”¨ (Industrial Applications)

**Googleå†…éƒ¨åº”ç”¨**:
- Google Translate
- Google Search (queryç†è§£)
- Gmail (smart compose)

**å¼€æºç”Ÿæ€**:
- HuggingFace Transformersåº“å¹¿æ³›æ”¯æŒ
- å¤§é‡åŸºäºT5çš„åº”ç”¨å’Œå¾®è°ƒ

### vs åŒæ—¶æœŸæ¨¡å‹ (vs Contemporary Models)

**vs GPT-2 (1.5B, 2019-02)**:
- T5: Encoder-decoder, å¤šä»»åŠ¡
- GPT-2: Decoder-only, ç”Ÿæˆä¸“æ³¨
- **äº’è¡¥æ€§**: T5æ›´é€‚åˆç†è§£ä»»åŠ¡ï¼ŒGPT-2æ›´é€‚åˆç”Ÿæˆ

**vs BERT (340M, 2018-10)**:
- T5: å¯ç”Ÿæˆï¼Œtext-to-text
- BERT: ä»…ç†è§£ï¼Œéœ€è¦ä»»åŠ¡ç‰¹å®šå¤´
- **ä¼˜åŠ¿**: T5æ›´ç»Ÿä¸€ï¼ŒBERTæ•ˆç‡æ›´é«˜

## æŠ€æœ¯ç»†èŠ‚ (Technical Details)

### é¢„è®­ç»ƒç›®æ ‡ (Pre-training Objective)

**Span Corruption** (ç±»ä¼¼BERTçš„MLM):
```
Original: "Thank you for inviting me to your party last week."
Corrupted: "Thank you <X> me to your party <Y> week."
Target: "<X> for inviting <Y> last <Z>"
```

**ç‰¹ç‚¹**:
- æ©ç›–è¿ç»­çš„token span
- æ¨¡å‹é¢„æµ‹è¢«æ©ç›–çš„span
- ç»“åˆäº†MLMå’Œå»å™ªè‡ªç¼–ç å™¨

### è®­ç»ƒç­–ç•¥ (Training Strategy)

**é¢„è®­ç»ƒé˜¶æ®µ**:
- C4æ•°æ®é›†ä¸Šè¿›è¡Œspan corruption
- å­¦ä¹ é€šç”¨è¯­è¨€è¡¨ç¤º
- è®­ç»ƒ1ä¸‡äº¿tokens

**å¾®è°ƒé˜¶æ®µ**:
- è½¬æ¢ä¸ºtext-to-textæ ¼å¼
- å¤šä»»åŠ¡è”åˆè®­ç»ƒæˆ–å•ä»»åŠ¡è®­ç»ƒ
- ä»»åŠ¡å‰ç¼€æŒ‡å®šå…·ä½“ä»»åŠ¡

### å®éªŒæ¢ç´¢ (Experimental Exploration)

**è®ºæ–‡è´¡çŒ®**:
- ç³»ç»Ÿæ¯”è¾ƒäº†ä¸åŒé¢„è®­ç»ƒç›®æ ‡
- æ¢ç´¢äº†æ¨¡å‹æ¶æ„å˜ä½“
- åˆ†æäº†æ•°æ®é›†è§„æ¨¡å’Œè´¨é‡
- éªŒè¯äº†scalingæ•ˆæœ

**å…³é”®å‘ç°**:
- Encoder-decoderä¼˜äºdecoder-only (å¯¹äºç†è§£ä»»åŠ¡)
- æ•°æ®è´¨é‡æ¯”æ•°é‡æ›´é‡è¦
- å¤šä»»åŠ¡é¢„è®­ç»ƒæœ‰ç›Šä½†éå¿…éœ€

## å±€é™æ€§ä¸æ”¹è¿› (Limitations and Improvements)

### å±€é™æ€§ (Limitations)

**æ•ˆç‡é—®é¢˜**:
- Encoder-decoderæ¯”decoder-onlyå‚æ•°æ•ˆç‡ä½
- æ¨ç†é€Ÿåº¦æ…¢äºBERT (encoder-only)

**ç”Ÿæˆèƒ½åŠ›**:
- ä¸å¦‚GPTç³»åˆ—çš„çº¯ç”Ÿæˆèƒ½åŠ›
- å¼€æ”¾å¼æ–‡æœ¬ç”Ÿæˆè´¨é‡ä¸€èˆ¬

**è§„æ¨¡é™åˆ¶**:
- T5-11B vs GPT-3çš„175B
- å—é™äºå½“æ—¶çš„è®¡ç®—èµ„æº

### åç»­æ”¹è¿› (Subsequent Improvements)

**Flan-T5 (2022)**:
- Instruction tuning
- æå‡zero-shotå’Œfew-shotèƒ½åŠ›
- æ›´å¥½çš„æŒ‡ä»¤éµå¾ª

**UL2 (2022)**:
- ç»Ÿä¸€çš„é¢„è®­ç»ƒç›®æ ‡
- ç»“åˆä¸åŒçš„å»å™ªç­–ç•¥
- æå‡å¤šä»»åŠ¡æ€§èƒ½

**LongT5 (2021)**:
- æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦åˆ°16K
- é€‚åˆé•¿æ–‡æ¡£ä»»åŠ¡

## å› æœå…³ç³»é“¾ (Causal Chain)

### ç›´æ¥å¯ç”¨ (Directly Enabled)

**mT5 (2020)**:
- å¤šè¯­è¨€ç‰ˆæœ¬T5
- 101ç§è¯­è¨€
- éªŒè¯text-to-textåœ¨å¤šè¯­è¨€ä¸Šçš„æœ‰æ•ˆæ€§

**BART (2019)**:
- Facebookçš„å»å™ªè‡ªç¼–ç å™¨
- å—T5å¯å‘ï¼Œæ¢ç´¢ä¸åŒçš„é¢„è®­ç»ƒç›®æ ‡

**Flan-T5 (2022)**:
- Instruction-tuned T5
- æå‡promptèƒ½åŠ›
- ä¸ºChatGPT-styleæ¨¡å‹é“ºè·¯

### é—´æ¥å½±å“ (Indirect Influence)

**ç»Ÿä¸€æ¡†æ¶æ€æƒ³ä¼ æ’­**:
- å½±å“äº†åç»­å¤šä»»åŠ¡å­¦ä¹ ç ”ç©¶
- å¯å‘äº†instruction tuningæ–¹å‘
- ä¸ºGPT-3çš„prompt learningæä¾›å‚è€ƒ

**ä¸­å›½æ¨¡å‹å€Ÿé‰´**:
- **ERNIE 3.0** (Baidu, 2021): å¤šä»»åŠ¡ç»Ÿä¸€æ¡†æ¶
- **M6** (Alibaba, 2021): å¤šæ¨¡æ€ç»Ÿä¸€

## éªŒè¯æ¥æº (Verification Sources)

**å­¦æœ¯è®ºæ–‡**:
- Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *Journal of Machine Learning Research*, 21(140), 1-67. https://arxiv.org/abs/1910.10683

**Googleå®˜æ–¹èµ„æº**:
- Google AI Blog: "Exploring Transfer Learning with T5"
- T5 GitHub Repository

**ç¤¾åŒºéªŒè¯**:
- HuggingFace Model Hub (æ•°åƒä¸ªT5å˜ä½“)
- å¤§é‡åŸºäºT5çš„ç ”ç©¶å’Œåº”ç”¨

## ç›¸å…³æ¦‚å¿µ (Related Concepts)

- [Text-to-Text Framework](../../concepts/text-to-text-framework.md)
- [Encoder-Decoder Transformer](../../concepts/encoder-decoder-transformer.md)
- [Transfer Learning](../../concepts/transfer-learning.md)
- [Multi-task Learning](../../concepts/multi-task-learning.md)

## ç›¸å…³ç« èŠ‚ (Related Chapters)

- [Chapter 2: æ—©æœŸåº”ç”¨ - ç»Ÿä¸€æ¡†æ¶æ¢ç´¢](../../../manuscript/01-foundation/early-applications.md)
- [Chapter 3: å¤šä»»åŠ¡å­¦ä¹ çš„å…´èµ·](../../../manuscript/02-gpt-era/multi-task-learning.md)

## æ—¶é—´çº¿ä½ç½® (Timeline Position)

**å‰ç½®äº‹ä»¶**:
- [Transformerè®ºæ–‡å‘è¡¨](transformer-paper-2017.md) (2017-06): æ¶æ„åŸºç¡€
- [BERTå‘å¸ƒ](bert-release-2018.md) (2018-10): Encoderé¢„è®­ç»ƒ
- [GPT-2å‘å¸ƒ](gpt2-release-2019.md) (2019-02): Decoderè§„æ¨¡åŒ–

**åç»­äº‹ä»¶**:
- [mT5å‘å¸ƒ](mt5-release-2020.md) (2020): å¤šè¯­è¨€æ‰©å±•
- [GPT-3å‘å¸ƒ](gpt3-release-2020.md) (2020-05): Decoder-onlyè§„æ¨¡åŒ–çªç ´

**åŒæ—¶æœŸäº‹ä»¶**:
- [DistilBERT](distilbert-release-2019.md) (2019-10): æ¨¡å‹å‹ç¼©

---

**Event Card Version**: 1.0
**Created**: 2025-10-17
**Last Updated**: 2025-10-17
**Verification Status**: âœ… Verified from academic sources
