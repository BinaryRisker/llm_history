---
event_id: gpt2-2019-02
date: 2019-02-14
title: GPT-2å‘å¸ƒ (GPT-2 Release)
title_en: "Language Models are Unsupervised Multitask Learners"
organization: OpenAI
event_type: model_release
significance_level: critical
verification_status: verified
sources:
  - radford2019gpt2
tags:
  - gpt-series
  - scaling
  - zero-shot-learning
  - ai-safety
causal_connections:
  - enabled_by: [gpt1-2018-06]
  - enables: [gpt3-2020-05]
  - demonstrates: zero_shot_capabilities
technical_concepts:
  - zero-shot-learning
  - scaling-laws
  - language-model-capabilities
  - ai-safety-concerns
chapter_id: 02-gpt-era
---

# GPT-2å‘å¸ƒ (2019-02-14)

**ğŸ”´ å…³é”®é‡Œç¨‹ç¢‘** | **Critical Milestone**

## äº‹ä»¶æ¦‚è¿° (Event Overview)

2019å¹´2æœˆ14æ—¥ï¼ŒOpenAIå‘å¸ƒè®ºæ–‡ã€ŠLanguage Models are Unsupervised Multitask Learnersã€‹(Radford et al., 2019)ï¼Œæå‡ºGPT-2æ¨¡å‹ã€‚è¿™æ˜¯é¦–æ¬¡å±•ç°è§„æ¨¡åŒ–è¯­è¨€æ¨¡å‹çš„å¼ºå¤§zero-shotèƒ½åŠ›ï¼Œå¼•å‘äº†"too dangerous to release"äº‰è®®ï¼Œå¼€å¯äº†AIå®‰å…¨è®¨è®ºã€‚

On February 14, 2019, OpenAI published the paper "Language Models are Unsupervised Multitask Learners" (Radford et al., 2019), introducing GPT-2. This was the first demonstration of powerful zero-shot capabilities in scaled language models, triggering the "too dangerous to release" controversy and initiating discussions on AI safety.

## æŠ€æœ¯åˆ›æ–° (Technical Innovation)

### è§„æ¨¡åŒ–çªç ´ (Scaling Breakthrough)

**æ¨¡å‹è§„æ ¼**:
- **å‚æ•°é‡**: 1.5B (15äº¿å‚æ•°)
- vs GPT-1çš„117M: **æ‰©å¤§13å€**
- **å±‚æ•°**: 48å±‚
- **æ³¨æ„åŠ›å¤´**: 25ä¸ª
- **ä¸Šä¸‹æ–‡é•¿åº¦**: 1024 tokens (vs GPT-1çš„512)

**è®­ç»ƒæ•°æ® (WebText)**:
- ä»Redditå¤–é“¾æŠ“å–é«˜è´¨é‡æ–‡æœ¬
- çº¦800ä¸‡æ–‡æ¡£
- 40GBæ–‡æœ¬æ•°æ® (vs GPT-1çš„5GB)
- è®­ç»ƒtokenæ•°: ~100äº¿ (vs GPT-1çš„~10äº¿)

### Zero-shotèƒ½åŠ›å‘ç° (Zero-shot Capabilities)

**æ— éœ€å¾®è°ƒï¼Œç›´æ¥ä»»åŠ¡æ‰§è¡Œ**:

**æ–‡æœ¬ç”Ÿæˆ**:
- è¿è´¯çš„é•¿æ–‡æœ¬ç”Ÿæˆ
- é£æ ¼è¿ç§»èƒ½åŠ›
- æ•…äº‹åˆ›ä½œèƒ½åŠ›

**é—®ç­”èƒ½åŠ›**:
- CoQAæ•°æ®é›†: 55 F1 (vs 89äººç±»æ°´å¹³)
- æ— éœ€å¾®è°ƒå³å¯å›ç­”é—®é¢˜

**ç¿»è¯‘èƒ½åŠ›**:
- WMT-14 Fr-En: 5 BLEU (vs ç›‘ç£ç³»ç»Ÿçš„33)
- è¯æ˜äº†è¯­è¨€æ¨¡å‹éšå«å¤šè¯­è¨€èƒ½åŠ›

**æ‘˜è¦èƒ½åŠ›**:
- CNN/DMæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬æ‘˜è¦
- è´¨é‡ä½äºç›‘ç£æ–¹æ³•ï¼Œä½†å±•ç°æ½œåŠ›

## å†å²æ„ä¹‰ (Historical Significance)

### "Too Dangerous to Release"äº‰è®®

**OpenAIçš„å†³ç­–**:
- **åˆå§‹**: ä»…å‘å¸ƒ1.17äº¿å‚æ•°çš„å°ç‰ˆæœ¬
- **ç†ç”±**: "æ‹…å¿ƒæ¶æ„åº”ç”¨ï¼ˆè™šå‡ä¿¡æ¯ã€åƒåœ¾é‚®ä»¶ã€é’“é±¼ï¼‰"
- **åˆ†é˜¶æ®µå‘å¸ƒç­–ç•¥**:
  - 2019-02: 117Mç‰ˆæœ¬
  - 2019-05: 345Mç‰ˆæœ¬
  - 2019-08: 774Mç‰ˆæœ¬
  - 2019-11: 1.5Bå®Œæ•´ç‰ˆæœ¬

**ç¤¾åŒºåå“**:
- **æ”¯æŒè€…**: è´Ÿè´£ä»»çš„AIå‘å¸ƒæ–¹å¼
- **æ‰¹è¯„è€…**: è¿‡åº¦å¤¸å¤§é£é™©ï¼Œé˜»ç¢å¼€æ”¾ç ”ç©¶
- **å½±å“**: å¼•å‘äº†å…³äºAIå¼€æ”¾æ€§vså®‰å…¨æ€§çš„æ·±åº¦è®¨è®º

### è§„æ¨¡åŒ–æ³•åˆ™çš„æ—©æœŸéªŒè¯

**å…³é”®å‘ç°**:
- **æ›´å¤§æ¨¡å‹ = æ›´å¼ºèƒ½åŠ›**: 15äº¿å‚æ•°æ˜¾è‘—è¶…è¶Š1äº¿å‚æ•°
- **Zero-shotå­¦ä¹ **: æ— éœ€å¾®è°ƒå³å¯å®Œæˆä»»åŠ¡
- **å¯ç¤º**: è§„æ¨¡åŒ–æ˜¯æå‡æ€§èƒ½çš„æœ‰æ•ˆè·¯å¾„

**ä¸ºGPT-3é“ºè·¯**:
- GPT-2çš„æˆåŠŸéªŒè¯äº†è§„æ¨¡åŒ–æ–¹å‘
- OpenAIå†³å®šç»§ç»­æ‰©å¤§è§„æ¨¡ â†’ GPT-3 (175B)
- è¯æ˜äº†"scaling hypothesis"çš„ä»·å€¼

## å½±å“åˆ†æ (Impact Analysis)

### AIå®‰å…¨æ„è¯†è§‰é†’ (AI Safety Awareness)

**é¦–æ¬¡å¤§è§„æ¨¡å…¬å¼€è®¨è®º**:
- è¯­è¨€æ¨¡å‹çš„åŒé‡ç”¨é€” (dual-use)
- è™šå‡ä¿¡æ¯ç”Ÿæˆé£é™©
- AIå‘å¸ƒçš„ç¤¾ä¼šè´£ä»»

**å½±å“å†³ç­–**:
- OpenAIä»æ­¤è½¬å‘æ›´è°¨æ…çš„å‘å¸ƒç­–ç•¥
- ä¸ºåç»­GPT-3 API-onlyç­–ç•¥å¥ å®šåŸºç¡€
- å¼•å‘å­¦æœ¯ç•Œå’Œäº§ä¸šç•Œå¯¹AIä¼¦ç†çš„å…³æ³¨

### æŠ€æœ¯å½±å“ (Technical Impact)

**è¯æ˜äº†Scalingçš„ä»·å€¼**:
- ä¸šç•Œå¼€å§‹è¿½æ±‚æ›´å¤§è§„æ¨¡æ¨¡å‹
- Googleã€Metaã€ç™¾åº¦ç­‰çº·çº·è·Ÿè¿›
- å¼€å¯äº†"å‚æ•°è§„æ¨¡ç«èµ›"

**Zero-shotèƒ½åŠ›çš„å¯ç¤º**:
- è¯æ˜è¯­è¨€æ¨¡å‹å¯ä»¥æ³›åŒ–åˆ°æœªè§ä»»åŠ¡
- ä¸ºGPT-3çš„few-shot learningæ‰“ä¸‹åŸºç¡€
- æ”¹å˜äº†å¯¹è¯­è¨€æ¨¡å‹èƒ½åŠ›è¾¹ç•Œçš„è®¤çŸ¥

## æŠ€æœ¯ç»†èŠ‚ (Technical Details)

### æ¨¡å‹æ¶æ„æ”¹è¿›

**vs GPT-1çš„æ”¹è¿›**:
- Layer normalizationä½ç½®è°ƒæ•´
- åˆå§‹åŒ–æ–¹æ³•ä¼˜åŒ–
- æ®‹å·®å±‚æƒé‡æŒ‰æ·±åº¦ç¼©æ”¾

**è®­ç»ƒä¼˜åŒ–**:
- Batch size: 512 sequences
- ä¼˜åŒ–å™¨: Adam with gradient clipping
- å­¦ä¹ ç‡è°ƒåº¦: cosine annealing

### æ€§èƒ½è¡¨ç° (Performance)

**è¯­è¨€å»ºæ¨¡ (Perplexity)**:
- WebText test set: 18.34 perplexity
- æ˜¾è‘—ä¼˜äºä¹‹å‰çš„æœ€ä½³ç»“æœ

**Zero-shotä»»åŠ¡æ€§èƒ½**:
- **Reading Comprehension**: CoQA 55 F1
- **Summarization**: CNN/DM èƒ½ç”Ÿæˆåˆç†æ‘˜è¦
- **Translation**: WMT Fr-En 5 BLEU (ä½ä½†å¯ç”¨)
- **Question Answering**: èƒ½å›ç­”äº‹å®æ€§é—®é¢˜

### å±€é™æ€§ (Limitations)

**Zero-shotæ€§èƒ½ä¸è¶³**:
- å¤§å¤šæ•°ä»»åŠ¡ä¸Šä»ä½äºç›‘ç£æ–¹æ³•
- éœ€è¦GPT-3çš„few-shotæ‰èƒ½æ¥è¿‘SOTA

**ç”Ÿæˆè´¨é‡é—®é¢˜**:
- é•¿æ–‡æœ¬ä¸€è‡´æ€§æ¬ ä½³
- äº‹å®å‡†ç¡®æ€§ä¸è¶³
- å®¹æ˜“äº§ç”Ÿé‡å¤

## å› æœå…³ç³»é“¾ (Causal Chain)

### ç›´æ¥å¯ç”¨ (Directly Enabled)

**GPT-3 (2020-05)**:
- GPT-2æˆåŠŸ â†’ OpenAIå†³å®šæ‰©å¤§åˆ°175B
- éªŒè¯äº†scaling hypothesis
- Few-shot learningèƒ½åŠ›çš„é£è·ƒ

**å¼€æºç¤¾åŒºè·Ÿè¿›**:
- **GPT-Neo, GPT-J**: EleutherAIå¤ç°GPT-2/3
- **BLOOM**: å¤§è§„æ¨¡å¤šè¯­è¨€æ¨¡å‹
- æ¨åŠ¨å¼€æºLLMç”Ÿæ€å‘å±•

### é—´æ¥å½±å“ (Indirect Influence)

**ä¸­å›½LLMå‘å±•**:
- **ERNIE 2.0** (Baidu, 2019): å—GPT-2å¯å‘ï¼ŒæŒç»­é¢„è®­ç»ƒ
- **CPM** (æ¸…å, 2020): ä¸­æ–‡GPT-2å¤ç°
- éªŒè¯äº†è§„æ¨¡åŒ–åœ¨ä¸­æ–‡ä¸Šçš„æœ‰æ•ˆæ€§

**AIå®‰å…¨ç ”ç©¶**:
- è™šå‡ä¿¡æ¯æ£€æµ‹ç ”ç©¶æ¿€å¢
- AIç”Ÿæˆå†…å®¹æ£€æµ‹å·¥å…·å‘å±•
- è´Ÿè´£ä»»AIå‘å¸ƒè§„èŒƒå»ºç«‹

## äº‰è®®ä¸è®¨è®º (Controversy and Discussion)

### å‘å¸ƒç­–ç•¥äº‰è®®

**æ”¯æŒåˆ†é˜¶æ®µå‘å¸ƒ**:
- è´Ÿè´£ä»»çš„AIå¼€å‘ç¤ºèŒƒ
- ç»™ç¤¾ä¼šæ—¶é—´é€‚åº”å’Œå‡†å¤‡
- é™ä½æ½œåœ¨æ»¥ç”¨é£é™©

**æ‰¹è¯„åˆ†é˜¶æ®µå‘å¸ƒ**:
- é˜»ç¢å­¦æœ¯ç ”ç©¶å’Œå¼€æ”¾ç§‘å­¦
- å¤¸å¤§å®é™…é£é™©
- ä¸åˆ©äºç¤¾åŒºç›‘ç£å’Œæ”¹è¿›

### å®é™…å½±å“è¯„ä¼° (2019-2025)

**è™šå‡ä¿¡æ¯é—®é¢˜**:
- GPT-2ç¡®å®è¢«ç”¨äºç”Ÿæˆè™šå‡å†…å®¹
- ä½†è§„æ¨¡è¿œå°äºæœ€åˆæ‹…å¿§
- æ£€æµ‹æŠ€æœ¯åŒæ­¥å‘å±•

**å¼€æ”¾vsé—­æºçš„è½¬æŠ˜ç‚¹**:
- GPT-2ä¹‹å‰: OpenAIå®Œå…¨å¼€æº
- GPT-2: åˆ†é˜¶æ®µå‘å¸ƒ
- GPT-3ä¹‹å: API-onlyï¼Œä¸å¼€æº
- OpenAIä»"Open"è½¬å‘å•†ä¸šåŒ–

## éªŒè¯æ¥æº (Verification Sources)

**å­¦æœ¯è®ºæ–‡**:
- Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. *OpenAI Technical Report*. https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

**OpenAIå®˜æ–¹åšå®¢**:
- "Better Language Models and Their Implications" (2019-02-14)
- "GPT-2: 6-Month Follow-Up" (2019-08-20)

**ç¤¾åŒºåé¦ˆ**:
- HuggingFace GPT-2æ¨¡å‹é¡µé¢
- æ•°åƒä¸ªåŸºäºGPT-2çš„åº”ç”¨å’Œç ”ç©¶

**åª’ä½“æŠ¥é“**:
- The Verge: "OpenAI's new multitalented AI writes, translates, and slanders"
- MIT Technology Review: "An AI that writes convincing prose risks mass-producing fake news"

## ç›¸å…³æ¦‚å¿µ (Related Concepts)

- [Zero-shot Learning](../../concepts/zero-shot-learning.md)
- [Scaling Laws](../../concepts/scaling-laws.md)
- [AI Safety](../../concepts/ai-safety.md)
- [Dual-use Technology](../../concepts/dual-use-technology.md)

## ç›¸å…³ç« èŠ‚ (Related Chapters)

- [Chapter 3: GPTè§„æ¨¡åŒ–](../../../manuscript/02-gpt-era/scaling-up.md)
- [Chapter 4: AIå®‰å…¨ä¸ä¼¦ç†](../../../manuscript/03-alignment-breakthrough/ai-safety-concerns.md)

## æ—¶é—´çº¿ä½ç½® (Timeline Position)

**å‰ç½®äº‹ä»¶**:
- [GPT-1å‘å¸ƒ](gpt1-release-2018.md) (2018-06): éªŒè¯é¢„è®­ç»ƒèŒƒå¼

**åç»­äº‹ä»¶**:
- [T5å‘å¸ƒ](t5-release-2019.md) (2019-10): ç»Ÿä¸€æ¡†æ¶æ–¹æ³•
- [GPT-3å‘å¸ƒ](gpt3-release-2020.md) (2020-05): Few-shot learningçªç ´

**åŒæ—¶æœŸäº‹ä»¶**:
- [BERT](bert-release-2018.md) (2018-10): åŒå‘é¢„è®­ç»ƒçš„æˆåŠŸ

---

**Event Card Version**: 1.0
**Created**: 2025-10-17
**Last Updated**: 2025-10-17
**Verification Status**: âœ… Verified from academic sources and official announcements
