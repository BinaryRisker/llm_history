---
event_id: llama-2023-02
date: 2023-02-24
title: Meta LLaMAå‘å¸ƒ (Meta LLaMA Release)
title_en: "LLaMA: Open and Efficient Foundation Language Models"
organization: Meta
event_type: model_release
significance_level: critical
verification_status: verified
sources:
  - meta2023llama
tags:
  - llama-series
  - open-source
  - efficient-models
  - meta
causal_connections:
  - response_to: [chatgpt-2022-11, gpt4-2023-03]
  - enables: [llama2-2023-07, llama3-2024-04]
  - triggers: open_source_llm_wave
technical_concepts:
  - efficient-training
  - chinchilla-optimal
  - open-source-llm
  - model-family
chapter_id: 05-global-race-2023
---

# Meta LLaMAå‘å¸ƒ (2023-02-24)

**ğŸ”´ å…³é”®é‡Œç¨‹ç¢‘** | **Critical Milestone**

## äº‹ä»¶æ¦‚è¿° (Event Overview)

2023å¹´2æœˆ24æ—¥ï¼ŒMetaå‘å¸ƒLLaMA (Large Language Model Meta AI)ï¼Œä¸€ç»„å‚æ•°è§„æ¨¡ä»7Båˆ°65Bçš„å¼€æºè¯­è¨€æ¨¡å‹ã€‚LLaMAè¯æ˜äº†å°è§„æ¨¡é«˜è´¨é‡æ¨¡å‹å¯ä»¥ä¸å¤§è§„æ¨¡æ¨¡å‹ç«äº‰ï¼Œå¼•å‘äº†å¼€æºLLMæµªæ½®ï¼Œæ”¹å˜äº†å¼€æºvsé—­æºçš„ç«äº‰æ ¼å±€ã€‚

On February 24, 2023, Meta released LLaMA (Large Language Model Meta AI), a collection of open-source language models ranging from 7B to 65B parameters. LLaMA proved that smaller high-quality models can compete with large-scale models, triggering an open-source LLM wave and changing the competitive landscape between open and closed source.

## æŠ€æœ¯åˆ›æ–° (Technical Innovation)

### æ¨¡å‹å®¶æ— (Model Family)

**4ä¸ªä¸åŒè§„æ¨¡**:
- **LLaMA-7B**: 70äº¿å‚æ•°
- **LLaMA-13B**: 130äº¿å‚æ•°
- **LLaMA-33B**: 330äº¿å‚æ•°
- **LLaMA-65B**: 650äº¿å‚æ•°

**è®¾è®¡ç†å¿µ**: Chinchilla-optimal
- éµå¾ªChinchilla Scaling Laws (Hoffmann et al., 2022)
- æ›´å¤šè®­ç»ƒæ•°æ® vs æ›´å¤§æ¨¡å‹
- 7Bæ¨¡å‹è®­ç»ƒ1T tokens (vs GPT-3çš„300B)

### é«˜è´¨é‡è®­ç»ƒæ•°æ® (High-quality Training Data)

**æ•°æ®æ¥æº (1.4T tokens)**:
- **CommonCrawl**: 67%
- **C4**: 15%
- **GitHub**: 4.5%
- **Wikipedia**: 4.5%
- **Books**: 4.5%
- **ArXiv**: 2.5%
- **StackExchange**: 2%

**æ•°æ®æ¸…æ´—**:
- ä¸¥æ ¼çš„è´¨é‡è¿‡æ»¤
- å»é‡å’Œæœ‰æ¯’å†…å®¹è¿‡æ»¤
- å¤šè¯­è¨€ä½†ä»¥è‹±è¯­ä¸ºä¸»

### æ•ˆç‡ä¼˜åŒ– (Efficiency Optimization)

**æ¶æ„æ”¹è¿›**:
- Pre-normalization (GPT-3 style)
- SwiGLUæ¿€æ´»å‡½æ•° (vs ReLU)
- Rotary Positional Embeddings (RoPE)
- æ›´é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†

## æ€§èƒ½è¡¨ç° (Performance)

### Benchmarkç»“æœ

**ä¸æ›´å¤§æ¨¡å‹ç«äº‰**:
| Benchmark | LLaMA-13B | GPT-3 (175B) |
|-----------|-----------|--------------|
| MMLU | 46.9% | 43.9% |
| HellaSwag | 79.2% | 78.9% |
| PIQA | 79.8% | 81.0% |

**LLaMA-65Bæ€§èƒ½**:
- **MMLU**: 63.4% (vs GPT-3çš„43.9%)
- **æ¥è¿‘Chinchilla (70B)**: 67.5%
- è¯æ˜å¼€æºå¯ä»¥è¾¾åˆ°é«˜æ€§èƒ½

### æ•ˆç‡ä¼˜åŠ¿ (Efficiency Advantage)

**å°æ¨¡å‹é«˜æ€§èƒ½**:
- LLaMA-13Båœ¨å¾ˆå¤šä»»åŠ¡ä¸Šè¶…è¶ŠGPT-3 (175B)
- æ¨ç†æˆæœ¬å¤§å¹…é™ä½
- å¯ä»¥åœ¨æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œ

## å†å²æ„ä¹‰ (Historical Significance)

### å¼€æºLLMæµªæ½®çš„å¼•çˆ†ç‚¹ (Triggering Open-Source LLM Wave)

**ä¹‹å‰çš„å¼€æºå›°å¢ƒ**:
- GPT-3ã€ChatGPTé—­æº
- å¼€æºç¤¾åŒºç¼ºå°‘é«˜è´¨é‡åŸºç¡€æ¨¡å‹
- ä¾èµ–Metaã€Googleçš„å–„æ„

**LLaMAçš„çªç ´**:
- é«˜è´¨é‡å¼€æºæ¨¡å‹
- å¯å•†ä¸šåŒ–è·¯å¾„ (LLaMA 2å)
- ç¤¾åŒºå¯ä»¥è‡ªç”±å¾®è°ƒå’Œå®éªŒ

**å¼•å‘çš„å¼€æºé¡¹ç›® (2023)**:
- **Alpaca** (Stanford): LLaMAå¾®è°ƒï¼ŒæŒ‡ä»¤éµå¾ª
- **Vicuna** (UC Berkeley): å¯¹è¯èƒ½åŠ›ä¼˜åŒ–
- **Koala** (UC Berkeley): å¯¹è¯æ•°æ®å¾®è°ƒ
- **WizardLM**: å¤æ‚æŒ‡ä»¤æ•°æ®å¢å¼º
- **æ•°åä¸ªå¼€æºé¡¹ç›®**: åŸºäºLLaMAçš„å„ç§åº”ç”¨

### å¯¹å¼€æºvsé—­æºæ ¼å±€çš„å½±å“

**é—­æºä¼˜åŠ¿å‰Šå¼±**:
- LLaMAè¯æ˜å¼€æºå¯ä»¥é«˜æ€§èƒ½
- é™ä½AIåº”ç”¨é—¨æ§›
- å‰Šå¼±OpenAIã€Googleçš„å„æ–­åœ°ä½

**å¼€æºç”Ÿæ€ç¹è£**:
- HuggingFaceç”Ÿæ€çˆ†å‘
- ç¤¾åŒºè´¡çŒ®åŠ é€Ÿ
- å¤šæ ·åŒ–åº”ç”¨æ¶Œç°

## å½±å“åˆ†æ (Impact Analysis)

### å­¦æœ¯ç•Œå½±å“ (Academic Impact)

**ç ”ç©¶åŠ é€Ÿ**:
- å®éªŒå®¤å¯ä»¥æœ¬åœ°è¿è¡ŒLLaMA
- æ— éœ€æ˜‚è´µAPIè°ƒç”¨
- ç ”ç©¶é€æ˜åº¦æå‡

**å¾®è°ƒç ”ç©¶çˆ†å‘**:
- LoRA, QLoRAç­‰é«˜æ•ˆå¾®è°ƒæ–¹æ³•
- æŒ‡ä»¤å¾®è°ƒæ•°æ®ç ”ç©¶
- å¯¹é½æ–¹æ³•æ¢ç´¢

### äº§ä¸šç•Œå½±å“ (Industrial Impact)

**é™ä½AIåº”ç”¨é—¨æ§›**:
- åˆ›ä¸šå…¬å¸å¯ä»¥åŸºäºLLaMAæ„å»ºäº§å“
- æ— éœ€ä»å¤´è®­ç»ƒå¤§æ¨¡å‹
- åŠ é€ŸAIæ°‘ä¸»åŒ–

**äº‘æœåŠ¡å•†å—ç›Š**:
- æä¾›LLaMAæ‰˜ç®¡æœåŠ¡
- å¾®è°ƒå’Œéƒ¨ç½²å·¥å…·
- æ–°å•†ä¸šæ¨¡å¼

### ä¸­å›½å¼€æºç”Ÿæ€å¯ç¤º (Inspiration for Chinese Open Source)

**ä¸­å›½è·Ÿè¿›**:
- **ChatGLM-6B** (æ™ºè°±, 2023-03): ä¸­å›½é¦–ä¸ªå¼€æºå¯¹è¯æ¨¡å‹
- **Qwen-7B** (é˜¿é‡Œ, 2023-08): é«˜è´¨é‡ä¸­æ–‡å¼€æº
- **Baichuan** (ç™¾å·, 2023-06): å•†ä¸šå‹å¥½å¼€æº

**éªŒè¯æ–¹å‘**:
- å¼€æºæ˜¯è¿½èµ¶é—­æºçš„æœ‰æ•ˆè·¯å¾„
- é«˜è´¨é‡è®­ç»ƒæ•°æ®è‡³å…³é‡è¦
- æ•ˆç‡ä¼˜åŒ–å¯ä»¥å¼¥è¡¥è§„æ¨¡åŠ£åŠ¿

## äº‰è®®ä¸æ³„æ¼äº‹ä»¶ (Controversies and Leak)

### åˆå§‹å‘å¸ƒé™åˆ¶ (Initial Release Restrictions)

**ç ”ç©¶ä¸“ç”¨è®¸å¯**:
- ä»…å‘ç ”ç©¶æœºæ„æä¾›
- éœ€è¦ç”³è¯·å®¡æ‰¹
- ç¦æ­¢å•†ä¸šä½¿ç”¨

**æ³„æ¼äº‹ä»¶ (2023-03)**:
- LLaMAæƒé‡åœ¨äº’è”ç½‘ä¸Šæ³„æ¼
- é€šè¿‡BitTorrentå¹¿æ³›ä¼ æ’­
- Metaæ— æ³•é˜»æ­¢

### æ³„æ¼çš„å½±å“ (Impact of Leak)

**æ­£é¢å½±å“**:
- åŠ é€Ÿå¼€æºç”Ÿæ€å‘å±•
- æ›´å¤šäººå¯ä»¥å®éªŒå’Œå¾®è°ƒ
- æ¨åŠ¨LLaMA 2çš„å•†ä¸šå‹å¥½è®¸å¯

**è´Ÿé¢å½±å“**:
- Metaçš„æ§åˆ¶æ„å›¾å¤±è´¥
- æ½œåœ¨çš„æ¶æ„ä½¿ç”¨é£é™©
- çŸ¥è¯†äº§æƒé—®é¢˜

## æŠ€æœ¯ç»†èŠ‚ (Technical Details)

### è®­ç»ƒè§„æ¨¡ (Training Scale)

**è®¡ç®—èµ„æº**:
- GPUæ—¶æ•°: ~2048 A100 GPUs Ã— 21å¤© (LLaMA-65B)
- æ€»è®­ç»ƒæˆæœ¬: çº¦$2-3M
- vs GPT-3çš„$4.6M: æˆæœ¬é™ä½

**è®­ç»ƒæ•ˆç‡**:
- Chinchilla-optimalç­–ç•¥
- æ›´å¤šæ•°æ®è®­ç»ƒ
- æ¯ä¸ªtokençš„ä»·å€¼æœ€å¤§åŒ–

### æ¶æ„ç»†èŠ‚ (Architecture Details)

**Transformeræ”¹è¿›**:
- RMSNorm (Root Mean Square Layer Normalization)
- SwiGLUæ¿€æ´»å‡½æ•°
- RoPEä½ç½®ç¼–ç 
- å¤šæŸ¥è¯¢æ³¨æ„åŠ› (Multi-query attention) éƒ¨åˆ†ä½¿ç”¨

## å› æœå…³ç³»é“¾ (Causal Chain)

### ç›´æ¥å¯ç”¨ (Directly Enabled)

**LLaMA 2 (2023-07)**:
- å•†ä¸šå‹å¥½è®¸å¯
- æ€§èƒ½å¤§å¹…æå‡
- å¼€æºç”Ÿæ€è¿›ä¸€æ­¥ç¹è£

**å¼€æºå¾®è°ƒç”Ÿæ€**:
- Alpaca, Vicuna, Koalaç­‰æ•°åä¸ªé¡¹ç›®
- LoRA, QLoRAé«˜æ•ˆå¾®è°ƒæ–¹æ³•æ™®åŠ
- æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†çˆ†å‘

### é—´æ¥å½±å“ (Indirect Influence)

**ä¸­å›½å¼€æºè·Ÿè¿›**:
- ChatGLM-6B (2023-03)
- Qwen-7B (2023-08)
- éªŒè¯å¼€æºè·¯å¾„å¯è¡Œæ€§

**ç«äº‰æ ¼å±€æ”¹å˜**:
- å‰Šå¼±OpenAIé—­æºä¼˜åŠ¿
- æ¨åŠ¨Googleå¼€æºGemma (2024)
- å¼€æºvsé—­æºé•¿æœŸç«äº‰

## éªŒè¯æ¥æº (Verification Sources)

**å­¦æœ¯è®ºæ–‡**:
- Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., ... & Lample, G. (2023). LLaMA: Open and Efficient Foundation Language Models. https://arxiv.org/abs/2302.13971

**Metaå®˜æ–¹åšå®¢**:
- "Introducing LLaMA: A foundational, 65-billion-parameter large language model" (2023-02-24)

**ç¤¾åŒºéªŒè¯**:
- HuggingFaceä¸‹è½½é‡: æ•°åƒä¸‡æ¬¡
- GitHubé¡¹ç›®: æ•°ç™¾ä¸ªåŸºäºLLaMA

**åª’ä½“æŠ¥é“**:
- The Verge: "Meta's powerful AI language model has leaked online"
- MIT Technology Review: "Meta's LLaMA leak shows the power of open source"

## ç›¸å…³æ¦‚å¿µ (Related Concepts)

- [Open-source LLM](../../concepts/open-source-llm.md)
- [Chinchilla Scaling Laws](../../concepts/chinchilla-scaling.md)
- [Efficient Training](../../concepts/efficient-training.md)

## ç›¸å…³ç« èŠ‚ (Related Chapters)

- [Chapter 8: Metaå¼€æºæˆ˜ç•¥](../../../manuscript/05-global-race-2023/meta-llama.md)

## æ—¶é—´çº¿ä½ç½® (Timeline Position)

**å‰ç½®äº‹ä»¶**:
- [ChatGPTå‘å¸ƒ](chatgpt-launch-2022.md) (2022-11): å¼•å‘ç«äº‰å‹åŠ›

**åç»­äº‹ä»¶**:
- [LLaMA 2å‘å¸ƒ](llama2-release-2023.md) (2023-07): å•†ä¸šå‹å¥½ç‰ˆæœ¬
- [Llama 3å‘å¸ƒ](llama3-release-2024.md) (2024-04): æ€§èƒ½é€¼è¿‘é—­æº

**åŒæ—¶æœŸç«äº‰**:
- [GPT-4å‘å¸ƒ](gpt4-release-2023.md) (2023-03): é—­æºé¢†å…ˆ
- [ChatGLM-6Bå‘å¸ƒ](chatglm-release-2023.md) (2023-03): ä¸­å›½å¼€æºè·Ÿè¿›

---

**Event Card Version**: 1.0
**Created**: 2025-10-17
**Last Updated**: 2025-10-17
**Verification Status**: âœ… Verified from academic paper and official announcements
