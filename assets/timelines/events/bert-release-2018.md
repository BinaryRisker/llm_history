---
event_id: bert-2018-10
date: 2018-10-11
title: BERTå‘å¸ƒ (BERT Release)
title_en: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
organization: Google
event_type: model_release
significance_level: critical
verification_status: verified
sources:
  - devlin2018bert
tags:
  - bert
  - bidirectional-pretraining
  - masked-language-model
  - encoder-only
causal_connections:
  - enabled_by: [transformer-2017-06]
  - parallel_to: [gpt1-2018-06]
  - enables: [distilbert-2019-10]
  - demonstrates: bidirectional_contextualization
technical_concepts:
  - masked-language-modeling
  - next-sentence-prediction
  - bidirectional-encoding
  - encoder-only-transformer
chapter_id: 01-foundation
---

# BERTå‘å¸ƒ (2018-10-11)

**ğŸ”´ å…³é”®é‡Œç¨‹ç¢‘** | **Critical Milestone**

## äº‹ä»¶æ¦‚è¿° (Event Overview)

2018å¹´10æœˆ11æ—¥ï¼ŒGoogleå‘å¸ƒè®ºæ–‡ã€ŠBERT: Pre-training of Deep Bidirectional Transformers for Language Understandingã€‹(Devlin et al., 2018)ï¼Œæå‡ºBERTæ¨¡å‹ã€‚è¿™æ˜¯é¦–æ¬¡æˆåŠŸå°†Transformer encoderæ¶æ„åº”ç”¨äºåŒå‘é¢„è®­ç»ƒï¼Œåœ¨11ä¸ªNLPä»»åŠ¡ä¸Šåˆ·æ–°SOTAï¼Œè¯æ˜äº†åŒå‘ä¸Šä¸‹æ–‡ç†è§£çš„é‡è¦æ€§ã€‚

On October 11, 2018, Google published the paper "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (Devlin et al., 2018), introducing BERT. This was the first successful application of the Transformer encoder architecture to bidirectional pre-training, achieving state-of-the-art results on 11 NLP tasks and demonstrating the importance of bidirectional contextualization.

## æŠ€æœ¯åˆ›æ–° (Technical Innovation)

### æ ¸å¿ƒæ–¹æ³• (Core Methodology)

**Bidirectional Encoder Representations from Transformers (BERT)**:

**1. Masked Language Modeling (MLM, æ©ç è¯­è¨€å»ºæ¨¡)**:
- éšæœºæ©ç›–15%çš„è¾“å…¥token
- æ¨¡å‹é¢„æµ‹è¢«æ©ç›–çš„token
- å…è®¸æ¨¡å‹åˆ©ç”¨**åŒå‘ä¸Šä¸‹æ–‡**ä¿¡æ¯ (vs GPTçš„å•å‘)

**2. Next Sentence Prediction (NSP, ä¸‹ä¸€å¥é¢„æµ‹)**:
- é¢„æµ‹ä¸¤ä¸ªå¥å­æ˜¯å¦è¿ç»­
- å­¦ä¹ å¥å­é—´çš„å…³ç³»
- ä¸ºé—®ç­”ã€æ¨ç†ä»»åŠ¡æä¾›åŸºç¡€

### æ¨¡å‹è§„æ ¼ (Model Specifications)

**ä¸¤ä¸ªç‰ˆæœ¬**:

**BERT-Base**:
- **å‚æ•°é‡**: 110M (1.1äº¿å‚æ•°)
- **å±‚æ•°**: 12å±‚Transformer encoder
- **æ³¨æ„åŠ›å¤´**: 12ä¸ª
- **éšè—ç»´åº¦**: 768

**BERT-Large**:
- **å‚æ•°é‡**: 340M (3.4äº¿å‚æ•°)
- **å±‚æ•°**: 24å±‚
- **æ³¨æ„åŠ›å¤´**: 16ä¸ª
- **éšè—ç»´åº¦**: 1024

**è®­ç»ƒæ•°æ® (Training Data)**:
- BooksCorpus: 800M words
- English Wikipedia: 2,500M words
- æ€»è®¡: çº¦3.3B words

### æ€§èƒ½çªç ´ (Performance Breakthrough)

**åœ¨11ä¸ªNLPä»»åŠ¡ä¸Šåˆ·æ–°SOTA**:

**GLUEåŸºå‡†**:
- MNLI: 86.7% (vs ä¹‹å‰çš„80.6%)
- QQP: 71.2% (vs 66.1%)
- QNLI: 92.7% (vs 87.4%)
- SST-2: 94.9% (vs 93.2%)
- CoLA: 60.5% (vs 35.0%) - **25.5åˆ†ç»å¯¹æå‡**

**SQuAD 1.1é—®ç­”**:
- F1 score: 93.2% (vs 91.6%)

**SQuAD 2.0**:
- F1 score: 83.1% (vs 66.3%) - **16.8åˆ†ç»å¯¹æå‡**

## å†å²æ„ä¹‰ (Historical Significance)

### åŒå‘vså•å‘ (Bidirectional vs Unidirectional)

**vs GPT-1çš„å…³é”®åŒºåˆ«**:

| ç»´åº¦ | BERT (Encoder-only) | GPT-1 (Decoder-only) |
|------|---------------------|----------------------|
| æ–¹å‘ | åŒå‘ (bidirectional) | å•å‘ (left-to-right) |
| è®­ç»ƒç›®æ ‡ | MLM + NSP | Language Modeling |
| é€‚ç”¨ä»»åŠ¡ | ç†è§£ä»»åŠ¡ (NLU) | ç”Ÿæˆä»»åŠ¡ (NLG) |
| ä¸Šä¸‹æ–‡åˆ©ç”¨ | å®Œæ•´åŒå‘ | ä»…å·¦ä¾§ä¸Šä¸‹æ–‡ |

**äº’è¡¥æ€§éªŒè¯**:
- BERTå’ŒGPT-1åœ¨2018å¹´çš„æˆåŠŸå…±åŒè¯æ˜äº†é¢„è®­ç»ƒèŒƒå¼çš„ä»·å€¼
- åˆ†åˆ«è¯æ˜äº†encoderå’Œdecoderçš„æœ‰æ•ˆæ€§
- ä¸ºåç»­T5çš„encoder-decoderç»Ÿä¸€æ¶æ„é“ºè·¯

### å­¦æœ¯å½±å“ (Academic Impact)

**è®ºæ–‡å¼•ç”¨**:
- æˆªè‡³2025å¹´ï¼Œè¶…è¿‡90,000æ¬¡å¼•ç”¨
- æˆä¸ºNLPé¢†åŸŸæœ€å…·å½±å“åŠ›çš„è®ºæ–‡ä¹‹ä¸€

**å¼•å‘çš„ç ”ç©¶æ–¹å‘**:
- **æ¨¡å‹å‹ç¼©**: DistilBERT, TinyBERT, ALBERT
- **å¤šè¯­è¨€æ‰©å±•**: mBERT, XLM, XLM-R
- **é¢†åŸŸé€‚åº”**: BioBERT, SciBERT, FinBERT
- **æ¶æ„æ”¹è¿›**: RoBERTa, ELECTRA, DeBERTa

## å½±å“åˆ†æ (Impact Analysis)

### çŸ­æœŸå½±å“ (Short-term Impact, 2018-2020)

**Googleå†…éƒ¨åº”ç”¨ (2019)**:
- **Google Search**: BERTç”¨äºç†è§£æœç´¢æŸ¥è¯¢
- å½±å“10%çš„è‹±è¯­æœç´¢æŸ¥è¯¢
- å¤§å¹…æå‡æœç´¢è´¨é‡

**å¼€æºç”Ÿæ€çˆ†å‘**:
- Hugging Face Transformersåº“è¯ç”Ÿ
- BERTæˆä¸ºNLPåº”ç”¨çš„æ ‡å‡†èµ·ç‚¹
- å¤§é‡åŸºäºBERTçš„åº”ç”¨æ¶Œç°

### é•¿æœŸå½±å“ (Long-term Impact, 2018-2025)

**NLPä»»åŠ¡èŒƒå¼æ”¹å˜**:
- **ä¹‹å‰**: ä¸ºæ¯ä¸ªä»»åŠ¡ä»å¤´è®­ç»ƒæ¨¡å‹
- **ä¹‹å**: é¢„è®­ç»ƒæ¨¡å‹ + ä»»åŠ¡å¾®è°ƒ
- **é™ä½é—¨æ§›**: å°å›¢é˜Ÿä¹Ÿèƒ½åšå‡ºé«˜è´¨é‡NLPåº”ç”¨

**äº§ä¸šæ ‡å‡†**:
- BERTæˆä¸ºNLUä»»åŠ¡çš„baseline
- å½±å“äº†åç»­æ‰€æœ‰encoderæ¶æ„çš„è®¾è®¡
- ä¸ºå¤šæ¨¡æ€æ¨¡å‹ (CLIP, GPT-4V) ä¸­çš„encoderéƒ¨åˆ†æä¾›å‚è€ƒ

## æŠ€æœ¯ç»†èŠ‚ (Technical Details)

### é¢„è®­ç»ƒæ–¹æ³• (Pre-training Methodology)

**Masked Language Modeling (MLM)**:
```
è¾“å…¥: "The [MASK] sat on the [MASK]."
ç›®æ ‡: é¢„æµ‹ [MASK] = "cat", [MASK] = "mat"

Maskingç­–ç•¥:
- 80%æ›¿æ¢ä¸º[MASK]
- 10%æ›¿æ¢ä¸ºéšæœºtoken
- 10%ä¿æŒä¸å˜
```

**Next Sentence Prediction (NSP)**:
```
è¾“å…¥: [CLS] Sentence A [SEP] Sentence B [SEP]
ç›®æ ‡: é¢„æµ‹Bæ˜¯å¦æ˜¯Açš„ä¸‹ä¸€å¥ (IsNext vs NotNext)
```

### å¾®è°ƒç­–ç•¥ (Fine-tuning Strategy)

**åˆ†ç±»ä»»åŠ¡**:
- ä½¿ç”¨[CLS] tokençš„è¾“å‡ºè¡¨ç¤º
- æ·»åŠ ä¸€å±‚å…¨è¿æ¥å±‚
- åœ¨ç‰¹å®šä»»åŠ¡æ•°æ®ä¸Šå¾®è°ƒ

**é—®ç­”ä»»åŠ¡ (SQuAD)**:
- é¢„æµ‹ç­”æ¡ˆçš„startå’Œendä½ç½®
- ä¸¤ä¸ªçº¿æ€§å±‚åˆ†åˆ«é¢„æµ‹start/end logits

**å‘½åå®ä½“è¯†åˆ« (NER)**:
- æ¯ä¸ªtokené¢„æµ‹å®ä½“æ ‡ç­¾
- Token-levelåˆ†ç±»ä»»åŠ¡

### å±€é™æ€§ä¸æ”¹è¿› (Limitations and Improvements)

**RoBERTaæ”¹è¿› (2019)**:
- ç§»é™¤NSPä»»åŠ¡ (è¯æ˜NSPä¸å¿…è¦)
- æ›´å¤§çš„batch sizeå’Œè®­ç»ƒæ•°æ®
- åŠ¨æ€maskingç­–ç•¥

**ALBERTä¼˜åŒ– (2019)**:
- å‚æ•°å…±äº«å‡å°‘æ¨¡å‹å¤§å°
- Sentence-order predictionæ›¿ä»£NSP

## å› æœå…³ç³»é“¾ (Causal Chain)

### ç›´æ¥å¯ç”¨ (Directly Enabled)

**DistilBERT (2019-10)**:
- çŸ¥è¯†è’¸é¦å‹ç¼©BERT
- å‚æ•°å‡å°‘40%ï¼Œé€Ÿåº¦æå‡60%
- ä¿æŒ97%æ€§èƒ½

**RoBERTa (2019)**:
- ä¼˜åŒ–BERTè®­ç»ƒç­–ç•¥
- è¯æ˜BERTæœªè¢«å……åˆ†è®­ç»ƒ

**ALBERT (2019)**:
- å‚æ•°å…±äº«é™ä½å†…å­˜å ç”¨
- å…è®¸æ›´æ·±çš„ç½‘ç»œ

### é—´æ¥å½±å“ (Indirect Influence)

**å¯¹ä¸­å›½NLPçš„å½±å“**:
- **ERNIE 1.0** (Baidu, 2019): çŸ¥è¯†å¢å¼ºçš„BERT
- **ChatGLM** (Zhipu, 2023): åŒå‘ç¼–ç æ€æƒ³åœ¨å¯¹è¯æ¨¡å‹ä¸­çš„åº”ç”¨

**å¯¹å¤šæ¨¡æ€çš„å½±å“**:
- **CLIP** (2021): è§†è§‰encoderå€Ÿé‰´BERTæ€æƒ³
- **GPT-4V** (2023): Vision encoderéƒ¨åˆ†å—BERTå½±å“

## éªŒè¯æ¥æº (Verification Sources)

**å­¦æœ¯è®ºæ–‡**:
- Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *Proceedings of NAACL-HLT 2019*, 4171-4186. https://arxiv.org/abs/1810.04805

**æ€§èƒ½åŸºå‡†**:
- GLUE Leaderboard (2018-2019)
- SQuAD Leaderboard

**Googleå®˜æ–¹åšå®¢**:
- "Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing" (2018-11-02)

**ç¤¾åŒºéªŒè¯**:
- Hugging Face Transformersåº“ä¸­çš„BERTå®ç°
- æ•°åƒä¸ªåŸºäºBERTçš„å¼€æºé¡¹ç›®

## ç›¸å…³æ¦‚å¿µ (Related Concepts)

- [Masked Language Modeling](../../concepts/masked-language-modeling.md)
- [Next Sentence Prediction](../../concepts/next-sentence-prediction.md)
- [Bidirectional Encoding](../../concepts/bidirectional-encoding.md)
- [Encoder-only Transformer](../../concepts/encoder-only-transformer.md)

## ç›¸å…³ç« èŠ‚ (Related Chapters)

- [Chapter 2: æ—©æœŸåº”ç”¨ - GPTä¸BERT](../../../manuscript/01-foundation/early-applications.md)
- [Chapter 3: BERTç”Ÿæ€çš„ç¹è£](../../../manuscript/02-gpt-era/bert-ecosystem.md)

## æ—¶é—´çº¿ä½ç½® (Timeline Position)

**å‰ç½®äº‹ä»¶**:
- [Transformerè®ºæ–‡å‘è¡¨](transformer-paper-2017.md) (2017-06): æä¾›æ¶æ„åŸºç¡€
- [GPT-1å‘å¸ƒ](gpt1-release-2018.md) (2018-06): è¯æ˜é¢„è®­ç»ƒèŒƒå¼ (å•å‘)

**åç»­äº‹ä»¶**:
- [DistilBERTå‘å¸ƒ](distilbert-release-2019.md) (2019-10): æ¨¡å‹å‹ç¼©
- [RoBERTaå‘å¸ƒ](roberta-release-2019.md) (2019): è®­ç»ƒä¼˜åŒ–

**åŒæ—¶æœŸå¯¹æ¯”**:
- vs GPT-1 (2018-06): åŒå‘ vs å•å‘ï¼Œç†è§£ vs ç”Ÿæˆ

---

**Event Card Version**: 1.0
**Created**: 2025-10-17
**Last Updated**: 2025-10-17
**Verification Status**: âœ… Verified from academic sources
