---
event_id: gpt1-2018-06
date: 2018-06-11
title: GPT-1å‘å¸ƒ (GPT-1 Release)
title_en: "Improving Language Understanding by Generative Pre-Training"
organization: OpenAI
event_type: model_release
significance_level: critical
verification_status: verified
sources:
  - radford2018gpt1
tags:
  - gpt-series
  - pre-training
  - language-model
  - decoder-only
causal_connections:
  - enabled_by: [transformer-2017-06]
  - enables: [gpt2-2019-02, gpt3-2020-05]
  - demonstrates: generative_pretraining_paradigm
technical_concepts:
  - generative-pretraining
  - decoder-only-transformer
  - unsupervised-pretraining
  - fine-tuning
chapter_id: 01-foundation
---

# GPT-1å‘å¸ƒ (2018-06-11)

**ğŸ”´ å…³é”®é‡Œç¨‹ç¢‘** | **Critical Milestone**

## äº‹ä»¶æ¦‚è¿° (Event Overview)

2018å¹´6æœˆ11æ—¥ï¼ŒOpenAIå‘å¸ƒè®ºæ–‡ã€ŠImproving Language Understanding by Generative Pre-Trainingã€‹(Radford et al., 2018)ï¼Œæå‡ºGPT-1æ¨¡å‹ã€‚è¿™æ˜¯é¦–æ¬¡æˆåŠŸå°†Transformer decoderæ¶æ„åº”ç”¨äºå¤§è§„æ¨¡è¯­è¨€å»ºæ¨¡ï¼ŒéªŒè¯äº†"é¢„è®­ç»ƒ-å¾®è°ƒ"èŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚

On June 11, 2018, OpenAI published the paper "Improving Language Understanding by Generative Pre-Training" (Radford et al., 2018), introducing GPT-1. This was the first successful application of the Transformer decoder architecture to large-scale language modeling, validating the "pre-training + fine-tuning" paradigm.

## æŠ€æœ¯åˆ›æ–° (Technical Innovation)

### æ ¸å¿ƒæ–¹æ³• (Core Methodology)

**Generative Pre-training + Fine-tuning**:
1. **Pre-trainingé˜¶æ®µ (Unsupervised)**:
   - åœ¨å¤§è§„æ¨¡æœªæ ‡æ³¨æ–‡æœ¬ä¸Šè¿›è¡Œè¯­è¨€å»ºæ¨¡
   - å­¦ä¹ é€šç”¨è¯­è¨€è¡¨ç¤º
   - å•å‘(left-to-right)è‡ªå›å½’è®­ç»ƒ

2. **Fine-tuningé˜¶æ®µ (Supervised)**:
   - åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå¾®è°ƒ
   - æœ€å°åŒ–æ ‡æ³¨æ•°æ®éœ€æ±‚
   - å¿«é€Ÿé€‚åº”ä¸åŒNLPä»»åŠ¡

### æ¨¡å‹è§„æ ¼ (Model Specifications)

**æ¶æ„ç»†èŠ‚ (Architecture Details)**:
- **å‚æ•°é‡**: 117M (1.17äº¿å‚æ•°)
- **å±‚æ•°**: 12å±‚Transformer decoder
- **æ³¨æ„åŠ›å¤´**: 12ä¸ª
- **éšè—ç»´åº¦**: 768
- **ä¸Šä¸‹æ–‡é•¿åº¦**: 512 tokens

**è®­ç»ƒæ•°æ® (Training Data)**:
- BooksCorpus: ~7,000æœ¬æœªå‡ºç‰ˆä¹¦ç±
- çº¦5GBæ–‡æœ¬æ•°æ®
- è®­ç»ƒtokenæ•°: ~10äº¿

### æ€§èƒ½çªç ´ (Performance Breakthrough)

**9ä¸ªNLPä»»åŠ¡ä¸Šçš„æˆæœ**:
- **è‡ªç„¶è¯­è¨€æ¨ç† (NLI)**: 8.9%ç»å¯¹æå‡
- **é—®ç­” (QA)**: 5.7åˆ†æå‡
- **è¯­ä¹‰ç›¸ä¼¼åº¦**: 5.5åˆ†æå‡
- **æ–‡æœ¬åˆ†ç±»**: æ˜¾è‘—æå‡

## å†å²æ„ä¹‰ (Historical Significance)

### èŒƒå¼éªŒè¯ (Paradigm Validation)

**è¯æ˜äº†ä¸¤ä¸ªå…³é”®å‡è®¾**:
1. **é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§**: å¤§è§„æ¨¡æ— ç›‘ç£é¢„è®­ç»ƒèƒ½å­¦åˆ°æœ‰ç”¨çš„è¯­è¨€è¡¨ç¤º
2. **Transformer decoderçš„æ½œåŠ›**: å•å‘è¯­è¨€æ¨¡å‹è¶³ä»¥æ”¯æ’‘å¤šç§NLPä»»åŠ¡

### ä¸åŒæ—¶æœŸå·¥ä½œçš„å¯¹æ¯” (Comparison with Contemporary Work)

**vs BERT (2018-10)**:
- GPT-1: å•å‘(left-to-right)ï¼Œç”Ÿæˆå¼
- BERT: åŒå‘ï¼Œæ©ç å¼
- **äº’è¡¥æ€§**: ä¸¤è€…å…±åŒéªŒè¯äº†é¢„è®­ç»ƒèŒƒå¼çš„ä»·å€¼

**vs ULMFiT (2018-12)**:
- ULMFiT: é¦–æ¬¡ç³»ç»ŸåŒ–æå‡ºé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼
- GPT-1: åœ¨Transformeræ¶æ„ä¸ŠæˆåŠŸå®ç°
- **å…±åŒè´¡çŒ®**: ç¡®ç«‹äº†ç°ä»£NLPçš„åŸºç¡€æ–¹æ³•è®º

## å½±å“åˆ†æ (Impact Analysis)

### çŸ­æœŸå½±å“ (Short-term Impact, 2018-2019)

**å­¦æœ¯ç•Œåå“**:
- éªŒè¯äº†Transformeråœ¨è¯­è¨€å»ºæ¨¡ä¸Šçš„æœ‰æ•ˆæ€§
- å¼•å‘äº†å¯¹é¢„è®­ç»ƒæ–¹æ³•çš„å¹¿æ³›ç ”ç©¶
- ä¸ºGPT-2çš„è§„æ¨¡åŒ–é“ºå¹³é“è·¯

**äº§ä¸šç•Œé‡‡ç”¨**:
- è¯æ˜äº†AIå…¬å¸å¯ä»¥é€šè¿‡é¢„è®­ç»ƒæ¨¡å‹é™ä½åº”ç”¨é—¨æ§›
- å¼€å§‹äº†"é¢„è®­ç»ƒæ¨¡å‹å³æœåŠ¡"çš„å•†ä¸šæ¨¡å¼æ¢ç´¢

### é•¿æœŸå½±å“ (Long-term Impact, 2018-2025)

**å¥ å®šGPTç³»åˆ—åŸºç¡€**:
- **GPT-2** (2019): 15äº¿å‚æ•°ï¼Œæ‰©å¤§10å€
- **GPT-3** (2020): 1750äº¿å‚æ•°ï¼ŒFew-shot learning
- **GPT-4** (2023): å¤šæ¨¡æ€ï¼Œæ¥è¿‘äººç±»ä¸“å®¶æ°´å¹³
- **GPT-5** (2025): ä¸šç•Œæœ€é«˜æ°´å¹³

**å¯å‘è¡Œä¸šæ–¹å‘**:
- è¯æ˜"è§„æ¨¡åŒ–"æ˜¯æå‡æ€§èƒ½çš„æœ‰æ•ˆè·¯å¾„
- å•å‘è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›è¢«åç»­å¹¿æ³›åº”ç”¨
- ä¸ºChatGPTç­‰åº”ç”¨å¥ å®šæŠ€æœ¯åŸºç¡€

## æŠ€æœ¯ç»†èŠ‚ (Technical Details)

### è®­ç»ƒæ–¹æ³• (Training Methodology)

**Pre-trainingç›®æ ‡**:
```
L_unsupervised = Î£ log P(token_i | token_1, ..., token_{i-1})
```
- æœ€å¤§åŒ–è¯­è¨€æ¨¡å‹å¯¹æ•°ä¼¼ç„¶
- å•å‘è‡ªå›å½’é¢„æµ‹

**Fine-tuningç›®æ ‡**:
```
L_supervised = Î£ log P(y | x_1, ..., x_m)
```
- ä»»åŠ¡ç‰¹å®šç›®æ ‡
- ä¿ç•™è¯­è¨€æ¨¡å‹ç›®æ ‡ä½œä¸ºè¾…åŠ©

### å±€é™æ€§ (Limitations)

**ç›¸æ¯”åç»­æ¨¡å‹**:
- **è§„æ¨¡è¾ƒå°**: 117M vs GPT-3çš„175B
- **ä¸Šä¸‹æ–‡çª—å£çŸ­**: 512 tokens vs GPT-4çš„32K+
- **å•å‘é™åˆ¶**: æ— æ³•åˆ©ç”¨åŒå‘ä¸Šä¸‹æ–‡ä¿¡æ¯ (BERTè§£å†³)

**å½“æ—¶çš„æŒ‘æˆ˜**:
- è®¡ç®—èµ„æºé™åˆ¶
- è®­ç»ƒæ•°æ®è§„æ¨¡æœ‰é™
- å¾®è°ƒç­–ç•¥éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–

## å› æœå…³ç³»é“¾ (Causal Chain)

### ç›´æ¥å¯ç”¨ (Directly Enabled)

**GPT-2 (2019-02)**:
- éªŒè¯äº†è§„æ¨¡åŒ–çš„ä»·å€¼
- GPT-1æˆåŠŸ â†’ OpenAIå†³å®šæ‰©å¤§è§„æ¨¡
- 15äº¿å‚æ•°ï¼Œæ€§èƒ½å¤§å¹…æå‡

**GPT-3 (2020-05)**:
- Few-shot learningèƒ½åŠ›çš„å‘ç°
- è¯æ˜è¶³å¤Ÿå¤§çš„æ¨¡å‹å¯ä»¥é€šè¿‡promptå®Œæˆä»»åŠ¡
- æ”¹å˜AIåº”ç”¨èŒƒå¼

### é—´æ¥å½±å“ (Indirect Influence)

**å¯¹æ•´ä¸ªè¡Œä¸šçš„å¯ç¤º**:
- è¯æ˜äº†Transformer decoderçš„ç”Ÿæˆèƒ½åŠ›
- ä¸ºä¸­å›½LLMå‘å±•æä¾›æŠ€æœ¯å‚è€ƒ (ERNIE, ChatGLMç­‰)
- æ¿€å‘äº†å¯¹å¤§è§„æ¨¡é¢„è®­ç»ƒçš„æŠ•èµ„

## éªŒè¯æ¥æº (Verification Sources)

**å­¦æœ¯è®ºæ–‡**:
- Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving Language Understanding by Generative Pre-Training. *OpenAI Technical Report*. https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf

**æ€§èƒ½æ•°æ®**:
- è®ºæ–‡ä¸­çš„benchmarkç»“æœ
- 9ä¸ªNLPä»»åŠ¡çš„è¯¦ç»†è¯„ä¼°

**ç¤¾åŒºéªŒè¯**:
- å¤šä¸ªç ”ç©¶å›¢é˜Ÿå¤ç°ç»“æœ
- å¼€æºç¤¾åŒºåŸºäºGPT-1æ¶æ„çš„å®ç°

## ç›¸å…³æ¦‚å¿µ (Related Concepts)

- [Generative Pre-training](../../concepts/generative-pretraining.md)
- [Decoder-only Transformer](../../concepts/decoder-only-transformer.md)
- [Fine-tuning](../../concepts/fine-tuning.md)
- [Language Modeling](../../concepts/language-modeling.md)

## ç›¸å…³ç« èŠ‚ (Related Chapters)

- [Chapter 2: æ—©æœŸåº”ç”¨ - GPTä¸BERT](../../../manuscript/01-foundation/early-applications.md)
- [Chapter 3: GPTè§„æ¨¡åŒ–](../../../manuscript/02-gpt-era/scaling-up.md)

## æ—¶é—´çº¿ä½ç½® (Timeline Position)

**å‰ç½®äº‹ä»¶**:
- [Transformerè®ºæ–‡å‘è¡¨](transformer-paper-2017.md) (2017-06): æä¾›æ¶æ„åŸºç¡€

**åç»­äº‹ä»¶**:
- [BERTå‘å¸ƒ](bert-release-2018.md) (2018-10): åŒå‘é¢„è®­ç»ƒæ–¹æ³•
- [GPT-2å‘å¸ƒ](gpt2-release-2019.md) (2019-02): è§„æ¨¡åŒ–éªŒè¯

**åŒæ—¶æœŸäº‹ä»¶**:
- ULMFiT (2018-12): åŒæœŸéªŒè¯é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼

---

**Event Card Version**: 1.0
**Created**: 2025-10-17
**Last Updated**: 2025-10-17
**Verification Status**: âœ… Verified from academic sources
