---
event_id: gpt3-2020-05
date: 2020-05-28
title: GPT-3å‘å¸ƒ (GPT-3 Release)
title_en: "Language Models are Few-Shot Learners"
organization: OpenAI
event_type: model_release
significance_level: critical
verification_status: verified
sources:
  - brown2020gpt3
tags:
  - gpt-series
  - few-shot-learning
  - scaling
  - in-context-learning
causal_connections:
  - enabled_by: [gpt2-2019-02]
  - enables: [chatgpt-2022-11, instructgpt-2022-03]
  - demonstrates: few_shot_learning_emergence
technical_concepts:
  - few-shot-learning
  - in-context-learning
  - emergence
  - scaling-laws
chapter_id: 02-gpt-era
---

# GPT-3å‘å¸ƒ (2020-05-28)

**ğŸ”´ å…³é”®é‡Œç¨‹ç¢‘** | **Critical Milestone**

## äº‹ä»¶æ¦‚è¿° (Event Overview)

2020å¹´5æœˆ28æ—¥ï¼ŒOpenAIå‘å¸ƒè®ºæ–‡ã€ŠLanguage Models are Few-Shot Learnersã€‹(Brown et al., 2020)ï¼Œæå‡ºGPT-3æ¨¡å‹ï¼Œæ‹¥æœ‰1750äº¿å‚æ•°ã€‚è¿™æ˜¯é¦–æ¬¡å±•ç°å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å¼ºå¤§few-shotå­¦ä¹ èƒ½åŠ›ï¼Œæ ‡å¿—ç€è§„æ¨¡åŒ–çš„èƒœåˆ©ï¼Œæ”¹å˜äº†AIåº”ç”¨èŒƒå¼ï¼šä»ä»»åŠ¡ç‰¹å®šæ¨¡å‹åˆ°é€šç”¨promptæ¥å£ã€‚

On May 28, 2020, OpenAI published the paper "Language Models are Few-Shot Learners" (Brown et al., 2020), introducing GPT-3 with 175 billion parameters. This was the first demonstration of powerful few-shot learning capabilities in large-scale language models, marking a triumph of scaling and transforming the AI application paradigm from task-specific models to general-purpose prompt interfaces.

## æŠ€æœ¯åˆ›æ–° (Technical Innovation)

### è§„æ¨¡åŒ–é£è·ƒ (Scaling Leap)

**æ¨¡å‹è§„æ ¼ (GPT-3 175B)**:
- **å‚æ•°é‡**: 175B (1750äº¿å‚æ•°)
- vs GPT-2çš„1.5B: **æ‰©å¤§117å€**
- vs GPT-1çš„117M: **æ‰©å¤§1500å€**
- **å±‚æ•°**: 96å±‚
- **æ³¨æ„åŠ›å¤´**: 96ä¸ª
- **éšè—ç»´åº¦**: 12,288
- **ä¸Šä¸‹æ–‡é•¿åº¦**: 2048 tokens

**è®­ç»ƒæ•°æ® (Common Crawl + ç²¾é€‰æ•°æ®é›†)**:
- Common Crawl: 410B tokens (60%)
- WebText2: 19B tokens (22%)
- Books1: 12B tokens (8%)
- Books2: 55B tokens (8%)
- Wikipedia: 3B tokens (3%)
- **æ€»è®¡**: ~500B tokens

**è®¡ç®—èµ„æº**:
- è®­ç»ƒæˆæœ¬: çº¦$4.6M (460ä¸‡ç¾å…ƒ)
- ç¡¬ä»¶: V100 GPUs (æ•°åƒå¼ )
- è®­ç»ƒæ—¶é—´: æ•°æœˆ

### Few-shot Learningçªç ´ (Few-shot Learning Breakthrough)

**ä¸‰ç§å­¦ä¹ æ¨¡å¼**:

**1. Zero-shot**:
```
Input: "Translate to French: Hello"
Output: "Bonjour"
```
- æ— ç¤ºä¾‹ï¼Œç›´æ¥æ‰§è¡Œä»»åŠ¡

**2. One-shot**:
```
Input: "Translate to French:
        English: Hello â†’ French: Bonjour
        English: How are you? â†’ French:"
Output: "Comment allez-vous?"
```
- å•ä¸ªç¤ºä¾‹ï¼Œå¿«é€Ÿå­¦ä¹ 

**3. Few-shot**:
```
Input: "Translate to French:
        English: Hello â†’ French: Bonjour
        English: Good morning â†’ French: Bonjour
        English: How are you? â†’ French:"
Output: "Comment allez-vous?"
```
- å¤šä¸ªç¤ºä¾‹(é€šå¸¸3-10ä¸ª)ï¼Œæœ€ä½³æ€§èƒ½

### In-context Learningèƒ½åŠ› (In-context Learning)

**å…³é”®å‘ç°**:
- **æ— éœ€æ¢¯åº¦æ›´æ–°**: ä»…é€šè¿‡promptä¸­çš„ç¤ºä¾‹å­¦ä¹ 
- **å³æ—¶é€‚åº”**: ä¸éœ€è¦å¾®è°ƒï¼Œç«‹å³æ‰§è¡Œæ–°ä»»åŠ¡
- **ä»»åŠ¡æ³›åŒ–**: å¯ä»¥å®Œæˆè®­ç»ƒæ—¶æœªè§è¿‡çš„ä»»åŠ¡ç»„åˆ

**æ”¹å˜AIåº”ç”¨èŒƒå¼**:
- **ä¹‹å‰**: æ”¶é›†æ•°æ® â†’ è®­ç»ƒæ¨¡å‹ â†’ éƒ¨ç½²
- **ä¹‹å**: ç¼–å†™prompt â†’ ç«‹å³ä½¿ç”¨
- **æ„ä¹‰**: é™ä½AIåº”ç”¨é—¨æ§›ï¼ŒåŠ é€Ÿè¿­ä»£å‘¨æœŸ

## æ€§èƒ½è¡¨ç° (Performance)

### Benchmarkç»“æœ

**è¯­è¨€å»ºæ¨¡**:
- Penn Tree Bank: 20.50 perplexity (SOTA)
- LAMBADA: 76.2% accuracy (vs ä¹‹å‰çš„68.0%)

**é—®ç­”ä»»åŠ¡**:
- TriviaQA: 71.2% (vs ä¹‹å‰çš„68.0%)
- Natural Questions: 29.9% (vs ä¹‹å‰çš„36.6%)

**ç¿»è¯‘ä»»åŠ¡**:
- WMT Frâ†’En: 25.2 BLEU (few-shot, vs ç›‘ç£çš„33.3)
- WMT Deâ†’En: 29.7 BLEU

**æ¨ç†ä»»åŠ¡**:
- SuperGLUE: 71.8 (vs äººç±»åŸºçº¿çš„89.8)
- PIQA: 81.0% (vs ä¹‹å‰çš„77.1%)

**ç®—æœ¯èƒ½åŠ›**:
- 2ä½åŠ æ³•: 100% accuracy
- 3ä½åŠ æ³•: 80% accuracy
- 4ä½åŠ æ³•: 25% accuracy
- 5ä½åŠ æ³•: 9% accuracy

### æ¶Œç°èƒ½åŠ› (Emergent Capabilities)

**è§„æ¨¡ä¾èµ–çš„èƒ½åŠ›**:
- **å°æ¨¡å‹ (<13B)**: å‡ ä¹æ— few-shotèƒ½åŠ›
- **ä¸­ç­‰æ¨¡å‹ (13B-175B)**: few-shotèƒ½åŠ›é€æ­¥æå‡
- **GPT-3 175B**: å¼ºå¤§çš„few-shotèƒ½åŠ›

**å…³é”®æ´å¯Ÿ**:
> "Performance increases smoothly with scale, suggesting that language model performance will continue to improve with larger models."

## å†å²æ„ä¹‰ (Historical Significance)

### Scaling LawséªŒè¯ (Scaling Laws Validation)

**Kaplan et al. (2020)é¢„æµ‹**:
- æ¨¡å‹æ€§èƒ½ä¸å‚æ•°é‡ã€æ•°æ®é‡ã€è®¡ç®—é‡å‘ˆå¹‚å¾‹å…³ç³»
- **GPT-3éªŒè¯**: 175Bå‚æ•°ç¡®å®å¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡
- **å¯ç¤º**: è§„æ¨¡åŒ–æ˜¯æå‡æ€§èƒ½çš„å¯é è·¯å¾„

### AIåº”ç”¨èŒƒå¼è½¬å˜ (AI Application Paradigm Shift)

**ä¹‹å‰çš„èŒƒå¼**:
```
ä»»åŠ¡1 â†’ æ”¶é›†æ•°æ®1 â†’ è®­ç»ƒæ¨¡å‹1 â†’ éƒ¨ç½²åº”ç”¨1
ä»»åŠ¡2 â†’ æ”¶é›†æ•°æ®2 â†’ è®­ç»ƒæ¨¡å‹2 â†’ éƒ¨ç½²åº”ç”¨2
```

**GPT-3ä¹‹å**:
```
æ‰€æœ‰ä»»åŠ¡ â†’ ç»Ÿä¸€GPT-3æ¨¡å‹ â†’ é€šè¿‡promptæ‰§è¡Œ
```

**æ„ä¹‰**:
- é™ä½AIåº”ç”¨å¼€å‘æˆæœ¬
- åŠ é€Ÿäº§å“è¿­ä»£é€Ÿåº¦
- å°å›¢é˜Ÿä¹Ÿèƒ½å¿«é€Ÿæ„å»ºAIåº”ç”¨

### OpenAIå•†ä¸šåŒ–è½¬æŠ˜ç‚¹

**API-onlyç­–ç•¥**:
- **ä¸å¼€æº**: ä»…é€šè¿‡APIæä¾›æœåŠ¡
- **ç†ç”±**: æˆæœ¬é«˜æ˜‚($4.6Mè®­ç»ƒ)ï¼Œéœ€è¦å•†ä¸šåŒ–å›æŠ¥
- **å½±å“**: OpenAIä»"Open"è½¬å‘é—­æºå•†ä¸šåŒ–

**GPT-3 API (2020å¹´6æœˆ)**:
- å€™è¡¥åå•åˆ¶åº¦
- æŒ‰tokenæ”¶è´¹æ¨¡å¼
- ä¸ºåç»­ChatGPTä»˜è´¹è®¢é˜…é“ºè·¯

## å½±å“åˆ†æ (Impact Analysis)

### çŸ­æœŸå½±å“ (2020-2022)

**å­¦æœ¯ç•Œ**:
- å¼•å‘å¯¹scaling lawsçš„æ·±å…¥ç ”ç©¶
- æ¢ç´¢few-shot learningçš„åŸç†
- ç ”ç©¶in-context learningçš„æœºåˆ¶

**äº§ä¸šç•Œ**:
- å¤§å‚ç«ç›¸è¿½èµ¶ (Google PaLM, Meta OPT, ç™¾åº¦ERNIE 3.0)
- APIç”Ÿæ€çˆ†å‘ (æ•°ç™¾ä¸ªåŸºäºGPT-3çš„åº”ç”¨)
- éªŒè¯äº†"æ¨¡å‹å³æœåŠ¡"å•†ä¸šæ¨¡å¼

### é•¿æœŸå½±å“ (2020-2025)

**ä¸ºChatGPTé“ºè·¯**:
- GPT-3è¯æ˜äº†åŸºç¡€æ¨¡å‹çš„å¼ºå¤§
- RLHF (InstructGPT, 2022) æå‡å¯¹é½
- ChatGPT (2022) = GPT-3.5 + RLHF + å¯¹è¯ä¼˜åŒ–

**å¼€æºvsé—­æºåˆ†åŒ–**:
- **é—­æº**: OpenAI GPT-3/4, Anthropic Claude
- **å¼€æº**: Meta LLaMA, Alibaba Qwen, äº‰å¤ºç”Ÿæ€ä¸»å¯¼æƒ

**AIæ°‘ä¸»åŒ–äº‰è®º**:
- æ”¯æŒè€…: APIé™ä½ä½¿ç”¨é—¨æ§›
- æ‰¹è¯„è€…: é—­æºé™åˆ¶ç ”ç©¶å’Œåˆ›æ–°
- ç°å®: ä¸¤ç§è·¯çº¿å¹¶å­˜ï¼Œå„æœ‰ä¼˜åŠ¿

## æŠ€æœ¯ç»†èŠ‚ (Technical Details)

### æ¨¡å‹å®¶æ— (Model Family)

**8ä¸ªä¸åŒè§„æ¨¡ç‰ˆæœ¬**:
- GPT-3 Small: 125M
- GPT-3 Medium: 350M
- GPT-3 Large: 760M
- GPT-3 XL: 1.3B
- GPT-3 2.7B
- GPT-3 6.7B
- GPT-3 13B
- GPT-3 175B

**ç”¨é€”**:
- ç ”ç©¶scaling laws
- éªŒè¯èƒ½åŠ›æ¶Œç°
- æˆæœ¬vsæ€§èƒ½æƒè¡¡

### è®­ç»ƒæŠ€æœ¯ (Training Techniques)

**æ··åˆç²¾åº¦è®­ç»ƒ**:
- FP16æƒé‡å’Œæ¿€æ´»
- FP32æ¢¯åº¦ç´¯ç§¯
- åŠ é€Ÿè®­ç»ƒï¼Œå‡å°‘å†…å­˜

**æ¨¡å‹å¹¶è¡Œ (Model Parallelism)**:
- 175Bå‚æ•°æ— æ³•è£…å…¥å•ä¸ªGPU
- è·¨å¤šä¸ªGPUåˆ†å¸ƒæ¨¡å‹
- éœ€è¦é«˜æ•ˆé€šä¿¡ä¼˜åŒ–

**æ•°æ®å¹¶è¡Œ (Data Parallelism)**:
- å¤šä¸ªæ•°æ®æ‰¹æ¬¡å¹¶è¡Œå¤„ç†
- æ¢¯åº¦èšåˆ

### å±€é™æ€§ (Limitations)

**æ€§èƒ½ä¸è¶³é¢†åŸŸ**:
- å¤æ‚æ¨ç†ä»»åŠ¡
- é•¿æ–‡æ¡£ç†è§£
- äº‹å®å‡†ç¡®æ€§ (å¹»è§‰é—®é¢˜)
- ç®—æœ¯å’Œç¬¦å·æ¨ç†

**èµ„æºæ¶ˆè€—**:
- æ¨ç†æˆæœ¬é«˜æ˜‚
- å»¶è¿Ÿè¾ƒé«˜
- éœ€è¦ä¸“ç”¨ç¡¬ä»¶

**å¯¹é½é—®é¢˜**:
- æœ‰æ¯’è¾“å‡º
- åè§é—®é¢˜
- æ— æ³•å¯é éµå¾ªæŒ‡ä»¤
- éœ€è¦InstructGPTè§£å†³

## å› æœå…³ç³»é“¾ (Causal Chain)

### ç›´æ¥å¯ç”¨ (Directly Enabled)

**InstructGPT (2022-03)**:
- GPT-3 + RLHFå¯¹é½
- æå‡æŒ‡ä»¤éµå¾ªèƒ½åŠ›
- å‡å°‘æœ‰å®³è¾“å‡º

**ChatGPT (2022-11)**:
- GPT-3.5 (GPT-3å¾®è°ƒç‰ˆ) + RLHF + å¯¹è¯ä¼˜åŒ–
- å¼•çˆ†å…¨çƒAIçƒ­æ½®
- æ”¹å˜AIäº§å“å½¢æ€

**GPT-3 APIç”Ÿæ€**:
- Jasper (AIå†™ä½œ)
- Copy.ai (è¥é”€æ–‡æ¡ˆ)
- GitHub Copilot (ä»£ç è¡¥å…¨)
- æ•°ç™¾ä¸ªAIåº”ç”¨

### é—´æ¥å½±å“ (Indirect Influence)

**ç«äº‰å¯¹æ‰‹è·Ÿè¿›**:
- **Google PaLM** (540B, 2022): è¶…è¶ŠGPT-3è§„æ¨¡
- **Meta OPT** (175B, 2022): å¼€æºå¤ç°GPT-3
- **ç™¾åº¦ERNIE 3.0** (10B, 2021): ä¸­æ–‡å¤§æ¨¡å‹
- **é˜¿é‡ŒM6** (10Tç¨€ç–, 2021): å¤šæ¨¡æ€è¶…å¤§æ¨¡å‹

**å¼€æºç¤¾åŒºåŠªåŠ›**:
- **EleutherAI GPT-J/GPT-NeoX**: å¼€æºå¤ç°
- **BLOOM** (176B, 2022): å¤šè¯­è¨€å¼€æºæ¨¡å‹
- æ¨åŠ¨AIæ°‘ä¸»åŒ–

## éªŒè¯æ¥æº (Verification Sources)

**å­¦æœ¯è®ºæ–‡**:
- Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language Models are Few-Shot Learners. *Advances in Neural Information Processing Systems (NeurIPS) 33*, 1877-1901. https://arxiv.org/abs/2005.14165

**OpenAIå®˜æ–¹åšå®¢**:
- "OpenAI API" (2020-06-11)
- "GPT-3 Powers the Next Generation of Apps" (2021-03-25)

**åª’ä½“æŠ¥é“**:
- MIT Technology Review: "OpenAI's new language generator GPT-3 is shockingly good"
- The Guardian: "A robot wrote this entire article. Are you scared yet, human?"

**ç¤¾åŒºéªŒè¯**:
- æ•°åƒä¸ªåŸºäºGPT-3 APIçš„åº”ç”¨
- å­¦æœ¯ç•Œå¤§é‡åŸºäºGPT-3çš„ç ”ç©¶

## ç›¸å…³æ¦‚å¿µ (Related Concepts)

- [Few-shot Learning](../../concepts/few-shot-learning.md)
- [In-context Learning](../../concepts/in-context-learning.md)
- [Emergence](../../concepts/emergence.md)
- [Scaling Laws](../../concepts/scaling-laws.md)

## ç›¸å…³ç« èŠ‚ (Related Chapters)

- [Chapter 3: GPTè§„æ¨¡åŒ–](../../../manuscript/02-gpt-era/scaling-up.md)
- [Chapter 4: Few-shot Learningçš„å‘ç°](../../../manuscript/02-gpt-era/few-shot-discovery.md)

## æ—¶é—´çº¿ä½ç½® (Timeline Position)

**å‰ç½®äº‹ä»¶**:
- [GPT-2å‘å¸ƒ](gpt2-release-2019.md) (2019-02): éªŒè¯è§„æ¨¡åŒ–æ–¹å‘
- [Scaling Lawsè®ºæ–‡](scaling-laws-2020.md) (2020-01): ç†è®ºé¢„æµ‹

**åç»­äº‹ä»¶**:
- [Codexå‘å¸ƒ](codex-release-2021.md) (2021-07): GPT-3ä»£ç å¾®è°ƒ
- [InstructGPTå‘å¸ƒ](instructgpt-2022.md) (2022-03): RLHFå¯¹é½
- [ChatGPTå‘å¸ƒ](chatgpt-launch-2022.md) (2022-11): å¯¹è¯åº”ç”¨

**åŒæ—¶æœŸäº‹ä»¶**:
- [Switch Transformer](switch-transformer-2020.md) (2020-01): MoEæ¶æ„ï¼Œ1.6Tå‚æ•°

---

**Event Card Version**: 1.0
**Created**: 2025-10-17
**Last Updated**: 2025-10-17
**Verification Status**: âœ… Verified from academic sources and official announcements
