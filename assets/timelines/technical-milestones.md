# æŠ€æœ¯é‡Œç¨‹ç¢‘æ—¶é—´çº¿ (2017-2025)

**Focus**: å…³é”®æŠ€æœ¯åˆ›æ–°å’Œæ¶æ„çªç ´ï¼Œè€Œéç‰¹å®šæ¨¡å‹å‘å¸ƒ
**Coverage**: ä»Transformeråˆ°æ¨ç†æ¨¡å‹ï¼Œæ¶µç›–æ¶æ„ã€è®­ç»ƒæ–¹æ³•ã€å¯¹é½æŠ€æœ¯ã€å¤šæ¨¡æ€ç­‰

---

## æŠ€æœ¯æ¼”è¿›è·¯çº¿å›¾

```
æ¶æ„åˆ›æ–°
â”œâ”€â”€ 2017: Transformer (Self-Attention)
â”œâ”€â”€ 2019: Sparse Transformer, Reformer
â”œâ”€â”€ 2020: Switch Transformer (MoE)
â”œâ”€â”€ 2024: Hybrid-Mamba-Transformer
â””â”€â”€ 2025: ä¸‡äº¿å‚æ•°MoEæ¶æ„

é¢„è®­ç»ƒæ–¹æ³•
â”œâ”€â”€ 2018: GPT (å•å‘), BERT (åŒå‘), ULMFiT
â”œâ”€â”€ 2019: T5 (Unified text-to-text)
â”œâ”€â”€ 2020: Few-shot learning (GPT-3)
â”œâ”€â”€ 2021: Instruction tuning
â””â”€â”€ 2022: RLHF (InstructGPT)

è§„æ¨¡åŒ–æ³•åˆ™
â”œâ”€â”€ 2020: Scaling Laws (Kaplan et al.)
â”œâ”€â”€ 2022: Chinchilla Scaling (Training Compute Optimal)
â””â”€â”€ 2024: MoE Scaling

å¯¹é½æŠ€æœ¯
â”œâ”€â”€ 2022: RLHF (Reinforcement Learning from Human Feedback)
â”œâ”€â”€ 2023: Constitutional AI (Anthropic)
â”œâ”€â”€ 2024: Preference Optimization (DPO)
â””â”€â”€ 2025: Self-Playå¯¹é½

æ¨ç†èƒ½åŠ›
â”œâ”€â”€ 2022: Chain-of-Thought Prompting
â”œâ”€â”€ 2024: o1 Thinking Models
â””â”€â”€ 2025: Deep Reasoning (DeepSeek R1, Hunyuan T1)

å¤šæ¨¡æ€
â”œâ”€â”€ 2021: CLIP (Vision-Language), DALL-E
â”œâ”€â”€ 2022: Flamingo (Visual QA)
â”œâ”€â”€ 2024: GPT-4V, Gemini (çœŸæ­£å¤šæ¨¡æ€)
â”œâ”€â”€ 2024: Sora (Video Generation)
â””â”€â”€ 2025: å…¨æ¨¡æ€ç»Ÿä¸€å¤„ç†

æ•ˆç‡ä¼˜åŒ–
â”œâ”€â”€ 2019: Model Compression (DistilBERT)
â”œâ”€â”€ 2020: MoE (Switch Transformer)
â”œâ”€â”€ 2023: Flash Attention
â”œâ”€â”€ 2024: Quantization (4-bit, 1-bit)
â””â”€â”€ 2025: æˆæœ¬é©å‘½ (Doubao, Hunyuan T1)
```

---

## 1. æ¶æ„åˆ›æ–° (Architecture Innovations)

### ğŸ”´ 2017-06: Transformeræ¶æ„
**åˆ›æ–°**: Self-Attentionæœºåˆ¶ï¼Œå®Œå…¨æŠ›å¼ƒRNN/LSTM
**è®ºæ–‡**: "Attention is All You Need" (Vaswani et al., Google Brain)
**å½±å“**: æ‰€æœ‰åç»­å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€æ¶æ„
**å…³é”®æŠ€æœ¯**:
- Multi-head self-attention
- Positional encoding
- Layer normalization
- Residual connections

**ä¸ºä»€ä¹ˆé©å‘½æ€§**:
- å¹¶è¡ŒåŒ–è®­ç»ƒï¼ˆvs RNNçš„åºåˆ—åŒ–ï¼‰
- é•¿è·ç¦»ä¾èµ–å»ºæ¨¡èƒ½åŠ›
- å¯æ‰©å±•æ€§å¼ºï¼ˆé€‚åˆè¶…å¤§è§„æ¨¡ï¼‰

---

### ğŸ”µ 2019-01: Sparse Transformer
**åˆ›æ–°**: Sparse attention patternsï¼Œé™ä½è®¡ç®—å¤æ‚åº¦
**ç»„ç»‡**: OpenAI
**å½±å“**: å¤„ç†æ›´é•¿åºåˆ—æˆä¸ºå¯èƒ½

---

### ğŸ”µ 2020-01: Switch Transformer (MoE)
**åˆ›æ–°**: Mixture of Expertsæ¶æ„ï¼Œç¨€ç–æ¿€æ´»
**ç»„ç»‡**: Google
**è§„æ¨¡**: é¦–ä¸ªä¸‡äº¿å‚æ•°æ¨¡å‹
**å½±å“**: è¯æ˜MoEæ¶æ„å¯ä»¥æå¤§æ‰©å±•æ¨¡å‹å®¹é‡è€Œä¸æˆæ¯”ä¾‹å¢åŠ è®¡ç®—

---

### ğŸ”µ 2024-12: DeepSeek-V3 MoE
**åˆ›æ–°**: 671Bæ€»å‚æ•°ï¼Œ37Bæ¿€æ´»å‚æ•°
**ç»„ç»‡**: DeepSeek
**å½±å“**: MoEæ¶æ„çš„æè‡´ä¼˜åŒ–ï¼Œæˆæœ¬æ•ˆç‡å¤§å¹…æå‡

---

### ğŸ”´ 2025-02: Hybrid-Mamba-Transformer
**åˆ›æ–°**: é¦–æ¬¡æˆåŠŸå°†Mambaæ¶æ„åº”ç”¨äºè¶…å¤§è§„æ¨¡MoEæ¨¡å‹
**ç»„ç»‡**: Tencent (Hunyuan Turbo S)
**å½±å“**: é™ä½Transformerè®¡ç®—å¤æ‚åº¦å’ŒKV-Cacheå ç”¨
**æ€§èƒ½**: MMLU 89.5ï¼Œè¶…è¶ŠGPT-4oå’ŒDeepSeek V3

---

### ğŸ”µ 2025-09: ä¸‡äº¿å‚æ•°æ—¶ä»£
**åˆ›æ–°**: Qwen3-Max 1T+ å‚æ•°ï¼Œ262Kä¸Šä¸‹æ–‡
**ç»„ç»‡**: Alibaba
**å½±å“**: å‚æ•°è§„æ¨¡çªç ´ä¸‡äº¿ï¼Œé—­æºæ——èˆ°ä¸GPT-5ç«äº‰

---

## 2. é¢„è®­ç»ƒæ–¹æ³• (Pre-training Methods)

### ğŸ”´ 2018-06: GPTé¢„è®­ç»ƒèŒƒå¼
**åˆ›æ–°**: Generative Pre-training + Fine-tuning
**è®ºæ–‡**: "Improving Language Understanding by Generative Pre-Training" (Radford et al., OpenAI)
**æ–¹æ³•**: å•å‘è¯­è¨€å»ºæ¨¡ï¼ˆleft-to-rightï¼‰
**å½±å“**: è¯æ˜é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼çš„æœ‰æ•ˆæ€§

---

### ğŸ”´ 2018-10: BERTåŒå‘é¢„è®­ç»ƒ
**åˆ›æ–°**: Bidirectional Encoder Representations from Transformers
**è®ºæ–‡**: Devlin et al., Google
**æ–¹æ³•**: Masked Language Modeling (MLM) + Next Sentence Prediction (NSP)
**å½±å“**: åŒå‘ä¸Šä¸‹æ–‡ç†è§£ï¼Œåˆ·æ–°11ä¸ªNLPä»»åŠ¡SOTA

---

### ğŸ”µ 2018-12: ULMFiT
**åˆ›æ–°**: Universal Language Model Fine-tuning
**ç»„ç»‡**: fast.ai (Jeremy Howard, Sebastian Ruder)
**æ–¹æ³•**: è¿ç§»å­¦ä¹ ä¸‰é˜¶æ®µï¼ˆé¢„è®­ç»ƒâ†’é¢†åŸŸé€‚åº”â†’ä»»åŠ¡å¾®è°ƒï¼‰
**å½±å“**: é¦–æ¬¡ç³»ç»ŸåŒ–æå‡ºé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼

---

### ğŸ”µ 2019-10: T5ç»Ÿä¸€æ¡†æ¶
**åˆ›æ–°**: Text-to-Text Transfer Transformer
**è®ºæ–‡**: Raffel et al., Google
**æ–¹æ³•**: æ‰€æœ‰NLPä»»åŠ¡ç»Ÿä¸€ä¸ºtext-to-textæ ¼å¼
**å½±å“**: ç®€åŒ–æ¨¡å‹è®¾è®¡ï¼Œç»Ÿä¸€è®­ç»ƒæµç¨‹

---

### ğŸ”´ 2020-05: Few-shot Learning
**åˆ›æ–°**: GPT-3å±•ç°å¼ºå¤§few-shotèƒ½åŠ›
**è®ºæ–‡**: "Language Models are Few-Shot Learners" (Brown et al., OpenAI)
**æ–¹æ³•**: In-context learningï¼Œæ— éœ€fine-tuning
**å½±å“**: æ”¹å˜AIåº”ç”¨èŒƒå¼ï¼Œä»ä»»åŠ¡ç‰¹å®šæ¨¡å‹åˆ°é€šç”¨æ¨¡å‹

---

### ğŸ”µ 2021-09: Instruction Tuning
**åˆ›æ–°**: Flan (Finetuned Language Net)
**ç»„ç»‡**: Google
**æ–¹æ³•**: åœ¨å¤šæ ·åŒ–instructionæ•°æ®ä¸Šå¾®è°ƒ
**å½±å“**: æå‡æ¨¡å‹æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œä¸ºChatGPTé“ºè·¯

---

### ğŸ”´ 2022-03: RLHFå¯¹é½
**åˆ›æ–°**: Reinforcement Learning from Human Feedback
**è®ºæ–‡**: "Training language models to follow instructions with human feedback" (Ouyang et al., OpenAI)
**æ–¹æ³•**: äººç±»åå¥½ â†’ å¥–åŠ±æ¨¡å‹ â†’ PPOä¼˜åŒ–
**å½±å“**: ChatGPTçš„å…³é”®æŠ€æœ¯ï¼Œæ¨¡å‹å¯¹é½çš„é‡Œç¨‹ç¢‘

---

## 3. è§„æ¨¡åŒ–æ³•åˆ™ (Scaling Laws)

### ğŸ”´ 2020-01: Scaling Laws
**è®ºæ–‡**: "Scaling Laws for Neural Language Models" (Kaplan et al., OpenAI)
**å‘ç°**:
- æ¨¡å‹æ€§èƒ½ä¸æ¨¡å‹å¤§å°ã€æ•°æ®é‡ã€è®¡ç®—é‡å‘ˆå¹‚å¾‹å…³ç³»
- æ›´å¤§çš„æ¨¡å‹é€šå¸¸æ›´sample-efficient
**å½±å“**: æŒ‡å¯¼GPT-3è§„æ¨¡åŒ–å†³ç­–

---

### ğŸ”´ 2022-03: Chinchilla Scaling Laws
**è®ºæ–‡**: "Training Compute-Optimal Large Language Models" (Hoffmann et al., DeepMind)
**å‘ç°**:
- ä¹‹å‰çš„æ¨¡å‹è®­ç»ƒä¸è¶³ï¼ˆundertrainedï¼‰
- 700äº¿å‚æ•°Chinchillaè¶…è¿‡2800äº¿å‚æ•°Gopher
- æœ€ä¼˜ç­–ç•¥ï¼šæ¨¡å‹å¤§å°å’Œè®­ç»ƒæ•°æ®é‡åŒæ­¥å¢é•¿
**å½±å“**: æ”¹å˜è®­ç»ƒç­–ç•¥ï¼Œæ›´æ³¨é‡æ•°æ®è´¨é‡å’Œæ•°é‡

---

### ğŸ”µ 2024: MoE Scaling Laws
**å‘ç°**: MoEæ¶æ„å¯ä»¥æ‰©å±•æ€»å‚æ•°é‡è€Œä¿æŒæ¿€æ´»å‚æ•°å¯æ§
**å®è·µ**: DeepSeek-V3 (671Bæ€»å‚æ•°, 37Bæ¿€æ´»)
**å½±å“**: å®ç°æ›´å¤§å®¹é‡å’Œæ›´ä½æˆæœ¬çš„å¹³è¡¡

---

## 4. å¯¹é½æŠ€æœ¯ (Alignment Techniques)

### ğŸ”´ 2022-03: RLHF (Reinforcement Learning from Human Feedback)
**ç»„ç»‡**: OpenAI (InstructGPT)
**æ­¥éª¤**:
1. ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰
2. å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼ˆRMï¼‰
3. PPOå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–

**å½±å“**: ChatGPTæ ¸å¿ƒæŠ€æœ¯ï¼Œä½¿æ¨¡å‹å®‰å…¨ã€æœ‰å¸®åŠ©ã€è¯šå®

---

### ğŸ”µ 2023-05: Constitutional AI
**ç»„ç»‡**: Anthropic (Claude)
**æ–¹æ³•**:
- Self-critique and revision
- åŸºäºå®ªæ³•åŸåˆ™çš„å¯¹é½
- å‡å°‘äººç±»æ ‡æ³¨ä¾èµ–

**å½±å“**: AIå®‰å…¨çš„æ–°èŒƒå¼

---

### ğŸ”µ 2024: DPO (Direct Preference Optimization)
**åˆ›æ–°**: ç®€åŒ–RLHFï¼Œç›´æ¥ä¼˜åŒ–åå¥½
**æ–¹æ³•**: æ— éœ€æ˜¾å¼å¥–åŠ±æ¨¡å‹
**å½±å“**: é™ä½å¯¹é½è®­ç»ƒå¤æ‚åº¦

---

### ğŸ”µ 2025: Self-Play Alignment
**ç»„ç»‡**: DeepSeek R1
**æ–¹æ³•**: çº¯å¼ºåŒ–å­¦ä¹ è‡ªæˆ‘å¯¹å¼ˆ
**å½±å“**: å‡å°‘äººç±»ç›‘ç£ï¼Œæ¨¡å‹è‡ªæˆ‘æ”¹è¿›

---

## 5. æ¨ç†èƒ½åŠ› (Reasoning Capabilities)

### ğŸ”µ 2022-01: Chain-of-Thought Prompting
**è®ºæ–‡**: "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" (Wei et al., Google)
**æ–¹æ³•**: åœ¨promptä¸­å±•ç¤ºæ¨ç†æ­¥éª¤
**å½±å“**: æ˜¾è‘—æå‡å¤æ‚æ¨ç†ä»»åŠ¡æ€§èƒ½

---

### ğŸ”´ 2024-09: Thinking Models (o1)
**ç»„ç»‡**: OpenAI
**åˆ›æ–°**: å†…ç½®æ€è€ƒé“¾ï¼Œæ¨¡å‹è‡ªä¸»è¿›è¡Œå¤šæ­¥æ¨ç†
**æ–¹æ³•**: å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨ç†è¿‡ç¨‹
**æ€§èƒ½**: æ•°å­¦ã€ç¼–ç¨‹æ¨ç†èƒ½åŠ›å¤§å¹…æå‡ï¼ˆAIME 2024: 83.3%ï¼‰
**å½±å“**: æ–°çš„æ¨¡å‹èŒƒå¼ï¼Œä»ç›´æ¥è¾“å‡ºåˆ°æ·±åº¦æ€è€ƒ

---

### ğŸ”´ 2025-01: DeepSeek R1 (å¼€æºæ¨ç†æ¨¡å‹)
**åˆ›æ–°**: å¼€æºæ¨ç†æ¨¡å‹ï¼Œæ€§èƒ½æ¥è¿‘o1
**æ–¹æ³•**: çº¯RLè‡ªæˆ‘å¯¹å¼ˆï¼Œæ— ç›‘ç£å­¦ä¹ 
**æ€§èƒ½**: AIME 2024: 79.8%, Math-500: 97.3%
**å½±å“**: è¯æ˜ä¸­å›½åœ¨æ¨ç†æ¨¡å‹ä¸Šçš„çªç ´ï¼ŒèŠ¯ç‰‡é™åˆ¶ä¸‹ä»å¯åˆ›æ–°

---

### ğŸ”µ 2025-03: Hunyuan T1 (æˆæœ¬æ•ˆç‡)
**ç»„ç»‡**: Tencent
**åˆ›æ–°**: æ€§èƒ½ä¸DeepSeek R1ç›¸å½“ï¼Œæˆæœ¬ä»…1/4
**å½±å“**: æ¨ç†æ¨¡å‹çš„æˆæœ¬é©å‘½

---

### ğŸ”µ 2025-05: DeepSeek R1-0528
**åˆ›æ–°**: AIME 2025è¾¾åˆ°87.5%ï¼ŒCodeforces Elo ~1930
**å½±å“**: æ¨ç†èƒ½åŠ›æŒç»­æå‡ï¼Œæ¥è¿‘äººç±»é¡¶å°–ç«èµ›é€‰æ‰‹æ°´å¹³

---

## 6. å¤šæ¨¡æ€ (Multimodal)

### ğŸ”µ 2021-02: DALL-E
**ç»„ç»‡**: OpenAI
**åˆ›æ–°**: æ–‡æœ¬ç”Ÿæˆå›¾åƒ
**å½±å“**: å¤šæ¨¡æ€ç”Ÿæˆçš„å¼€ç«¯

---

### ğŸ”µ 2021-09: CLIP
**ç»„ç»‡**: OpenAI
**åˆ›æ–°**: å¯¹æ¯”å­¦ä¹ çš„è§†è§‰-è¯­è¨€é¢„è®­ç»ƒ
**å½±å“**: å›¾æ–‡ç†è§£çš„ç»Ÿä¸€è¡¨ç¤ºï¼Œzero-shotå›¾åƒåˆ†ç±»

---

### ğŸ”µ 2022-04: Flamingo
**ç»„ç»‡**: DeepMind
**åˆ›æ–°**: Few-shot visual question answering
**å½±å“**: è§†è§‰è¯­è¨€ç†è§£çš„çªç ´

---

### ğŸ”´ 2023-09: GPT-4V
**ç»„ç»‡**: OpenAI
**åˆ›æ–°**: GPT-4 with Visionï¼ŒçœŸæ­£çš„å¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹
**å½±å“**: å›¾æ–‡ç†è§£è¾¾åˆ°æ–°é«˜åº¦

---

### ğŸ”´ 2024-02: Sora
**ç»„ç»‡**: OpenAI
**åˆ›æ–°**: æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ï¼ˆæœ€é•¿60ç§’ï¼‰
**å½±å“**: è§†é¢‘ç”Ÿæˆçš„é‡å¤§çªç ´

---

### ğŸ”µ 2024-05: GPT-4o
**ç»„ç»‡**: OpenAI
**åˆ›æ–°**: å…¨æ¨¡æ€å®æ—¶äº¤äº’ï¼ˆæ–‡æœ¬ã€éŸ³é¢‘ã€è§†è§‰ç»Ÿä¸€å¤„ç†ï¼‰
**å½±å“**: å¤šæ¨¡æ€ç»Ÿä¸€ï¼Œå®æ—¶å“åº”

---

### ğŸ”µ 2024-12: Gemini 2.0
**ç»„ç»‡**: Google
**åˆ›æ–°**: åŸç”Ÿå¤šæ¨¡æ€ï¼Œthinking mode
**å½±å“**: ä¸OpenAIç«äº‰å¤šæ¨¡æ€é¢†å¯¼åœ°ä½

---

## 7. æ•ˆç‡ä¼˜åŒ– (Efficiency Optimization)

### ğŸ”µ 2019-10: DistilBERT
**ç»„ç»‡**: Hugging Face
**æ–¹æ³•**: çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰
**æ•ˆæœ**: å‚æ•°å‡å°‘40%ï¼Œé€Ÿåº¦æå‡60%ï¼Œä¿æŒ97%æ€§èƒ½
**å½±å“**: æ¨¡å‹å‹ç¼©å’Œéƒ¨ç½²ä¼˜åŒ–æ–¹å‘

---

### ğŸ”µ 2020-01: MoE (Mixture of Experts)
**æ–¹æ³•**: ç¨€ç–æ¿€æ´»ï¼Œåªæ¿€æ´»éƒ¨åˆ†ä¸“å®¶
**æ•ˆæœ**: æ‰©å±•å®¹é‡è€Œä¸æˆæ¯”ä¾‹å¢åŠ è®¡ç®—
**ä»£è¡¨**: Switch Transformer, DeepSeek-V2/V3

---

### ğŸ”µ 2023-06: Flash Attention
**åˆ›æ–°**: I/Oä¼˜åŒ–çš„attentionç®—æ³•
**æ•ˆæœ**: è®­ç»ƒé€Ÿåº¦æå‡2-4å€ï¼Œå†…å­˜å ç”¨å‡å°‘
**å½±å“**: ä½¿é•¿ä¸Šä¸‹æ–‡è®­ç»ƒæˆä¸ºå¯èƒ½

---

### ğŸ”µ 2024: é‡åŒ–æŠ€æœ¯ (Quantization)
**æ–¹æ³•**: 4-bit, 1-bit quantization
**ä»£è¡¨**: BitNet (1-bit LLM)
**æ•ˆæœ**: æ¨ç†æˆæœ¬å¤§å¹…é™ä½
**å½±å“**: è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²æˆä¸ºå¯èƒ½

---

### ğŸ”´ 2025-07: æˆæœ¬é©å‘½
**ä»£è¡¨**:
- **Doubao 1.6**: æˆæœ¬é™ä½70%ï¼Œæ—¥å‡4ä¸‡äº¿tokens
- **Hunyuan T1**: æˆæœ¬ä»…DeepSeek 1/4
**å½±å“**: AIåº”ç”¨æˆæœ¬é—¨æ§›å¤§å¹…é™ä½

---

## 8. é•¿ä¸Šä¸‹æ–‡ (Long Context)

### ğŸ”µ 2023-07: Claude 2 (100K context)
**ç»„ç»‡**: Anthropic
**å½±å“**: é•¿æ–‡æ¡£å¤„ç†èƒ½åŠ›çªç ´

---

### ğŸ”µ 2023-09: Claude 2.1 (200K context)
**ç»„ç»‡**: Anthropic
**å½±å“**: è¿›ä¸€æ­¥æ‰©å±•é•¿ä¸Šä¸‹æ–‡

---

### ğŸ”´ 2024-02: Gemini 1.5 (1M context)
**ç»„ç»‡**: Google
**åˆ›æ–°**: ç™¾ä¸‡tokenä¸Šä¸‹æ–‡çª—å£
**å½±å“**: é•¿ä¸Šä¸‹æ–‡å¤„ç†çš„é‡å¤§çªç ´ï¼Œå¯å¤„ç†æ•´æœ¬ä¹¦

---

### ğŸ”µ 2024-12: Gemini 2.5 (10M context)
**ç»„ç»‡**: Google
**åˆ›æ–°**: åƒä¸‡tokenä¸Šä¸‹æ–‡
**å½±å“**: ç†è®ºä¸Šå¯å¤„ç†è¶…å¤§ä»£ç åº“æˆ–çŸ¥è¯†åº“

---

### ğŸ”µ 2025-09: Qwen3-Max (262K context with caching)
**ç»„ç»‡**: Alibaba
**åˆ›æ–°**: æ”¯æŒä¸Šä¸‹æ–‡ç¼“å­˜çš„è¶…é•¿ä¸Šä¸‹æ–‡
**å½±å“**: å®ç”¨æ€§ä¼˜åŒ–

---

## æŠ€æœ¯æ¼”è¿›è¶‹åŠ¿åˆ†æ

### è¶‹åŠ¿ 1: è§„æ¨¡åŒ–æŒç»­ä½†æ›´æ³¨é‡æ•ˆç‡
- 2020: è¿½æ±‚å‚æ•°é‡ï¼ˆGPT-3 175Bï¼‰
- 2022: ä¼˜åŒ–è®­ç»ƒæ•°æ®ï¼ˆChinchillaï¼‰
- 2024: MoEæ¶æ„ï¼ˆDeepSeek-V3 671Bæ€»å‚æ•°, 37Bæ¿€æ´»ï¼‰
- 2025: æˆæœ¬é©å‘½ï¼ˆDoubao, Hunyuané™ä»·70%ï¼‰

### è¶‹åŠ¿ 2: ä»å¯¹è¯åˆ°æ¨ç†
- 2022: ChatGPTï¼ˆå¯¹è¯èƒ½åŠ›ï¼‰
- 2024: o1ï¼ˆæ¨ç†èƒ½åŠ›ï¼‰
- 2025: DeepSeek R1ï¼ˆå¼€æºæ¨ç†ï¼‰ï¼Œå¤šå®¶è·Ÿè¿›

### è¶‹åŠ¿ 3: å¤šæ¨¡æ€ç»Ÿä¸€
- 2021: å•æ¨¡æ€ç”Ÿæˆï¼ˆDALL-Eï¼‰
- 2023: å›¾æ–‡ç†è§£ï¼ˆGPT-4Vï¼‰
- 2024: å…¨æ¨¡æ€å®æ—¶äº¤äº’ï¼ˆGPT-4oï¼‰ï¼Œè§†é¢‘ç”Ÿæˆï¼ˆSoraï¼‰
- 2025: åŸç”Ÿå¤šæ¨¡æ€æˆä¸ºæ ‡é…

### è¶‹åŠ¿ 4: é•¿ä¸Šä¸‹æ–‡å†›å¤‡ç«èµ›
- 2023: 100K â†’ 200K (Claude)
- 2024: 1M (Gemini 1.5) â†’ 10M (Gemini 2.5)
- 2025: å®ç”¨æ€§ä¼˜åŒ–ï¼ˆç¼“å­˜ã€æ£€ç´¢å¢å¼ºï¼‰

### è¶‹åŠ¿ 5: å¼€æºvsé—­æºæŠ€æœ¯è·¯çº¿åˆ†åŒ–
- **é—­æº**: æŠ€æœ¯é¢†å…ˆï¼Œå¤šæ¨¡æ€ï¼Œæ¨ç†æ¨¡å‹
- **å¼€æº**: å¿«é€Ÿè¿­ä»£ï¼Œæˆæœ¬æ•ˆç‡ï¼Œç”Ÿæ€å»ºè®¾
- **æ··åˆ**: å¼€æºå°æ¨¡å‹ + é—­æºæ——èˆ°

---

## æ ¸å¿ƒæŠ€æœ¯æ¦‚å¿µæ€»è§ˆ

**å·²è¯†åˆ«æ ¸å¿ƒæ¦‚å¿µ** (20+):
1. Transformer
2. Self-Attention
3. Positional Encoding
4. Multi-head Attention
5. Pre-training
6. Fine-tuning
7. Transfer Learning
8. Bidirectional Encoding (BERT)
9. Masked Language Modeling (MLM)
10. Few-shot Learning
11. In-context Learning
12. Instruction Tuning
13. RLHF (Reinforcement Learning from Human Feedback)
14. Scaling Laws
15. Mixture of Experts (MoE)
16. Chain-of-Thought
17. Thinking Models / Reasoning Models
18. Constitutional AI
19. Multimodal Learning
20. Knowledge Distillation
21. Quantization
22. Flash Attention
23. Long Context Processing
24. Text-to-Text Framework
25. Sparse Transformers

**å‚è§**: [æœ¯è¯­è¡¨](../../manuscript/99-backmatter/glossary.md) è·å–è¯¦ç»†è§£é‡Š

---

**Timeline Version**: 1.0
**Created**: 2025-10-17
**Last Updated**: 2025-10-17
**Maintained By**: LLM History Chronicle Project
