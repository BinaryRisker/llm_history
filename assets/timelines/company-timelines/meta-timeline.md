---
timeline_type: company_specific
organization: Meta (Facebook)
focus: 2017-2025年Meta的LLM发展与开源战略
last_updated: 2025-10-17
---

# Meta (Facebook) Timeline (2017-2025)

**战略核心**: 开源领导者，通过开源削弱闭源优势
**关键团队**: FAIR (Facebook AI Research), Meta AI
**开源哲学**: "Open innovation drives progress"

---

## Timeline Overview

```
2017    2018    2019    2020    2021    2022    2023        2024        2025
 |       |       |       |       |       |       |           |           |
[Transformer合作]          OPT    LLaMA   LLaMA2  Llama3     Llama3.1   Llama4
                                 (泄露)  商业开源  70B/8B     405B      (MoE)
```

---

## Detailed Timeline

### 2017: Transformer论文贡献者

**参与但非主导**: Meta研究员参与Transformer论文
- Google Brain主导
- Meta (当时Facebook) 研究员贡献

### 2022: OPT开源尝试

**2022-05**: OPT (Open Pre-trained Transformer)
- 175B参数，对标GPT-3
- 研究社区许可（有限开源）
- **意义**: Meta首次尝试开源大模型

### 2023: LLaMA引爆开源浪潮

**2023-02-24**: 🔴 **LLaMA发布 - 开源战略转折点**
- 7B-65B参数家族
- Chinchilla-optimal训练策略
- **性能**: LLaMA-13B超越GPT-3 (175B)
- **影响**: 引爆全球开源LLM浪潮

**2023-03**: LLaMA泄漏事件
- 权重在互联网上泄漏
- Meta无法阻止传播
- **结果**: 推动LLaMA 2商业友好许可

**2023-07-18**: 🔴 **LLaMA 2 - 商业友好开源**
- 7B-70B参数
- **商业许可**: 可商业使用（月活<7亿限制）
- **性能**: 接近GPT-3.5水平
- **影响**: 成为开源社区首选基础模型

### 2024: Llama 3性能逼近闭源

**2024-04-18**: 🔴 **Llama 3发布**
- 8B和70B参数
- **性能**: 逼近GPT-4部分能力
- **训练数据**: 15T tokens
- **开源**: 完全权重开源

**2024-07-23**: 🔴 **Llama 3.1 - 开源最大模型**
- **405B参数**: 开源模型最大规模
- **128K上下文**: 长上下文支持
- **多语言**: 8种语言优化
- **影响**: 证明开源可以达到顶级性能

### 2025: Llama 4 MoE架构

**2025**: 🔵 **Llama 4系列 (MoE)**
- Mixture of Experts架构
- 继续扩大开源优势
- 与闭源模型性能差距缩小

---

## 开源战略 (Open Source Strategy)

### 战略目标
1. **削弱OpenAI垄断**: 通过开源降低闭源优势
2. **生态建设**: 开发者生态支持Meta产品
3. **人才吸引**: 吸引AI研究人才
4. **社会影响**: 推动AI民主化

### 实施路径
- **高质量模型**: 性能接近或超越闭源
- **商业友好**: 宽松许可，鼓励商业应用
- **持续迭代**: 每年1-2次重大更新
- **社区支持**: 文档、工具、生态

### 战略成效
- **开源领导者**: 全球最流行开源LLM基础
- **生态繁荣**: 数千个基于Llama的应用
- **削弱闭源**: OpenAI, Google压力增加
- **技术影响**: 推动整个行业发展

---

## 技术贡献 (3+ Major Innovations) ✅

1. **高质量开源模型** (2023-):
   - LLaMA/Llama系列性能优异
   - 证明开源可以高性能
   - 全球最广泛使用的开源基础模型

2. **开放商业许可** (2023-):
   - LLaMA 2商业友好许可
   - 推动AI民主化
   - 降低AI应用门槛

3. **大规模MoE架构** (2025):
   - Llama 4 MoE创新
   - 开源社区MoE探索
   - 效率与性能平衡

---

## 影响力评估

**全球影响**:
- HuggingFace下载量: 数千万次
- 衍生项目: Alpaca, Vicuna, 数百个
- 学术引用: 数万次

**产业影响**:
- 降低AI创业门槛
- 推动开源生态发展
- 削弱闭源垄断

**竞争地位**:
- 开源领导者地位稳固
- vs OpenAI: 性能差距缩小
- vs 中国开源 (Qwen): 竞争激烈

---

**Timeline Version**: 1.0
**Created**: 2025-10-17
