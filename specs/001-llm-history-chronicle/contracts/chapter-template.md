# Chapter Template Contract

**Version**: 1.0
**Purpose**: Standard structure for all book chapters
**Applies To**: All files in `manuscript/` directory

---

## File Structure

```markdown
---
# YAML Frontmatter (required)
chapter_number: [integer]
title: "[Chinese title]"
title_en: "[English title]"
period: "[YYYY-YYYY]"
status: draft | reviewed | final
word_count: [integer]
key_events:
  - [event-id-1]
  - [event-id-2]
key_organizations:
  - [org-id-1]
  - [org-id-2]
technical_concepts:
  - [concept-id-1]
  - [concept-id-2]
anecdote_count: [integer]
created_date: YYYY-MM-DD
last_updated: YYYY-MM-DD
---

# Chapter [N]: [Title in Chinese]

## å¼•è¨€ (Introduction)
[2-3 paragraphs setting context, connecting to previous chapter]

## [Main Section 1]
[Content with technical explanations, timeline events, citations]

### [Subsection 1.1]
[Detailed content]

## [Main Section 2]
[Content continues...]

### ğŸ’¡ è½¶äº‹ï¼š[Anecdote Title] (Optional Anecdote Callout)
[Anecdote content with verification status if needed]

## [Main Section 3]
[Content continues...]

## å°ç»“ (Summary)
[2-3 paragraphs summarizing key developments, setting up next chapter]

---

**æœ¬ç« è¦ç‚¹** (Key Takeaways):
- [Key point 1]
- [Key point 2]
- [Key point 3]

**å‚è€ƒæ–‡çŒ®** (Chapter References):
[List inline citations used in this chapter]
```

---

## Required Elements

### 1. YAML Frontmatter
**Mandatory Fields**:
- `chapter_number`: Sequential integer (1-11)
- `title`: Chinese chapter title
- `title_en`: English chapter title
- `period`: Time period covered (format: "YYYY-YYYY" or "YYYY-MM to YYYY-MM")
- `status`: One of: draft, reviewed, final
- `word_count`: Integer count of Chinese characters (updated after major edits)
- `created_date`: ISO 8601 date (YYYY-MM-DD)
- `last_updated`: ISO 8601 date (YYYY-MM-DD)

**Optional Fields** (populate if applicable):
- `key_events`: Array of event IDs covered in chapter
- `key_organizations`: Array of organization IDs featured
- `technical_concepts`: Array of concept IDs explained
- `anecdote_count`: Number of anecdotes included

### 2. Chapter Title (H1)
- Format: `# Chapter [N]: [Title]`
- Must match frontmatter `title` field
- Only one H1 per file

### 3. Introduction Section (å¼•è¨€)
- Required first section after title
- 2-3 paragraphs
- Purpose:
  - Establish context for this chapter
  - Connect to previous chapter's ending
  - Preview what's coming in this chapter

### 4. Main Content Sections (H2)
- At least 3-5 main sections per chapter
- Use descriptive headings (avoid generic "Section 1", "Section 2")
- Each section focuses on one major theme, event, or organization

### 5. Subsections (H3, H4)
- Use H3 for major subsections
- Use H4 for detailed breakdowns
- Maximum depth: H4 (don't go deeper)

### 6. Summary Section (å°ç»“)
- Required last section before backmatter
- 2-3 paragraphs
- Purpose:
  - Recap key developments covered
  - Emphasize significance
  - Transition to next chapter

### 7. Key Takeaways Box
- Bulleted list (3-5 items)
- Distill chapter's most important points
- Written in accessible language

### 8. Chapter References
- List all inline citations used in chapter
- Follow bibliography format from citations contract
- Links to full bibliography in backmatter

---

## Content Guidelines

### Technical Explanations
**Format**:
```markdown
### [Technical Concept Name]

**ä»€ä¹ˆæ˜¯[æ¦‚å¿µ]ï¼Ÿ** (What is [Concept]?)
[Accessible explanation for non-experts, 2-3 paragraphs]

**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ** (Why It Matters)
[Significance and impact, 1-2 paragraphs]

**ç±»æ¯”** (Analogy, optional):
[Helpful comparison if concept is complex]

å‚è§ï¼š[Reference to source]
```

**Requirements**:
- Explain on first use (90% of technical terms, SC-004)
- Accessible to readers with basic technical literacy (FR-008)
- Prioritize conceptual accuracy over mathematical precision (assumptions section)

### Timeline Events
**Format**:
```markdown
### [Event Title] (YYYY-MM-DD)

[Description of what happened, 2-3 paragraphs]

**ä¸»è¦è´¡çŒ®** (Key Contributions):
- [Innovation 1]
- [Innovation 2]

**å½±å“** (Impact):
[How this enabled future developments]

å‚è€ƒï¼š[Citation]
```

### Anecdotes
**Format**:
```markdown
### ğŸ’¡ è½¶äº‹ï¼š[Anecdote Title]

[Story content, 1-2 paragraphs]

[If unverified: âš ï¸ **æ³¨**ï¼šæ­¤ä¿¡æ¯æœªç»å®˜æ–¹è¯å®ï¼Œæ¥æºäºä¸šç•Œä¼ é—»ã€‚
Note: This information is unverified and based on industry reports.]

---
```

**Requirements**:
- Relevant to surrounding content (User Story 4, Scenario 1)
- Clear demarcation from main text (callout box, horizontal rules)
- Verification status marked if unverified (FR-010)
- Smooth transition back to main narrative (User Story 4, Scenario 3)

### Citations
**Inline Format**: `(Author, Year)` or `(ä½œè€…, Year)`

**Examples**:
```markdown
æ ¹æ®Googleçš„è®ºæ–‡ (Vaswani et al., 2017)ï¼ŒTransformeræ¶æ„...
According to Google's paper (Vaswani et al., 2017), the Transformer architecture...

OpenAIåœ¨åšå®¢ä¸­å®£å¸ƒ (Radford, 2018) GPT-1çš„å‘å¸ƒ...
OpenAI announced in their blog (Radford, 2018) the release of GPT-1...
```

**Requirements**:
- 80%+ of major claims cited (SC-008)
- Format consistent across all chapters (FR-013)

### Cross-References
**Format**:
```markdown
è¿™ä¸€çªç ´ä¸ºåæ¥çš„ChatGPTå¥ å®šäº†åŸºç¡€ï¼ˆå‚è§[ç¬¬6ç« ](../04-chatgpt-revolution/chatgpt-launch.md)ï¼‰ã€‚
This breakthrough laid the foundation for ChatGPT (see [Chapter 6](../04-chatgpt-revolution/chatgpt-launch.md)).

å¦‚[ç¬¬2ç« ](../01-foundation/attention-mechanism.md)æ‰€è¿°ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶...
As described in [Chapter 2](../01-foundation/attention-mechanism.md), the self-attention mechanism...
```

---

## Validation Checklist

**Before marking chapter as `status: reviewed`**:

- [ ] Frontmatter complete and accurate
- [ ] Word count between 8,000-12,000 characters
- [ ] Introduction connects to previous chapter
- [ ] Summary sets up next chapter
- [ ] 90%+ technical terms explained on first use
- [ ] Major claims cited (80%+ with references)
- [ ] Anecdotes marked with verification status
- [ ] Cross-references valid (links work)
- [ ] Consistent voice and tone with other chapters
- [ ] No orphaned H1, H2, H3 headers (each must have content)
- [ ] Key takeaways box present with 3-5 items

**Before marking chapter as `status: final`**:

- [ ] Beta reader feedback incorporated
- [ ] Fact-checking complete for all claims
- [ ] References verified and accessible
- [ ] Narrative flow validated
- [ ] Anecdote frequency consistent with other chapters

---

## Example Chapter Excerpt

```markdown
---
chapter_number: 1
title: "Transformeré©å‘½ï¼šæ³¨æ„åŠ›æœºåˆ¶çš„è¯ç”Ÿ"
title_en: "The Transformer Revolution: Birth of Attention Mechanism"
period: "2017-2018"
status: draft
word_count: 10500
key_events:
  - transformer-paper-2017
  - gpt1-release-2018
key_organizations:
  - google-brain
  - openai
technical_concepts:
  - self-attention
  - positional-encoding
anecdote_count: 3
created_date: 2025-10-17
last_updated: 2025-10-17
---

# Chapter 1: Transformeré©å‘½ï¼šæ³¨æ„åŠ›æœºåˆ¶çš„è¯ç”Ÿ

## å¼•è¨€

åœ¨æ·±åº¦å­¦ä¹ çš„å†å²é•¿æ²³ä¸­ï¼Œ2017å¹´6æœˆæ˜¯ä¸€ä¸ªå€¼å¾—é“­è®°çš„æ—¶åˆ»ã€‚Google Brainå›¢é˜Ÿå‘è¡¨çš„è®ºæ–‡ã€ŠAttention is All You Needã€‹ä¸ä»…ä»…æ˜¯åˆä¸€ç¯‡å­¦æœ¯è®ºæ–‡ï¼Œå®ƒæ ‡å¿—ç€è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ä¸€æ¬¡èŒƒå¼è½¬å˜ã€‚

åœ¨æ­¤ä¹‹å‰ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ä¸»å¯¼ç€åºåˆ—å»ºæ¨¡ä»»åŠ¡...

## Transformeræ¶æ„çš„è¯ç”Ÿ

### è®ºæ–‡å‘è¡¨ï¼šAttention is All You Need (2017-06-12)

2017å¹´6æœˆ12æ—¥ï¼ŒGoogle Brainå›¢é˜Ÿåœ¨arXivä¸Šå‘è¡¨äº†å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰çš„è®ºæ–‡ (Vaswani et al., 2017)ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºçš„Transformeræ¶æ„...

**ä¸»è¦è´¡çŒ®**:
- å®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ¶æ„
- æ‘’å¼ƒå¾ªç¯å’Œå·ç§¯ç»“æ„
- å¹¶è¡ŒåŒ–è®­ç»ƒèƒ½åŠ›

**å½±å“**:
Transformeræ¶æ„æˆä¸ºäº†åç»­æ‰€æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºç¡€ï¼ŒåŒ…æ‹¬GPTç³»åˆ—ã€BERTã€T5ç­‰...

### ä»€ä¹ˆæ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ

**ä»€ä¹ˆæ˜¯è‡ªæ³¨æ„åŠ›ï¼Ÿ**

è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ æ—¶ï¼ŒåŒæ—¶å…³æ³¨åºåˆ—ä¸­çš„æ‰€æœ‰å…¶ä»–å…ƒç´ ...

[Continue with explanation...]

### ğŸ’¡ è½¶äº‹ï¼šè®ºæ–‡æ ‡é¢˜çš„ç”±æ¥

æ®å›¢é˜Ÿæˆå‘˜å›å¿†ï¼Œè®ºæ–‡æ ‡é¢˜"Attention is All You Need"å®é™…ä¸Šæ˜¯å¯¹æŠ«å¤´å£«ä¹é˜Ÿæ­Œæ›²"All You Need is Love"çš„è‡´æ•¬ã€‚è¿™ä¸ªå……æ»¡è‡ªä¿¡çš„æ ‡é¢˜åœ¨å½“æ—¶å¼•å‘äº†å­¦æœ¯ç•Œçš„å¹¿æ³›è®¨è®º...

---

## å°ç»“

æœ¬ç« ä»‹ç»äº†Transformeræ¶æ„çš„è¯ç”Ÿ...è¿™ä¸€åˆ›æ–°ä¸ºæ¥ä¸‹æ¥æˆ‘ä»¬å°†åœ¨ç¬¬2ç« è®¨è®ºçš„GPT-1é“ºå¹³äº†é“è·¯ã€‚

---

**æœ¬ç« è¦ç‚¹**:
- Transformeræ¶æ„å®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‘’å¼ƒäº†RNNå’ŒCNNç»“æ„
- è‡ªæ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹å¹¶è¡Œå¤„ç†åºåˆ—ï¼Œå¤§å¹…æå‡è®­ç»ƒæ•ˆç‡
- è¿™ä¸€æ¶æ„æˆä¸ºç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€

**å‚è€ƒæ–‡çŒ®**:
- Vaswani, A., et al. (2017). Attention is All You Need. *NeurIPS 2017*.
```

---

## Version History

**v1.0** (2025-10-17): Initial contract based on data model and research decisions
