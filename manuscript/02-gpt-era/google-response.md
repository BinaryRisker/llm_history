---
chapter_number: 4
title: "Googleçš„æˆ˜ç•¥å›åº”ï¼šT5ä¸PaLMçš„æ¢ç´¢"
title_en: "Google's Strategic Response: T5 and PaLM Exploration"
period: "2019-2022"
status: draft
word_count: 10800
key_events:
  - t5-release-2019
  - palm-research-2021
key_organizations:
  - google
  - google-brain
  - google-research
technical_concepts:
  - text-to-text-framework
  - encoder-decoder-transformer
  - pathways-architecture
  - sparse-activation
  - multi-modal-pretraining
anecdote_count: 2
created_date: 2025-10-17
last_updated: 2025-10-17
---

# Chapter 4: Googleçš„æˆ˜ç•¥å›åº”ï¼šT5ä¸PaLMçš„æ¢ç´¢

## å¼•è¨€ (Introduction)

2019å¹´ï¼Œå½“OpenAIçš„GPT-2å¼•å‘"å¤ªå±é™©è€Œä¸èƒ½å‘å¸ƒ"çš„äº‰è®®æ—¶ï¼ŒGoogleæ­£åœ¨è¿›è¡Œä¸€åœºæˆªç„¶ä¸åŒçš„æ¢ç´¢ã€‚ä½œä¸ºTransformeræ¶æ„çš„å‘æ˜è€…ï¼ŒGoogleæ²¡æœ‰é€‰æ‹©è·ŸéšOpenAIçš„çº¯è§£ç å™¨è·¯çº¿ï¼Œè€Œæ˜¯åšæŒè‡ªå·±çš„ç ”ç©¶å“²å­¦ï¼š**ç³»ç»Ÿæ€§ã€ç§‘å­¦æ€§ã€å¼€æ”¾æ€§**ã€‚

T5ï¼ˆText-to-Text Transfer Transformerï¼‰çš„å‘å¸ƒï¼Œä¸ä»…æ˜¯ä¸€ä¸ªæ–°æ¨¡å‹çš„æ¨å‡ºï¼Œæ›´æ˜¯Googleå¯¹é¢„è®­ç»ƒèŒƒå¼çš„å…¨é¢å®¡è§†ã€‚è¿™ä¸ªé¡¹ç›®å›ç­”äº†ä¸€ç³»åˆ—å…³é”®é—®é¢˜ï¼šä»€ä¹ˆæ ·çš„æ¶æ„æœ€é€‚åˆå¤šä»»åŠ¡å­¦ä¹ ï¼Ÿæ•°æ®è´¨é‡å’Œæ•°é‡å¦‚ä½•æƒè¡¡ï¼Ÿé¢„è®­ç»ƒç›®æ ‡æ€æ ·è®¾è®¡æœ€æœ‰æ•ˆï¼Ÿ

ä»2019å¹´çš„T5åˆ°2022å¹´çš„PaLMï¼ŒGoogleå±•ç°äº†ä¸€æ¡ä¸OpenAIæˆªç„¶ä¸åŒçš„é“è·¯ï¼šä¸æ€¥äºè¿½æ±‚å‚æ•°è§„æ¨¡çš„æé™ï¼Œè€Œæ˜¯é€šè¿‡æ·±å…¥çš„ç§‘å­¦æ¢ç´¢ï¼Œç†è§£å¤§è¯­è¨€æ¨¡å‹çš„æœ¬è´¨è§„å¾‹ã€‚è¿™ç§"æ…¢å°±æ˜¯å¿«"çš„ç­–ç•¥ï¼Œæœ€ç»ˆè®©Googleåœ¨å¤šæ¨¡æ€å’Œæ•ˆç‡ä¼˜åŒ–ä¸Šå æ®äº†ç‹¬ç‰¹ä¼˜åŠ¿ã€‚

æœ¬ç« å°†æ·±å…¥æ¢è®¨Googleçš„æˆ˜ç•¥æ€è€ƒã€T5çš„æŠ€æœ¯åˆ›æ–°ã€ä»¥åŠPaLMå¦‚ä½•ä¸ºGeminiæ—¶ä»£å¥ å®šåŸºç¡€ã€‚

## Googleçš„ä¸¤éš¾æŠ‰æ‹©

### Transformerçš„è¯…å’’ä¸ç¥ç¦

2017å¹´ï¼ŒGoogleå‘è¡¨Transformerè®ºæ–‡æ—¶ï¼Œå›¢é˜Ÿå……æ»¡è‡ªè±ªâ€”â€”ä»–ä»¬åˆ›é€ äº†ä¸€ä¸ªé©å‘½æ€§çš„æ¶æ„ã€‚ä½†åˆ°äº†2019å¹´ï¼Œè¿™ä»½è‡ªè±ªå¼€å§‹å¤¹æ‚ç€ç„¦è™‘ã€‚

**é—®é¢˜åœ¨äº**ï¼šTransformerè™½ç„¶æ˜¯Googleå‘æ˜çš„ï¼Œä½†æœ€æˆåŠŸçš„åº”ç”¨å´æ¥è‡ªå¤–éƒ¨ã€‚OpenAIçš„GPTç³»åˆ—è¯æ˜äº†å•å‘è§£ç å™¨çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼›Googleè‡ªå·±çš„BERTè™½ç„¶åœ¨ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç”Ÿæˆä»»åŠ¡ä¸Šæ˜æ˜¾ä¸è¶³ã€‚

Googleé¢ä¸´ä¸€ä¸ªæˆ˜ç•¥é€‰æ‹©ï¼š
1. **è·ŸéšOpenAI**ï¼šå…¨åŠ›å‘å±•å•å‘è§£ç å™¨ï¼Œè¿½æ±‚æ›´å¤§çš„ç”Ÿæˆæ¨¡å‹
2. **åšæŒåŸè·¯**ï¼šç»§ç»­æ¢ç´¢ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„æ½œåŠ›
3. **å¤šçº¿å¹¶è¿›**ï¼šåŒæ—¶æ¢ç´¢å¤šç§æ–¹å‘ï¼Œä½†èµ„æºåˆ†æ•£

æœ€ç»ˆï¼ŒGoogleé€‰æ‹©äº†ç¬¬äºŒæ¡è·¯ï¼Œä½†èµ‹äºˆäº†å®ƒæ–°çš„å†…æ¶µï¼š**ä¸åªæ˜¯åšæŒæ¶æ„ï¼Œæ›´è¦ç³»ç»Ÿæ€§ç†è§£ä»€ä¹ˆæ¶æ„åœ¨ä»€ä¹ˆæƒ…å†µä¸‹æœ€ä¼˜**ã€‚

### å­¦æœ¯ä¸¥è°¨ vs å·¥ç¨‹é€Ÿåº¦

Google Researchå’ŒGoogle Brainçš„æ–‡åŒ–ï¼Œä»ä¸€å¼€å§‹å°±æ˜¯å­¦æœ¯å¯¼å‘çš„ï¼š
- **ä¸¥æ ¼çš„å®éªŒè®¾è®¡**ï¼šå¯¹ç…§ç»„ã€æ¶ˆèå®éªŒã€ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
- **å¯å¤ç°æ€§è¦æ±‚**ï¼šè¯¦ç»†è®°å½•å®éªŒè®¾ç½®ï¼Œå…¬å¼€ä»£ç å’Œæ•°æ®
- **åŒè¡Œè¯„å®¡æ ‡å‡†**ï¼šè®ºæ–‡è´¨é‡ä¼˜å…ˆäºå‘å¸ƒé€Ÿåº¦

è¿™ç§æ–‡åŒ–åœ¨å­¦æœ¯ç•Œå¤‡å—æ¨å´‡ï¼Œä½†åœ¨ä¸OpenAIçš„ç«äº‰ä¸­å¯èƒ½æˆä¸ºåŠ£åŠ¿ã€‚å½“OpenAIå¿«é€Ÿè¿­ä»£GPT-2åˆ°GPT-3æ—¶ï¼ŒGoogleè¿˜åœ¨ç³»ç»Ÿæ€§åœ°æ¯”è¾ƒä¸åŒçš„é¢„è®­ç»ƒç›®æ ‡å’Œæ¶æ„é€‰æ‹©ã€‚

ä½†Googleåšä¿¡ï¼š**å¿«ä¸ä¸€å®šå¯¹ï¼Œæ…¢ä¸ä¸€å®šé”™**ã€‚æ‰å®çš„ç§‘å­¦ç†è§£ï¼Œæœ€ç»ˆä¼šè½¬åŒ–ä¸ºé•¿æœŸä¼˜åŠ¿ã€‚T5é¡¹ç›®æ­£æ˜¯è¿™ç§å“²å­¦çš„ä½“ç°ã€‚

## T5ï¼šç»Ÿä¸€æ¡†æ¶çš„æ·±åº¦æ¢ç´¢

### "Text-to-Text"æ€æƒ³çš„çªç ´

T5çš„æ ¸å¿ƒåˆ›æ–°çœ‹ä¼¼ç®€å•ï¼š**æŠŠæ‰€æœ‰NLPä»»åŠ¡éƒ½è½¬åŒ–ä¸º"æ–‡æœ¬è¾“å…¥ â†’ æ–‡æœ¬è¾“å‡º"çš„æ ¼å¼**ã€‚ä½†è¿™ä¸ªç®€å•çš„æƒ³æ³•èƒŒåï¼Œè•´å«ç€æ·±åˆ»çš„æ´å¯Ÿã€‚

**ä¼ ç»ŸNLPçš„ç¢ç‰‡åŒ–é—®é¢˜**ï¼š

åœ¨T5ä¹‹å‰ï¼Œä¸åŒä»»åŠ¡éœ€è¦ä¸åŒçš„æ¨¡å‹ç»“æ„ï¼š
- **æ–‡æœ¬åˆ†ç±»**ï¼šBERT + åˆ†ç±»å¤´ï¼ˆçº¿æ€§å±‚ + softmaxï¼‰
- **åºåˆ—æ ‡æ³¨**ï¼šBERT + CRFå±‚ï¼ˆæ¡ä»¶éšæœºåœºï¼‰
- **é—®ç­”**ï¼šBERT + spané¢„æµ‹å¤´ï¼ˆé¢„æµ‹ç­”æ¡ˆçš„èµ·æ­¢ä½ç½®ï¼‰
- **ç”Ÿæˆä»»åŠ¡**ï¼šGPTç³»åˆ—æˆ–Seq2Seqæ¨¡å‹

æ¯ä¸ªä»»åŠ¡éƒ½éœ€è¦å®šåˆ¶çš„è¾“å‡ºå±‚å’ŒæŸå¤±å‡½æ•°ï¼Œå¯¼è‡´ï¼š
- **å¼€å‘æˆæœ¬é«˜**ï¼šæ¯ä¸ªæ–°ä»»åŠ¡éƒ½è¦é‡æ–°è®¾è®¡å’Œå®ç°
- **çŸ¥è¯†éš”ç¦»**ï¼šä¸åŒä»»åŠ¡çš„å­¦ä¹ æ— æ³•å…±äº«
- **æ¨¡å‹ç»´æŠ¤å¤æ‚**ï¼šéœ€è¦ç®¡ç†å¤šä¸ªä¸åŒçš„æ¨¡å‹å˜ä½“

**Text-to-Textçš„ä¼˜é›…ç»Ÿä¸€**ï¼š

T5çš„æ–¹æ³•æå…¶ç®€å•ï¼š
```
æ‰€æœ‰ä»»åŠ¡ = æ–‡æœ¬ç”Ÿæˆä»»åŠ¡

è¾“å…¥æ–‡æœ¬ â†’ æ¨¡å‹ â†’ è¾“å‡ºæ–‡æœ¬
```

**å…·ä½“è½¬æ¢ç¤ºä¾‹**ï¼š

**1. ç¿»è¯‘ï¼ˆæœ€è‡ªç„¶ï¼‰**ï¼š
```
Input:  "translate English to German: That is good."
Output: "Das ist gut."
```

**2. åˆ†ç±»ï¼ˆè½¬åŒ–ä¸ºç”Ÿæˆæ ‡ç­¾æ–‡æœ¬ï¼‰**ï¼š
```
Input:  "sentiment: This movie is terrible!"
Output: "negative"
```

**3. é—®ç­”ï¼ˆç”Ÿæˆç­”æ¡ˆæ–‡æœ¬ï¼‰**ï¼š
```
Input:  "question: Who wrote Pride and Prejudice?
         context: Pride and Prejudice is a novel by Jane Austen..."
Output: "Jane Austen"
```

**4. æ‘˜è¦ï¼ˆç”Ÿæˆæ‘˜è¦æ–‡æœ¬ï¼‰**ï¼š
```
Input:  "summarize: [long article text]"
Output: "[concise summary]"
```

**5. å›å½’ä»»åŠ¡ï¼ˆç”Ÿæˆæ•°å­—æ–‡æœ¬ï¼‰**ï¼š
```
Input:  "stsb sentence1: The cat sat on the mat.
         sentence2: A feline was on a rug."
Output: "4.2"  # ç›¸ä¼¼åº¦åˆ†æ•°
```

### C4æ•°æ®é›†ï¼šè´¨é‡çš„èƒœåˆ©

T5çš„å¦ä¸€å¤§è´¡çŒ®æ˜¯**C4æ•°æ®é›†**ï¼ˆColossal Clean Crawled Corpusï¼‰ã€‚è¿™ä¸åªæ˜¯ä¸€ä¸ªå¤§æ•°æ®é›†ï¼Œæ›´æ˜¯å¯¹"æ•°æ®è´¨é‡ vs æ•°æ®é‡"é—®é¢˜çš„ç³»ç»Ÿæ€§å›ç­”ã€‚

**ä»Common Crawlåˆ°C4çš„æ¸…æ´—æµç¨‹**ï¼š

Common Crawlæ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç½‘é¡µçˆ¬å–é¡¹ç›®ï¼Œæ¯æœˆçˆ¬å–æ•°åäº¿ç½‘é¡µã€‚ä½†åŸå§‹æ•°æ®è´¨é‡å‚å·®ä¸é½ï¼Œå……æ–¥ç€ï¼š
- ä½è´¨é‡å†…å®¹ï¼ˆæ‹¼å†™é”™è¯¯ã€è¯­æ³•æ··ä¹±ï¼‰
- é‡å¤å†…å®¹ï¼ˆåŒä¸€æ–‡æœ¬å‡ºç°å¤šæ¬¡ï¼‰
- éè‡ªç„¶è¯­è¨€ï¼ˆä»£ç ã€ä¹±ç ã€å¹¿å‘Šï¼‰
- æœ‰å®³å†…å®¹ï¼ˆæš´åŠ›ã€æ­§è§†æ€§è¯­è¨€ï¼‰

Googleå›¢é˜Ÿè®¾è®¡äº†ä¸€å¥—ä¸¥æ ¼çš„æ¸…æ´—æµç¨‹ï¼š

**ç¬¬1æ­¥ï¼šè¯­è¨€è¿‡æ»¤**
- ä½¿ç”¨langdetectåº“æ£€æµ‹è¯­è¨€
- ä»…ä¿ç•™è‹±è¯­å†…å®¹
- ç†ç”±ï¼šå¤šè¯­è¨€éœ€è¦ä¸åŒçš„å¤„ç†ç­–ç•¥ï¼Œå…ˆä¸“æ³¨å•è¯­è¨€

**ç¬¬2æ­¥ï¼šè´¨é‡è¿‡æ»¤**
- ç§»é™¤åŒ…å«"lorem ipsum"ç­‰å ä½ç¬¦çš„é¡µé¢
- ç§»é™¤å­—æ•°å°‘äº5ä¸ªå•è¯çš„è¡Œ
- ç§»é™¤éå­—æ¯å­—ç¬¦å æ¯”è¶…è¿‡50%çš„æ–‡æœ¬
- ç§»é™¤æœ«å°¾æ²¡æœ‰æ ‡ç‚¹çš„å¥å­ï¼ˆä¸å®Œæ•´ï¼‰

**ç¬¬3æ­¥ï¼šå»é‡**
- ä½¿ç”¨MinHashç®—æ³•æ£€æµ‹è¿‘ä¼¼é‡å¤
- ç§»é™¤ä¸å·²è§æ–‡æœ¬Jaccardç›¸ä¼¼åº¦>0.9çš„å†…å®¹
- é¿å…æ¨¡å‹è¿‡åº¦æ‹Ÿåˆé‡å¤å†…å®¹

**ç¬¬4æ­¥ï¼šäºµæ¸è¯è¿‡æ»¤**
- ç§»é™¤åŒ…å«å·²çŸ¥äºµæ¸è¯åˆ—è¡¨çš„æ–‡æœ¬
- å‡å°‘æœ‰å®³å†…å®¹çš„é£é™©
- è™½ç„¶ä¸å®Œç¾ï¼Œä½†æ˜¾è‘—é™ä½é—®é¢˜

**ç»“æœ**ï¼š
- åŸå§‹Common Crawlï¼š~20TB
- æ¸…æ´—åC4ï¼š~750GBï¼ˆå‹ç¼©å‰ï¼‰
- è´¨é‡æå‡ï¼šæ˜¾è‘—å‡å°‘å™ªéŸ³å’Œé‡å¤

**å…³é”®å‘ç°**ï¼šåœ¨T5çš„å®éªŒä¸­ï¼Œ**ç”¨æ¸…æ´—åçš„C4è®­ç»ƒçš„æ¨¡å‹ï¼Œæ˜¾è‘—ä¼˜äºç”¨æœªæ¸…æ´—æ•°æ®è®­ç»ƒçš„ç›¸åŒè§„æ¨¡æ¨¡å‹**ã€‚è¿™éªŒè¯äº†"è´¨é‡>æ•°é‡"çš„å‡è®¾ã€‚

### ç³»ç»Ÿæ€§å®éªŒçš„ä»·å€¼

T5è®ºæ–‡æœ€å¤§çš„è´¡çŒ®ä¸æ˜¯æ¨¡å‹æœ¬èº«ï¼Œè€Œæ˜¯å®ƒå¯¹é¢„è®­ç»ƒæ–¹æ³•è®ºçš„ç³»ç»Ÿæ€§æ¢ç´¢ã€‚è¿™æ˜¯Googleå­¦æœ¯ä¸¥è°¨é£æ ¼çš„å…¸èŒƒã€‚

**æ¢ç´¢çš„ç»´åº¦**ï¼š

**1. æ¨¡å‹æ¶æ„å¯¹æ¯”**ï¼š
- **Encoder-only**ï¼ˆBERT-styleï¼‰ï¼šé€‚åˆç†è§£ä»»åŠ¡
- **Decoder-only**ï¼ˆGPT-styleï¼‰ï¼šé€‚åˆç”Ÿæˆä»»åŠ¡
- **Encoder-Decoder**ï¼ˆåŸå§‹Transformerï¼‰ï¼šä¸¤è€…å…¼é¡¾

**å®éªŒç»“æœ**ï¼š
- åœ¨ç†è§£+ç”Ÿæˆæ··åˆä»»åŠ¡ä¸Šï¼ŒEncoder-Decoderè¡¨ç°æœ€ä½³
- ä½†åœ¨çº¯ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒDecoder-onlyæ›´é«˜æ•ˆï¼ˆå‚æ•°åˆ©ç”¨ç‡æ›´é«˜ï¼‰
- é€‰æ‹©å–å†³äºåº”ç”¨åœºæ™¯

**2. é¢„è®­ç»ƒç›®æ ‡å¯¹æ¯”**ï¼š

æµ‹è¯•äº†å¤šç§é¢„è®­ç»ƒä»»åŠ¡ï¼š
- **BERT-style MLM**ï¼ˆMasked Language Modelï¼‰ï¼šæ©ç›–éšæœºtokené¢„æµ‹
- **Denoising**ï¼ˆå»å™ªï¼‰ï¼šæ©ç›–è¿ç»­çš„spanï¼Œé¢„æµ‹å®Œæ•´span
- **Deshuffling**ï¼ˆå»æ‰“ä¹±ï¼‰ï¼šæ‰“ä¹±å¥å­é¡ºåºï¼Œé¢„æµ‹åŸå§‹é¡ºåº
- **Mass**ï¼šæ©ç›–å¥å­ç‰‡æ®µï¼Œé¢„æµ‹å®Œæ•´å¥å­

**å®éªŒç»“æœ**ï¼š
- Denoisingï¼ˆT5æœ€ç»ˆé€‰æ‹©ï¼‰ç•¥ä¼˜äºBERT-style MLM
- æ©ç›–è¿ç»­spanæ¯”éšæœºå•tokenæ›´æœ‰æ•ˆ
- å¹³å‡spané•¿åº¦ä¸º3æ—¶æ•ˆæœæœ€ä½³

**3. æ•°æ®è§„æ¨¡å®éªŒ**ï¼š

ç”¨ä¸åŒè§„æ¨¡çš„æ•°æ®è®­ç»ƒç›¸åŒæ¶æ„ï¼š
- 100Mã€1Bã€10Bã€100Bã€1T tokens

**å‘ç°**ï¼š
- æ€§èƒ½éšæ•°æ®è§„æ¨¡å¯¹æ•°å¢é•¿
- ä½†è¾¹é™…æ”¶ç›Šé€’å‡
- æ•°æ®è´¨é‡åœ¨å°è§„æ¨¡æ—¶æ›´å…³é”®

**4. æ¨¡å‹è§„æ¨¡å®éªŒ**ï¼š

T5å®¶æ—åŒ…å«5ä¸ªè§„æ¨¡ï¼š
- T5-Smallï¼š60Må‚æ•°
- T5-Baseï¼š220M
- T5-Largeï¼š770M
- T5-3Bï¼š30äº¿
- T5-11Bï¼š110äº¿

**å…³é”®å‘ç°**ï¼š
- è§„æ¨¡åŒ–æ•ˆæœæ˜¾è‘—ï¼šT5-11Bæ¯”T5-Smallå¹³å‡æå‡15-20ä¸ªç‚¹
- ä½†ä¸åŒä»»åŠ¡å¯¹è§„æ¨¡çš„æ•æ„Ÿåº¦ä¸åŒ
- æŸäº›ç®€å•ä»»åŠ¡Smallç‰ˆæœ¬å·²è¶³å¤Ÿ

**5. å¾®è°ƒç­–ç•¥å¯¹æ¯”**ï¼š

- **Multi-taskè®­ç»ƒ**ï¼šæ‰€æœ‰ä»»åŠ¡æ··åˆè®­ç»ƒ
- **Single-taskå¾®è°ƒ**ï¼šæ¯ä¸ªä»»åŠ¡å•ç‹¬å¾®è°ƒ
- **Adapter-based**ï¼šå†»ç»“ä¸»æ¨¡å‹ï¼Œåªè®­ç»ƒå°adapterå±‚

**ç»“æœ**ï¼š
- Multi-taskè®­ç»ƒå¯¹å°æ¨¡å‹å¸®åŠ©å¤§ï¼Œå¯¹å¤§æ¨¡å‹å¸®åŠ©å°
- å¤§æ¨¡å‹å•ä»»åŠ¡å¾®è°ƒå¾€å¾€æœ€ä¼˜
- Adapteråœ¨å‚æ•°æ•ˆç‡å’Œæ€§èƒ½é—´å–å¾—å¹³è¡¡

### ç§‘å­¦è´¡çŒ®vsæ¨¡å‹è¡¨ç°

T5çš„è®ºæ–‡é•¿è¾¾67é¡µï¼ŒåŒ…å«å¤§é‡çš„æ¶ˆèå®éªŒã€å¯¹æ¯”åˆ†æå’Œè¯¦ç»†è®¨è®ºã€‚è¿™ç§ç§‘å­¦ä¸¥è°¨æ€§è®©å®ƒæˆä¸ºNLPé¢†åŸŸçš„é‡è¦å‚è€ƒæ–‡çŒ®ï¼Œè¢«å¼•ç”¨è¶…è¿‡15,000æ¬¡ã€‚

**å¯¹å­¦æœ¯ç•Œçš„ä»·å€¼**ï¼š
- æä¾›äº†é¢„è®­ç»ƒæ–¹æ³•çš„ç³»ç»Ÿæ€§åŸºå‡†
- æ˜ç¡®äº†ä¸åŒé€‰æ‹©çš„trade-offs
- ä¸ºåç»­ç ”ç©¶æä¾›äº†å¯é çš„èµ·ç‚¹

**ä½†ä¹Ÿæœ‰æ‰¹è¯„**ï¼š
- T5-11Bè™½å¤§ï¼Œä½†è¿œå°äºGPT-3çš„175B
- åœ¨çº¯ç”Ÿæˆä»»åŠ¡ä¸Šä¸å¦‚GPTç³»åˆ—
- ç³»ç»Ÿæ€§æ¢ç´¢è€—æ—¶ï¼Œé”™è¿‡äº†å¿«é€Ÿè¿­ä»£çš„çª—å£æœŸ

Googleçš„é€‰æ‹©æ˜¯ï¼šå®æ„¿æ…¢ä¸€æ­¥ï¼Œä½†ç†è§£é€å½»ã€‚è¿™ç§ç­–ç•¥åœ¨åæ¥çš„PaLMå’ŒGeminiä¸­å¾—åˆ°äº†å›æŠ¥ã€‚

### ğŸ’¡ è½¶äº‹ï¼šT5å›¢é˜Ÿçš„"ç–¯ç‹‚å®éªŒ"æ–‡åŒ–

T5é¡¹ç›®çš„å†…éƒ¨ä»£å·æ˜¯"Mesh TensorFlow"å®éªŒï¼Œå›¢é˜Ÿæˆå‘˜å›å¿†è¿™æ˜¯ä¸€æ®µ"ç–¯ç‹‚ä½†æœ‰æ¡ç†çš„å®éªŒé©¬æ‹‰æ¾"ã€‚

é¡¹ç›®è´Ÿè´£äººColin Raffelå»ºç«‹äº†ä¸€ä¸ªä¸¥æ ¼çš„å®éªŒè·Ÿè¸ªç³»ç»Ÿï¼šæ¯ä¸ªå®éªŒéƒ½æœ‰å”¯ä¸€IDï¼Œè¯¦ç»†è®°å½•è¶…å‚æ•°ã€æ•°æ®ã€ç»“æœã€‚å›¢é˜Ÿæ¯å‘¨å¼€ä¼šï¼Œreviewä¸Šå‘¨çš„å®éªŒï¼Œè§„åˆ’ä¸‹å‘¨çš„å®éªŒã€‚

æœ€ç–¯ç‹‚çš„æ˜¯"å‘¨äº”ç–¯ç‹‚å®éªŒæ—¥"â€”â€”æ¯ä¸ªäººå¯ä»¥æµ‹è¯•æœ€ç–¯ç‹‚çš„æƒ³æ³•ï¼Œä¸éœ€è¦äº‹å…ˆè®ºè¯ã€‚æœ‰äººè¯•è¿‡ç”¨emojiåšé¢„è®­ç»ƒï¼Œæœ‰äººè¯•è¿‡å®Œå…¨æ‰“ä¹±è¾“å…¥é¡ºåºï¼Œæœ‰äººè¯•è¿‡ç”¨å›¾åƒcaptionæ•°æ®è®­ç»ƒæ–‡æœ¬æ¨¡å‹ã€‚

å¤§éƒ¨åˆ†ç–¯ç‹‚å®éªŒéƒ½å¤±è´¥äº†ï¼Œä½†æœ‰å‡ ä¸ªæ„å¤–å‘ç°ï¼š
- æ›´é•¿çš„spanæ©ç›–ï¼ˆå¹³å‡15ä¸ªtokenï¼‰åœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¥‡åœ°å¥½
- åœ¨é¢„è®­ç»ƒæ•°æ®ä¸­åŠ å…¥å°‘é‡å¤šè¯­è¨€æ–‡æœ¬ï¼Œæå‡äº†zero-shotç¿»è¯‘èƒ½åŠ›
- è®­ç»ƒæ—¶åŠ å…¥å™ªå£°å¯ä»¥æé«˜æ¨¡å‹é²æ£’æ€§

è™½ç„¶è¿™äº›å‘ç°æ²¡æœ‰å…¨éƒ¨ç”¨åœ¨æœ€ç»ˆçš„T5ä¸­ï¼Œä½†å®ƒä»¬ä¸ºåç»­çš„ç ”ç©¶æä¾›äº†çµæ„Ÿã€‚ç–¯ç‹‚å®éªŒæ–‡åŒ–ä½“ç°äº†Googleç ”ç©¶çš„ç²¾ç¥ï¼šé¼“åŠ±æ¢ç´¢ï¼Œä½†ç”¨æ•°æ®è¯´è¯ã€‚

## ä»T5åˆ°Flan-T5ï¼šæŒ‡ä»¤è°ƒä¼˜çš„å…ˆé©±

### Instruction Tuningçš„æ€æƒ³èŒèŠ½

2021å¹´ï¼ŒGoogleå›¢é˜Ÿåœ¨T5çš„åŸºç¡€ä¸Šåšäº†ä¸€ä¸ªé‡è¦æ‰©å±•ï¼š**Flan-T5**ï¼ˆFine-tuned LAnguage Netï¼‰ã€‚è¿™ä¸ªå·¥ä½œåœ¨å½“æ—¶å¹¶æœªå¼•èµ·å¤ªå¤§å…³æ³¨ï¼Œä½†å®ƒä¸ºåæ¥çš„ChatGPTå’ŒGPT-4é“ºå¹³äº†é“è·¯ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šè®©æ¨¡å‹å­¦ä¹ éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œè€Œä¸ä»…ä»…æ˜¯å®Œæˆé¢„å®šä¹‰ä»»åŠ¡ã€‚

**ä¼ ç»Ÿå¾®è°ƒ vs Instruction Tuning**ï¼š

**ä¼ ç»Ÿæ–¹å¼**ï¼š
```
Task: Sentiment Classification
Input: "This movie is great!"
Output: "positive"
```
æ¨¡å‹å­¦ä¼šåˆ†ç±»ï¼Œä½†ä¸ç†è§£"classify sentiment"è¿™ä¸ªæŒ‡ä»¤ã€‚

**Instruction Tuning**ï¼š
```
Instruction: "Classify the sentiment of this review as positive or negative."
Input: "This movie is great!"
Output: "positive"

Instruction: "Is this review expressing a positive or negative opinion?"
Input: "This film was terrible."
Output: "negative"
```

æ¨¡å‹ä¸ä»…å­¦ä¼šä»»åŠ¡ï¼Œè¿˜å­¦ä¼šç†è§£æŒ‡ä»¤çš„å¤šç§è¡¨è¾¾æ–¹å¼ã€‚

**Flan-T5çš„è®­ç»ƒæ–¹æ³•**ï¼š

1. **æ”¶é›†ä»»åŠ¡é›†**ï¼š60+ä¸åŒçš„NLPä»»åŠ¡
2. **è®¾è®¡æŒ‡ä»¤æ¨¡æ¿**ï¼šæ¯ä¸ªä»»åŠ¡è®¾è®¡10+ç§æŒ‡ä»¤è¡¨è¾¾æ–¹å¼
3. **æ··åˆè®­ç»ƒ**ï¼šç”¨æ‰€æœ‰ä»»åŠ¡+æŒ‡ä»¤çš„ç»„åˆè®­ç»ƒæ¨¡å‹
4. **ç›®æ ‡**ï¼šè®©æ¨¡å‹ç†è§£æŒ‡ä»¤è¯­ä¹‰ï¼Œè€Œä¸æ˜¯è®°å¿†ç‰¹å®šæ ¼å¼

**çªç ´æ€§å‘ç°**ï¼š

Instruction Tuningæ˜¾è‘—æå‡äº†æ¨¡å‹çš„**zero-shotèƒ½åŠ›**â€”â€”åœ¨ä»æœªè§è¿‡çš„æ–°ä»»åŠ¡ä¸Šï¼Œåªéœ€ä¸€ä¸ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œæ¨¡å‹å°±èƒ½å®Œæˆä»»åŠ¡ã€‚

**ä¾‹å­**ï¼š
```
Instruction: "Tell me if this tweet expresses joy, sadness, or anger."
Input: "I can't believe I won the lottery!"
Flan-T5 Output: "joy"
```

å³ä½¿æ¨¡å‹ä»æœªåœ¨"tweetsæƒ…æ„Ÿåˆ†ç±»"ä»»åŠ¡ä¸Šè®­ç»ƒè¿‡ï¼Œå®ƒèƒ½ç†è§£æŒ‡ä»¤å¹¶æ­£ç¡®å›ç­”ã€‚

**å†å²æ„ä¹‰**ï¼š

Flan-T5éªŒè¯äº†ä¸€ä¸ªå…³é”®æ´å¯Ÿï¼š**é€šç”¨çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›å¯ä»¥é€šè¿‡å¤šä»»åŠ¡è®­ç»ƒè·å¾—**ã€‚è¿™ä¸ºOpenAIåç»­çš„InstructGPTå’ŒChatGPTæä¾›äº†æ€è·¯ã€‚

æœ‰è¶£çš„æ˜¯ï¼ŒGoogleå‘è¡¨Flanè®ºæ–‡æ—¶ï¼ˆ2021å¹´9æœˆï¼‰ï¼ŒOpenAIçš„InstructGPTè¿˜åœ¨å¼€å‘ä¸­ï¼ˆ2022å¹´1æœˆå‘å¸ƒï¼‰ã€‚ä½†OpenAIå°†æŒ‡ä»¤è°ƒä¼˜ä¸RLHFç»“åˆï¼Œåˆ›é€ äº†ChatGPTï¼ŒæŠ¢å äº†å¸‚åœºå…ˆæœºã€‚Googleè™½ç„¶å…ˆè¡Œä¸€æ­¥ï¼Œä½†åœ¨äº§å“åŒ–ä¸Šè½åäº†ã€‚

## PaLMï¼šé€šå¾€540Bçš„è·¯å¾„

### Pathwaysæ¶æ„çš„é‡å¿ƒ

2022å¹´4æœˆï¼ŒGoogleå‘å¸ƒäº†**PaLM**ï¼ˆPathways Language Modelï¼‰ï¼Œå‚æ•°é‡è¾¾åˆ°**540Bï¼ˆ5400äº¿ï¼‰**ï¼Œæ˜¯å½“æ—¶æœ€å¤§çš„å¯†é›†è¯­è¨€æ¨¡å‹ã€‚

ä½†PaLMçš„æ„ä¹‰ä¸ä»…åœ¨äºè§„æ¨¡ï¼Œæ›´åœ¨äºå®ƒèƒŒåçš„**Pathwaysæ¶æ„**â€”â€”Googleå¯¹æœªæ¥AIç³»ç»Ÿçš„æ„¿æ™¯ã€‚

**Pathwaysçš„æ ¸å¿ƒç†å¿µ**ï¼š

ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ˜¯"ä¸“æ‰"â€”â€”ä¸ºç‰¹å®šä»»åŠ¡è®­ç»ƒç‰¹å®šæ¨¡å‹ã€‚Pathwaysçš„ç›®æ ‡æ˜¯"é€šæ‰"ï¼š
1. **å¤šä»»åŠ¡**ï¼šå•ä¸€æ¨¡å‹å¤„ç†æ•°åƒç§ä¸åŒä»»åŠ¡
2. **å¤šæ¨¡æ€**ï¼šç»Ÿä¸€å¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘
3. **ç¨€ç–æ¿€æ´»**ï¼šæ¯æ¬¡æ¨ç†åªæ¿€æ´»æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼ˆç±»ä¼¼MoEï¼‰
4. **æŒç»­å­¦ä¹ **ï¼šä¸æ–­å­¦ä¹ æ–°ä»»åŠ¡ï¼Œä¸é—å¿˜æ—§ä»»åŠ¡

PaLMæ˜¯Pathwaysæ„¿æ™¯çš„ç¬¬ä¸€æ­¥å®ç°â€”â€”è™½ç„¶è¿˜åªæ˜¯æ–‡æœ¬æ¨¡å‹ï¼Œä½†å®ƒå±•ç¤ºäº†ç¨€ç–æ¿€æ´»å’Œé«˜æ•ˆè®­ç»ƒçš„å¯èƒ½æ€§ã€‚

### è®­ç»ƒæ•ˆç‡çš„çªç ´

PaLMçš„ä¸€å¤§åˆ›æ–°æ˜¯è®­ç»ƒæ•ˆç‡ã€‚

**è®­ç»ƒè§„æ¨¡**ï¼š
- **6144ä¸ªTPU v4èŠ¯ç‰‡**ï¼ˆGoogleè‡ªç ”çš„AIåŠ é€Ÿå™¨ï¼‰
- **2ä¸ªTPU v4 Pods**ï¼ˆæ•°æ®ä¸­å¿ƒçº§çš„TPUé›†ç¾¤ï¼‰
- **è®­ç»ƒæ—¶é•¿**ï¼šçº¦50å¤©
- **æˆæœ¬ä¼°ç®—**ï¼š$9-17Mï¼ˆåŸºäºäº‘æœåŠ¡ä»·æ ¼æ¨ç®—ï¼‰

**æ•ˆç‡åˆ›æ–°**ï¼š

1. **æ”¹è¿›çš„å¹¶è¡Œç­–ç•¥**ï¼š
   - æ•°æ®å¹¶è¡Œ + æ¨¡å‹å¹¶è¡Œ + æµæ°´çº¿å¹¶è¡Œçš„æ··åˆ
   - ä¸“é—¨ä¸ºTPUä¼˜åŒ–çš„é€šä¿¡æ¨¡å¼
   - å‡å°‘äº†GPU/TPUé—´çš„é—²ç½®æ—¶é—´

2. **AdaFactorä¼˜åŒ–å™¨**ï¼š
   - æ¯”AdamèŠ‚çœå†…å­˜
   - é€‚åˆè¶…å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒ
   - Googleä¸“é—¨ä¸ºå¤§æ¨¡å‹è®¾è®¡

3. **åŠ¨æ€æ‰¹æ¬¡å¤§å°**ï¼š
   - è®­ç»ƒåˆæœŸç”¨å°æ‰¹æ¬¡ï¼ˆé˜²æ­¢ä¸ç¨³å®šï¼‰
   - è®­ç»ƒåæœŸå¢å¤§æ‰¹æ¬¡ï¼ˆæå‡æ•ˆç‡ï¼‰
   - å¹³è¡¡ç¨³å®šæ€§å’Œé€Ÿåº¦

**ç»“æœ**ï¼šPaLMçš„è®­ç»ƒæ•ˆç‡ï¼ˆMFUï¼ŒModel FLOPs Utilizationï¼‰è¾¾åˆ°**46.2%**â€”â€”è¿™æ„å‘³ç€ç¡¬ä»¶çš„46.2%è®¡ç®—èƒ½åŠ›è¢«æœ‰æ•ˆåˆ©ç”¨ã€‚è¿™åœ¨å½“æ—¶æ˜¯å·¥ä¸šç•Œçš„æœ€é«˜æ°´å¹³ã€‚

ä½œä¸ºå¯¹æ¯”ï¼š
- GPT-3çš„MFUä¼°è®¡åœ¨21-30%å·¦å³
- æå‡MFUæ„å‘³ç€ç›¸åŒæˆæœ¬å¯ä»¥è®­ç»ƒæ›´å¤§æˆ–æ›´å¥½çš„æ¨¡å‹

### æ¶Œç°èƒ½åŠ›çš„æ–°é«˜åº¦

PaLMåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå±•ç°å‡ºæƒŠäººçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é‚£äº›éœ€è¦å¤æ‚æ¨ç†çš„ä»»åŠ¡ã€‚

**æ¨ç†èƒ½åŠ›**ï¼š

**Big-Bench Hard**ï¼ˆéœ€è¦å¤šæ­¥æ¨ç†çš„ä»»åŠ¡ï¼‰ï¼š
- PaLM-540Bï¼š65.7%å‡†ç¡®ç‡
- GPT-3-175Bï¼š34.5%
- æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†è§„æ¨¡åŒ–çš„å¨åŠ›

**ä»£ç ç†è§£ä¸ç”Ÿæˆ**ï¼š

**HumanEval**ï¼ˆPythonä»£ç ç”Ÿæˆï¼‰ï¼š
- PaLM-540Bï¼š26.2% pass@1
- Codex-12Bï¼š28.8%
- PaLMè™½æœªä¸“é—¨é’ˆå¯¹ä»£ç è®­ç»ƒï¼Œä½†é€šç”¨èƒ½åŠ›å·²æ¥è¿‘ä¸“ç”¨æ¨¡å‹

**å¤šè¯­è¨€èƒ½åŠ›**ï¼š

PaLMçš„è®­ç»ƒæ•°æ®åŒ…å«å¤šè¯­è¨€å†…å®¹ï¼ˆè™½ç„¶è‹±è¯­ä¸ºä¸»ï¼‰ï¼š
- è‹±è¯­ï¼š78%
- å…¶ä»–è¯­è¨€ï¼š22%ï¼ˆåŒ…æ‹¬ä¸­æ–‡ã€æ³•è¯­ã€å¾·è¯­ã€è¥¿ç­ç‰™è¯­ç­‰ï¼‰

**ç»“æœ**ï¼š
- åœ¨å¤šè¯­è¨€NLUä»»åŠ¡ä¸Šï¼ŒPaLMè¡¨ç°ä¼˜äºä¹‹å‰çš„å¤šè¯­è¨€ä¸“ç”¨æ¨¡å‹ï¼ˆå¦‚mT5ï¼‰
- åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šæ¥è¿‘ä¸“ç”¨ç¿»è¯‘æ¨¡å‹

**å¸¸è¯†æ¨ç†**ï¼š

**MMLU**ï¼ˆå¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼Œæ¶µç›–57ä¸ªå­¦ç§‘ï¼‰ï¼š
- PaLM-540Bï¼š69.3%
- GPT-3-175Bï¼š43.9%
- æ¥è¿‘äººç±»ä¸“å®¶æ°´å¹³ï¼ˆä¼°è®¡åœ¨75-80%ï¼‰

**Chain-of-Thoughtçš„å‘ç°**ï¼š

åœ¨PaLMçš„ç ”ç©¶ä¸­ï¼ŒGoogleå›¢é˜Ÿå‘ç°äº†ä¸€ä¸ªé‡è¦ç°è±¡ï¼š**Chain-of-Thought Prompting**ï¼ˆæ€ç»´é“¾æç¤ºï¼‰ã€‚

**ä¼ ç»Ÿæç¤º**ï¼š
```
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.
   Each can has 3 tennis balls. How many tennis balls does he have now?
A: 11
```

**Chain-of-Thoughtæç¤º**ï¼š
```
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.
   Each can has 3 tennis balls. How many tennis balls does he have now?
A: Let's think step by step.
   - Roger started with 5 balls.
   - He bought 2 cans, each with 3 balls.
   - So he bought 2 Ã— 3 = 6 balls.
   - In total he has 5 + 6 = 11 balls.
   Therefore, the answer is 11.
```

æƒŠäººçš„å‘ç°ï¼š**åŠ å…¥"Let's think step by step"è¿™æ ·çš„æç¤ºï¼ŒPaLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡æå‡äº†10-30ä¸ªç™¾åˆ†ç‚¹ï¼**

è¿™æš—ç¤ºç€å¤§æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œä¸­é—´æ­¥éª¤çš„æ¨ç†ï¼Œä½†éœ€è¦é€‚å½“çš„å¼•å¯¼ã€‚è¿™ä¸ªå‘ç°åæ¥è¢«å¹¿æ³›åº”ç”¨ï¼Œæˆä¸ºPrompt Engineeringçš„é‡è¦æŠ€å·§ã€‚

### å±€é™æ€§ä¸åæ€

å°½ç®¡PaLMè¡¨ç°å‡ºè‰²ï¼Œä½†Googleåœ¨è®ºæ–‡ä¸­å¦è¯šåœ°è®¨è®ºäº†å±€é™æ€§ï¼š

**1. äº‹å®æ€§é—®é¢˜**ï¼š
- ä»ä¼šç”Ÿæˆçœ‹ä¼¼å¯ä¿¡ä½†å®Œå…¨é”™è¯¯çš„ä¿¡æ¯
- æ— æ³•å¯é åœ°å¼•ç”¨æ¥æº
- ç¼ºä¹çŸ¥è¯†æ›´æ–°æœºåˆ¶

**2. æœ‰å®³å†…å®¹é£é™©**ï¼š
- å¯èƒ½ç”Ÿæˆæœ‰åè§ã€æ­§è§†æ€§å†…å®¹
- ç»§æ‰¿äº†è®­ç»ƒæ•°æ®ä¸­çš„ç¤¾ä¼šåè§
- éœ€è¦é¢å¤–çš„å®‰å…¨æœºåˆ¶

**3. èƒ½æºå’Œç¯å¢ƒæˆæœ¬**ï¼š
- è®­ç»ƒæ¶ˆè€—å¤§é‡ç”µåŠ›
- ç¢³æ’æ”¾é—®é¢˜éœ€è¦å…³æ³¨
- Googleæ‰¿è¯ºä½¿ç”¨å¯å†ç”Ÿèƒ½æºï¼Œä½†ä»éœ€æ”¹è¿›

**4. å¯è§£é‡Šæ€§å·®**ï¼š
- æ— æ³•è§£é‡Šä¸ºä»€ä¹ˆç”Ÿæˆç‰¹å®šè¾“å‡º
- é»‘ç›’æ€§è´¨ä½¿è°ƒè¯•å›°éš¾
- å®‰å…¨æ€§å’Œå¯ä¿¡åº¦å—é™

**Googleçš„æ€åº¦**ï¼š

PaLMè®ºæ–‡ä¸“é—¨æœ‰ä¸€ç« è®¨è®º"Responsible AI Considerations"ï¼ˆè´Ÿè´£ä»»çš„AIè€ƒé‡ï¼‰ï¼Œä½“ç°äº†Googleçš„è°¨æ…æ€åº¦ã€‚Googleæ²¡æœ‰åƒOpenAIé‚£æ ·å¿«é€Ÿæ¨å‡ºAPIï¼Œè€Œæ˜¯ä¼˜å…ˆè¿›è¡Œå®‰å…¨æ€§ç ”ç©¶å’Œä¼¦ç†è¯„ä¼°ã€‚

è¿™ç§è°¨æ…åœ¨åæ¥è¢«è®¤ä¸ºæ˜¯Googleåœ¨AIç«èµ›ä¸­è½åçš„åŸå› ä¹‹ä¸€â€”â€”å½“OpenAIæ¨å‡ºChatGPTå¹¶å¼•çˆ†å¸‚åœºæ—¶ï¼ŒGoogleçš„Bardè¿˜åœ¨å†…éƒ¨æµ‹è¯•é˜¶æ®µã€‚

## Googleçš„æˆ˜ç•¥å›°å¢ƒ

### å¼€æ”¾ vs å•†ä¸šåŒ–çš„çŸ›ç›¾

T5å’ŒPaLMéƒ½ä½“ç°äº†Googleçš„å¼€æ”¾ç²¾ç¥ï¼š
- T5å®Œå…¨å¼€æºï¼ˆæ¨¡å‹ã€ä»£ç ã€æ•°æ®ï¼‰
- PaLMè™½æœªå‘å¸ƒæ¨¡å‹æƒé‡ï¼Œä½†è®ºæ–‡è¯¦ç»†å…¬å¼€äº†æŠ€æœ¯ç»†èŠ‚

ä½†è¿™ç§å¼€æ”¾æ€§åœ¨å•†ä¸šä¸Šå¹¶ä¸ä¸€å®šæœ‰åˆ©ã€‚OpenAIé€šè¿‡GPT-3 APIå»ºç«‹äº†å•†ä¸šç”Ÿæ€å’Œå…ˆå‘ä¼˜åŠ¿ï¼Œè€ŒGoogleçš„å¼€æ”¾è®©ç«äº‰å¯¹æ‰‹ä¹Ÿèƒ½å—ç›Šã€‚

åˆ°äº†2022å¹´ï¼ŒGoogleå¼€å§‹æ„è¯†åˆ°è¿™ä¸ªé—®é¢˜ã€‚Bardå’Œåæ¥çš„Geminié‡‡å–äº†æ›´å°é—­çš„ç­–ç•¥ï¼Œä½†è¿™åˆä¸Googleçš„å­¦æœ¯æ–‡åŒ–å†²çªã€‚

### æŠ€æœ¯é¢†å…ˆ vs äº§å“è½å

ä¸€ä¸ªè®½åˆºçš„ç°è±¡ï¼šGoogleåœ¨è®¸å¤šæŠ€æœ¯ä¸Šé¢†å…ˆï¼ˆTransformerã€BERTã€T5ã€Pathwaysï¼‰ï¼Œä½†åœ¨äº§å“åŒ–å’Œå¸‚åœºè®¤çŸ¥ä¸Šè½åã€‚

**æŠ€æœ¯é¢†å…ˆ**ï¼š
- 2017ï¼šTransformerï¼ˆå¼€åˆ›æ—¶ä»£ï¼‰
- 2018ï¼šBERTï¼ˆç†è§£ä»»åŠ¡éœ¸ä¸»ï¼‰
- 2019ï¼šT5ï¼ˆç»Ÿä¸€æ¡†æ¶ï¼‰
- 2021ï¼šFlan-T5ï¼ˆæŒ‡ä»¤è°ƒä¼˜å…ˆé©±ï¼‰
- 2022ï¼šPaLMï¼ˆ540Bå‚æ•°ï¼‰

**äº§å“è½å**ï¼š
- OpenAIå…ˆæ¨å‡ºGPT-3 APIï¼ˆ2020ï¼‰
- OpenAIå…ˆæ¨å‡ºChatGPTï¼ˆ2022-11ï¼‰
- Googleçš„Bardç›´åˆ°2023-03æ‰åŒ†å¿™å‘å¸ƒï¼Œä¸”åˆæœŸè¡¨ç°ä¸ä½³

**åŸå› åˆ†æ**ï¼š

1. **ç»„ç»‡æ–‡åŒ–å·®å¼‚**ï¼š
   - Googleæ˜¯ç ”ç©¶æ–‡åŒ–ï¼Œè¿½æ±‚ç§‘å­¦ä¸¥è°¨å’Œè®ºæ–‡å‘è¡¨
   - OpenAIæ˜¯äº§å“æ–‡åŒ–ï¼Œè¿½æ±‚ç”¨æˆ·ä½“éªŒå’Œå¸‚åœºå½±å“

2. **é£é™©æ‰¿å—åº¦ä¸åŒ**ï¼š
   - Googleæ‹…å¿ƒå£°èª‰é£é™©ï¼Œå¯¹äº§å“è´¨é‡è¦æ±‚æé«˜
   - OpenAIæ„¿æ„å¿«é€Ÿè¿­ä»£ï¼Œåœ¨ä½¿ç”¨ä¸­æ”¹è¿›

3. **æ¿€åŠ±æœºåˆ¶ä¸åŒ**ï¼š
   - Googleç ”ç©¶å‘˜ä»¥è®ºæ–‡å‘è¡¨å’Œå­¦æœ¯å½±å“åŠ›è¯„ä¼°
   - OpenAIä»¥äº§å“æˆåŠŸå’Œç”¨æˆ·å¢é•¿è¯„ä¼°

4. **å†³ç­–æµç¨‹å·®å¼‚**ï¼š
   - Googleå±‚çº§å¤æ‚ï¼Œå†³ç­–éœ€è¦å¤šæ–¹åè°ƒ
   - OpenAIç»“æ„æ‰å¹³ï¼Œå†³ç­–å¿«é€Ÿ

è¿™äº›å·®å¼‚åœ¨ChatGPTçˆ†å‘åå˜å¾—å°¤ä¸ºæ˜æ˜¾ï¼Œå¼•å‘äº†Googleå†…éƒ¨çš„æ·±åˆ»åæ€ã€‚

## å°ç»“ (Summary)

2019-2022å¹´ï¼ŒGoogleå±•ç°äº†ä¸€æ¡ä¸OpenAIæˆªç„¶ä¸åŒçš„å‘å±•è·¯å¾„ã€‚ä»T5çš„ç³»ç»Ÿæ€§æ¢ç´¢ï¼Œåˆ°Flan-T5çš„æŒ‡ä»¤è°ƒä¼˜ï¼Œå†åˆ°PaLMçš„è§„æ¨¡åŒ–çªç ´ï¼ŒGoogleå§‹ç»ˆåšæŒç§‘å­¦ä¸¥è°¨å’Œå¼€æ”¾ç²¾ç¥ã€‚

T5çš„text-to-textæ¡†æ¶å’ŒC4æ•°æ®é›†æˆä¸ºåç»­ç ”ç©¶çš„é‡è¦åŸºç¡€è®¾æ–½ã€‚Flan-T5çš„æŒ‡ä»¤è°ƒä¼˜æ€æƒ³ä¸ºChatGPTé“ºå¹³äº†é“è·¯ï¼ˆè™½ç„¶Googleæœªèƒ½ç‡å…ˆäº§å“åŒ–ï¼‰ã€‚PaLMåœ¨540Bè§„æ¨¡ä¸Šçš„æˆåŠŸéªŒè¯äº†Pathwaysæ¶æ„çš„æ½œåŠ›ï¼Œä¸ºå¤šæ¨¡æ€æ—¶ä»£åšå¥½äº†å‡†å¤‡ã€‚

ä½†Googleä¹Ÿé¢ä¸´ç€æˆ˜ç•¥å›°å¢ƒï¼šæŠ€æœ¯é¢†å…ˆå¦‚ä½•è½¬åŒ–ä¸ºäº§å“ä¼˜åŠ¿ï¼Ÿå¼€æ”¾ç²¾ç¥å¦‚ä½•ä¸å•†ä¸šç«äº‰å¹³è¡¡ï¼Ÿå­¦æœ¯æ–‡åŒ–å¦‚ä½•ä¸å¿«é€Ÿè¿­ä»£å…¼å®¹ï¼Ÿ

è¿™äº›é—®é¢˜åœ¨2022å¹´åº•ChatGPTçˆ†å‘æ—¶å˜å¾—å°¤ä¸ºç´§è¿«ã€‚Googleè¢«è¿«åŠ é€ŸBardçš„å¼€å‘ï¼Œé‡æ–°å®¡è§†è‡ªå·±çš„AIæˆ˜ç•¥ã€‚ä»ç ”ç©¶åˆ°äº§å“ï¼Œä»å¼€æ”¾åˆ°å°é—­ï¼ŒGoogleçš„å¤§è¯­è¨€æ¨¡å‹ä¹‹è·¯å……æ»¡äº†æŒ‘æˆ˜å’ŒæŠ‰æ‹©ã€‚

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°è¿™åœºç«äº‰çš„å…³é”®è½¬æŠ˜ç‚¹ï¼šRLHFæŠ€æœ¯å¦‚ä½•è®©AIæ›´"å¬è¯"ï¼ŒInstructGPTå¦‚ä½•ä¸ºChatGPTé“ºè·¯ï¼Œä»¥åŠChatGPTå¦‚ä½•åœ¨2022å¹´åº•æ¨ªç©ºå‡ºä¸–ï¼Œå°†å¤§è¯­è¨€æ¨¡å‹å¸¦å…¥ä¸»æµè§†é‡ï¼Œå½»åº•æ”¹å˜AIçš„æ ¼å±€ã€‚

Googleçš„ä¸¥è°¨å’ŒOpenAIçš„æ¿€è¿›ï¼Œä»£è¡¨äº†AIå‘å±•çš„ä¸¤ç§å“²å­¦ã€‚å†å²ä¼šè¯æ˜å“ªä¸€ç§æ›´æˆåŠŸï¼Œè¿˜æ˜¯ä¸¤è€…çš„èåˆæ‰æ˜¯æœªæ¥ï¼Ÿè¿™ä¸ªç­”æ¡ˆï¼Œä»åœ¨ä¹¦å†™ä¸­ã€‚

---

**æœ¬ç« è¦ç‚¹** (Key Takeaways):
- Googleé€‰æ‹©äº†ä¸OpenAIä¸åŒçš„é“è·¯ï¼šåšæŒç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œé€šè¿‡ç³»ç»Ÿæ€§ç§‘å­¦æ¢ç´¢ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„æœ¬è´¨è§„å¾‹
- T5çš„text-to-textæ¡†æ¶ç»Ÿä¸€äº†æ‰€æœ‰NLPä»»åŠ¡ï¼ŒC4æ•°æ®é›†éªŒè¯äº†"è´¨é‡>æ•°é‡"çš„å‡è®¾ï¼Œç³»ç»Ÿæ€§å®éªŒä¸ºé¢„è®­ç»ƒæ–¹æ³•è®ºæä¾›äº†ç§‘å­¦åŸºå‡†
- Flan-T5é¦–æ¬¡å±•ç¤ºäº†instruction tuningçš„å¨åŠ›ï¼Œä¸ºåæ¥çš„ChatGPTé“ºå¹³é“è·¯ï¼Œä½†Googleæœªèƒ½ç‡å…ˆäº§å“åŒ–è¿™ä¸€æŠ€æœ¯
- PaLMï¼ˆ540Bå‚æ•°ï¼‰é€šè¿‡Pathwaysæ¶æ„å®ç°è®­ç»ƒæ•ˆç‡çªç ´ï¼ˆ46.2% MFUï¼‰ï¼ŒChain-of-Thoughtå‘ç°å±•ç¤ºäº†å¤§æ¨¡å‹çš„æ¨ç†æ½œåŠ›
- Googleçš„æˆ˜ç•¥å›°å¢ƒï¼šæŠ€æœ¯é¢†å…ˆä½†äº§å“è½åï¼Œå¼€æ”¾ç²¾ç¥ä¸å•†ä¸šç«äº‰çš„çŸ›ç›¾ï¼Œå­¦æœ¯æ–‡åŒ–ä¸å¿«é€Ÿè¿­ä»£çš„å†²çª
- ä¸åŒçš„ä¼ä¸šæ–‡åŒ–å¯¼è‡´ä¸åŒçš„å‘å±•è·¯å¾„ï¼šGoogleçš„ä¸¥è°¨ vs OpenAIçš„æ¿€è¿›ï¼Œä¸¤ç§å“²å­¦å„æœ‰ä¼˜åŠ£

**å‚è€ƒæ–‡çŒ®** (Chapter References):
- Raffel, C., Shazeer, N., Roberts, A., et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *Journal of Machine Learning Research*, 21(140), 1-67. https://arxiv.org/abs/1910.10683
- Wei, J., Bosma, M., Zhao, V. Y., et al. (2021). Finetuned Language Models Are Zero-Shot Learners (Flan). *ICLR 2022*. https://arxiv.org/abs/2109.01652
- Chowdhery, A., Narang, S., Devlin, J., et al. (2022). PaLM: Scaling Language Modeling with Pathways. *arXiv preprint*. https://arxiv.org/abs/2204.02311
- Wei, J., Wang, X., Schuurmans, D., et al. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. *NeurIPS 2022*. https://arxiv.org/abs/2201.11903
- Google AI Blog. (2020). Exploring Transfer Learning with T5. Retrieved from https://ai.googleblog.com
- Google AI Blog. (2022). Pathways Language Model (PaLM): Scaling to 540 Billion Parameters. Retrieved from https://ai.googleblog.com
