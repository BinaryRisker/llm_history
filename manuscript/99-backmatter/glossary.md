# 术语表 (Glossary)

**Last Updated**: 2025-10-17

本术语表包含书中出现的主要技术术语及其解释。所有术语按中文首字母排序，提供中英文对照。

---

## A

### Agent (AI智能体)
具有自主行为能力的AI系统，可以感知环境、制定计划、使用工具并执行复杂任务。代表了AI从"回答问题"到"完成任务"的演进。

**首次出现**: [第10章](../05-global-race-2023/multimodal-agents.md)
**相关概念**: 工具调用、自主性、Computer Use

### AGI (Artificial General Intelligence, 通用人工智能)
具有与人类相当的广泛认知能力的AI系统，能够理解、学习和应用知识到任何智力任务。AI研究的终极目标。

**首次出现**: [第11章](../08-present/agi-race.md)
**相关概念**: ASI、对齐问题、超级智能

### API (Application Programming Interface, 应用程序接口)
允许开发者通过代码调用AI模型服务的接口。OpenAI的API模式开创了AI商业化的新范式。

**首次出现**: [第4章](../02-gpt-era/gpt3-breakthrough.md)
**相关概念**: 云服务、商业化、付费订阅

### Anthropic (安思)
由Dario和Daniela Amodei兄妹于2021年创立的AI安全公司，开发Claude系列模型，倡导Constitutional AI方法。

**首次出现**: [第7章](../05-global-race-2023/openai-anthropic.md)
**相关概念**: Claude、Constitutional AI、AI安全

### 注意力机制 (Attention Mechanism)
一种允许神经网络模型在处理输入序列时，动态关注不同部分的技术。在Transformer架构中，注意力机制成为核心组件。

**首次出现**: [第1章](../01-foundation/transformer-revolution.md)
**相关概念**: 自注意力机制、多头注意力

---

## B

### 百度 (Baidu)
中国最大的搜索引擎公司，在ChatGPT发布后3.5个月推出文心一言，成为中国AI竞赛的首发响应者。

**首次出现**: [第9章](../05-global-race-2023/chinese-ai-development.md)
**相关概念**: 文心一言、ERNIE、百模大战

### 百模大战
2023年ChatGPT引发的中国AI产业竞赛现象，数十家公司在短时间内发布大语言模型，争夺市场份额。

**首次出现**: [第9章](../05-global-race-2023/chinese-ai-development.md)
**相关概念**: 中国AI、文心一言、通义千问

### BERT (Bidirectional Encoder Representations from Transformers)
由Google在2018年发布的双向Transformer编码器模型，通过预训练和微调范式在多项NLP任务上取得突破性表现。

**首次出现**: [第2章](../01-foundation/early-applications.md)
**相关概念**: 预训练、掩码语言模型

---

## C

### ChatGPT
OpenAI在2022年11月推出的对话式AI系统，基于GPT-3.5并使用RLHF技术优化，迅速成为主流现象，两个月用户破亿。

**首次出现**: [第6章](../04-chatgpt-revolution/chatgpt-launch.md)
**相关概念**: RLHF、指令微调、全球现象

### ChatGLM (智谱清言)
智谱AI在2023年3月开源的对话大模型，是中国首个开源对话大模型，引领了中国开源AI浪潮。

**首次出现**: [第9章](../05-global-race-2023/chinese-ai-development.md)
**相关概念**: 智谱AI、开源模型、中文优化

### Claude
Anthropic开发的大语言模型系列，以安全性、诚实性和长文本处理能力著称，是ChatGPT的主要竞争对手之一。

**首次出现**: [第7章](../05-global-race-2023/openai-anthropic.md)
**相关概念**: Anthropic、Constitutional AI、HHH原则

### Computer Use
Anthropic在Claude 3.5中引入的功能，允许AI直接控制计算机（鼠标、键盘、应用程序），实现真正的AI Agent。

**首次出现**: [第10章](../05-global-race-2023/multimodal-agents.md)
**相关概念**: Agent、自主性、工具调用

### Constitutional AI (宪法AI)
Anthropic提出的AI对齐方法，通过预定义的原则（Constitution）让AI自我评估和改进，减少人类标注依赖。

**首次出现**: [第7章](../05-global-race-2023/openai-anthropic.md)
**相关概念**: AI对齐、RLHF、安全性

### CUDA
Nvidia在2006年推出的并行计算平台，让GPU可用于通用计算，意外成为AI时代的关键基础设施。

**首次出现**: [第12章](../08-present/chip-war.md)
**相关概念**: GPU、Nvidia、算力

---

## D

### 大语言模型 (Large Language Model, LLM)
参数量通常在数十亿到数千亿规模的神经网络语言模型，通过在大规模文本语料上训练，展现出强大的语言理解和生成能力。

**首次出现**: [第3章](../02-gpt-era/scaling-up.md)
**相关概念**: 参数量、预训练、涌现能力

### DeepSeek (深度求索)
由幻方量化创始人梁文锋于2023年创立的中国AI公司，通过MoE架构创新和算法优化，在受限芯片环境下实现突破性性能。

**首次出现**: [第10章](../05-global-race-2023/chinese-innovation.md)
**相关概念**: MoE、算法优化、国产芯片

### 豆包 (Doubao)
字节跳动在2024年推出的AI助手产品，采用免费策略，快速成为中国日活最高的AI产品之一。

**首次出现**: [第10章](../05-global-race-2023/chinese-ai-development.md)
**相关概念**: 字节跳动、免费策略、Coze

---

## E

### ERNIE (Enhanced Representation through Knowledge Integration, 文心)
百度开发的预训练语言模型系列，针对中文和多语言场景优化，是中国AI发展的代表性成果。

**首次出现**: [第9章](../05-global-race-2023/chinese-ai-development.md)
**相关概念**: 知识增强、中文NLP、百度

---

## F

### Few-shot Learning (少样本学习)
模型仅需要少量示例（通常0-10个）就能执行新任务的能力，是大语言模型的重要涌现特性之一。

**首次出现**: [第3章](../02-gpt-era/scaling-up.md)
**相关概念**: In-context Learning、零样本学习

---

## G

### Gemini
Google开发的多模态大语言模型系列，作为对抗ChatGPT的主力产品，经历了从Bard到Gemini的品牌演进。

**首次出现**: [第7章](../05-global-race-2023/google-response.md)
**相关概念**: Google、多模态、TPU

### Google (谷歌)
发明Transformer架构的科技巨头，在AI研究领域技术领先但产品化落后，被ChatGPT现象倒逼加速产品迭代。

**首次出现**: [第1章](../01-foundation/transformer-revolution.md)
**相关概念**: Transformer、BERT、Gemini、TPU

### GPU (Graphics Processing Unit, 图形处理器)
原本用于图形渲染的处理器，因其强大的并行计算能力成为AI训练的核心硬件，Nvidia GPU主导了AI算力市场。

**首次出现**: [第12章](../08-present/chip-war.md)
**相关概念**: CUDA、Nvidia、并行计算、算力

### GPT (Generative Pre-trained Transformer)
OpenAI开发的生成式预训练Transformer模型系列，包括GPT-1、GPT-2、GPT-3、GPT-4、GPT-5等版本，推动了大语言模型的发展。

**首次出现**: [第2章](../01-foundation/early-applications.md)
**相关概念**: 生成式模型、预训练、自回归

### Grok
xAI（Elon Musk创立）开发的大语言模型，以"追求真相"和减少政治正确性为特色，Grok-1开源展现314B参数规模。

**首次出现**: [第11章](../08-present/new-players.md)
**相关概念**: xAI、开源模型、Elon Musk

---

## H

### HHH原则 (Helpful, Honest, Harmless)
Anthropic提出的AI对齐三原则：有用性、诚实性、无害性，是Claude模型训练和评估的核心标准。

**首次出现**: [第7章](../05-global-race-2023/openai-anthropic.md)
**相关概念**: Claude、Constitutional AI、AI对齐

### 幻觉 (Hallucination)
大语言模型生成看似合理但实际错误或虚构信息的现象，是当前LLM的主要技术挑战之一。

**首次出现**: [第4章](../02-gpt-era/gpt3-breakthrough.md)
**相关概念**: 可靠性、事实准确性、模型局限

### 华为 (Huawei)
中国通信设备巨头，在美国芯片禁令下自主研发昇腾910C芯片，成为芯片战中国突围的代表。

**首次出现**: [第12章](../08-present/chip-war.md)
**相关概念**: 昇腾910C、芯片战、国产芯片、盘古模型

### 混元 (Hunyuan)
腾讯开发的大语言模型系列，深度整合到微信、QQ等12亿用户的社交生态中。

**首次出现**: [第9章](../05-global-race-2023/chinese-ai-development.md)
**相关概念**: 腾讯、社交生态、应用整合

---

## I

### In-context Learning (上下文学习)
大语言模型通过在提示词中提供示例来学习新任务的能力，无需修改模型参数，是Few-shot Learning的基础。

**首次出现**: [第3章](../02-gpt-era/scaling-up.md)
**相关概念**: Few-shot Learning、零样本学习、提示工程

---

## J

---

## K

### 开源模型 (Open Source Model)
公开模型权重和代码的大语言模型，以Meta的LLaMA系列为代表，与OpenAI的闭源策略形成对比，引发开源vs闭源之争。

**首次出现**: [第8章](../05-global-race-2023/meta-llama.md)
**相关概念**: LLaMA、闭源模型、开源运动

---

## L

### LLaMA (Large Language Model Meta AI)
Meta在2023年发布的开源大语言模型系列，以较小的参数量实现了优异性能，推动了开源LLM生态发展。Llama 4系列（2025）包括Scout、Maverick、Behemoth三个版本。

**首次出现**: [第8章](../05-global-race-2023/meta-llama.md)
**相关概念**: 开源模型、模型效率、MoE架构

---

## M

### Meta
Facebook母公司，AI开源运动的引领者，通过LLaMA系列模型开源策略对抗OpenAI闭源垄断，Llama 4 Maverick击败GPT-4o。

**首次出现**: [第8章](../05-global-race-2023/meta-llama.md)
**相关概念**: LLaMA、开源战略、Llama 4

### Microsoft (微软)
通过100亿美元投资OpenAI获得AI时代入场券的科技巨头，将GPT能力整合到全产品线Copilot生态。

**首次出现**: [第6章](../04-chatgpt-revolution/industry-response.md)
**相关概念**: OpenAI、Copilot、Azure、GPT-5集成

### MoE (Mixture of Experts, 混合专家模型)
一种模型架构，通过多个专家子网络和路由机制提高模型效率，DeepSeek和Llama 4采用此架构实现性能突破。

**首次出现**: [第10章](../05-global-race-2023/chinese-innovation.md)
**相关概念**: DeepSeek、模型架构、参数效率

### 多模态模型 (Multimodal Model)
能够处理和生成多种类型数据（如文本、图像、音频、视频）的AI模型，代表了大语言模型发展的重要方向。

**首次出现**: [第7章](../05-global-race-2023/openai-anthropic.md)
**相关概念**: GPT-4、视觉-语言模型、Gemini

### 掩码语言模型 (Masked Language Model)
BERT采用的预训练方法，通过随机遮盖输入中的词汇让模型预测，学习双向上下文表示。

**首次出现**: [第2章](../01-foundation/early-applications.md)
**相关概念**: BERT、预训练、双向编码

---

## N

### Nvidia (英伟达)
AI算力时代的霸主，GPU芯片占据AI训练市场80%+份额，Blackwell系列架构（2025）实现25倍效率提升。

**首次出现**: [第12章](../08-present/chip-war.md)
**相关概念**: GPU、CUDA、H100、芯片战、Blackwell

### NLP (Natural Language Processing, 自然语言处理)
使计算机能够理解、解释和生成人类语言的人工智能分支，大语言模型的出现使NLP能力实现质的飞跃。

**首次出现**: [第1章](../01-foundation/transformer-revolution.md)
**相关概念**: Transformer、语言模型、文本理解

---

## O

### OpenAI
由Sam Altman领导的AI研究公司，开发ChatGPT和GPT系列模型，采用闭源商业化策略，2025年8月发布GPT-5。

**首次出现**: [第2章](../01-foundation/early-applications.md)
**相关概念**: GPT、ChatGPT、闭源策略、AGI

---

## P

### 盘古模型 (Pangu)
华为开发的大语言模型系列，专注于工业场景应用，覆盖煤矿、气象、药物研发等垂直领域。

**首次出现**: [第9章](../05-global-race-2023/chinese-ai-development.md)
**相关概念**: 华为、行业模型、垂直应用

### 参数量 (Parameters)
神经网络模型中可学习权重的数量，通常以B（十亿）为单位，是衡量模型规模的关键指标。GPT-3为175B，GPT-4估计1.76T。

**首次出现**: [第3章](../02-gpt-era/scaling-up.md)
**相关概念**: 模型规模、缩放定律、计算量

### 提示工程 (Prompt Engineering)
设计和优化输入提示词以引导大语言模型产生期望输出的技术，是使用LLM的关键技能。

**首次出现**: [第4章](../02-gpt-era/gpt3-breakthrough.md)
**相关概念**: In-context Learning、Few-shot Learning

### 预训练 (Pre-training)
在大规模无标注数据上训练模型的阶段，使模型学习语言的通用表示，为后续微调奠定基础。

**首次出现**: [第2章](../01-foundation/early-applications.md)
**相关概念**: 微调、迁移学习

---

## Q

### Qwen (通义千问)
阿里巴巴达摩院开发的大语言模型系列，面向中文和多语言场景，是中国AI发展的代表性成果之一。在HuggingFace排名全球前三。

**首次出现**: [第9章](../05-global-race-2023/chinese-ai-development.md)
**相关概念**: 阿里巴巴、多模态、开源模型

---

## R

### RLHF (Reinforcement Learning from Human Feedback)
通过人类反馈进行强化学习的训练方法，是使大语言模型更好地遵循人类意图和价值观的关键技术。ChatGPT的成功很大程度归功于RLHF。

**首次出现**: [第5章](../04-chatgpt-revolution/rlhf-innovation.md)
**相关概念**: 人类对齐、指令微调、ChatGPT

---

## S

### 算法优化
在硬件资源受限情况下，通过改进算法提升模型性能的技术路线，DeepSeek以1/10成本实现GPT-4级性能，证明算法可以弥补硬件差距。

**首次出现**: [第10章](../05-global-race-2023/chinese-innovation.md)
**相关概念**: DeepSeek、MoE、芯片战

### 昇腾910C (Ascend 910C)
华为自主研发的AI训练芯片，7nm工艺，性能接近Nvidia A100，是中国在芯片战中的重要突破。

**首次出现**: [第12章](../08-present/chip-war.md)
**相关概念**: 华为、国产芯片、芯片战

### 缩放定律 (Scaling Laws)
描述模型性能如何随着模型规模、数据量和计算量增长而提升的数学规律，是大语言模型发展的理论基础。

**首次出现**: [第3章](../02-gpt-era/scaling-up.md)
**相关概念**: 参数量、计算资源、涌现能力

### 自注意力机制 (Self-Attention)
Transformer架构的核心机制，允许序列中的每个元素同时关注序列中所有其他元素，实现了高效的并行计算。

**首次出现**: [第1章](../01-foundation/transformer-revolution.md)
**相关概念**: 注意力机制、Transformer

---

## T

### 腾讯 (Tencent)
中国社交生态巨头，拥有微信、QQ等12亿用户，通过混元模型深度整合AI能力到社交产品。

**首次出现**: [第9章](../05-global-race-2023/chinese-ai-development.md)
**相关概念**: 混元、社交生态、微信

### Token
语言模型处理文本的基本单位，通常一个token约等于0.75个英文单词或0.5个中文字符，是计算模型成本和上下文长度的基准。

**首次出现**: [第3章](../02-gpt-era/scaling-up.md)
**相关概念**: 上下文窗口、参数量

### TPU (Tensor Processing Unit, 张量处理单元)
Google自研的AI专用芯片，专为Transformer等深度学习模型优化，与Nvidia GPU形成竞争关系。

**首次出现**: [第7章](../05-global-race-2023/google-response.md)
**相关概念**: GPU、Google、AI芯片

### Transformer
Google在2017年提出的神经网络架构，完全基于注意力机制，摒弃了循环和卷积结构，成为现代大语言模型的基础。

**首次出现**: [第1章](../01-foundation/transformer-revolution.md)
**相关概念**: 注意力机制、编码器-解码器

---

## U

---

## V

---

## W

### 微调 (Fine-tuning)
在预训练模型基础上，使用特定任务的标注数据进行训练，使模型适应特定应用场景的技术。

**首次出现**: [第2章](../01-foundation/early-applications.md)
**相关概念**: 预训练、迁移学习、指令微调

### 文心一言 (ERNIE Bot)
百度基于ERNIE模型开发的对话式AI产品，2023年3月发布，是中国首个对标ChatGPT的大语言模型对话产品。

**首次出现**: [第9章](../05-global-race-2023/chinese-ai-development.md)
**相关概念**: 百度、ERNIE、百模大战

---

## X

### xAI
Elon Musk于2023年创立的AI公司，开发Grok模型，以"追求真相"和减少政治正确性为特色，建有全球最大AI训练集群（10万+H100）。

**首次出现**: [第11章](../08-present/new-players.md)
**相关概念**: Grok、Elon Musk、开源模型

---

## Y

### 涌现能力 (Emergent Abilities)
大语言模型在达到一定规模后突然展现出的、在较小模型中不存在的能力，如少样本学习、推理、算术等。

**首次出现**: [第3章](../02-gpt-era/scaling-up.md)
**相关概念**: 缩放定律、规模效应

---

## Z

### 字节跳动 (ByteDance)
开发抖音、TikTok的中国互联网公司，通过豆包AI产品和Coze平台在AI赛道快速崛起，采用免费策略获得最高日活。

**首次出现**: [第9章](../05-global-race-2023/chinese-ai-development.md)
**相关概念**: 豆包、Coze、算法基因

### 智谱AI (Zhipu AI)
由清华大学团队创立的AI公司，2023年3月开源ChatGLM成为中国开源AI先锋，以知识图谱增强为特色。

**首次出现**: [第9章](../05-global-race-2023/chinese-ai-development.md)
**相关概念**: ChatGLM、开源先锋、知识图谱

### 指令微调 (Instruction Tuning)
通过在格式化为指令-响应对的数据上进行微调，使模型更好地理解和遵循人类指令的训练方法。

**首次出现**: [第5章](../04-chatgpt-revolution/rlhf-innovation.md)
**相关概念**: RLHF、监督微调

### 芯片战 (Chip War)
2022年10月起美国对中国实施AI芯片出口管制，禁止Nvidia H100/A100出口，倒逼中国发展国产芯片和算法优化的地缘政治冲突。

**首次出现**: [第12章](../08-present/chip-war.md)
**相关概念**: H100、昇腾910C、算法优化、地缘政治

---

## 使用说明 (Usage Notes)

1. **查找术语**: 术语按中文拼音首字母排序
2. **英文对照**: 每个术语都提供英文原名
3. **首次出现**: 标注术语在书中首次详细解释的章节
4. **相关概念**: 列出相关的其他术语，便于交叉参考
5. **持续更新**: 本术语表随书的编写持续更新

---

**维护说明**:
- 新增术语时，请按拼音字母顺序插入
- 确保首次出现章节的链接正确
- 相关概念应双向关联
- 解释应简洁清晰，2-3句话为宜
