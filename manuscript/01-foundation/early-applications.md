---
chapter_number: 2
title: "预训练范式的诞生：GPT-1与BERT"
title_en: "The Birth of Pre-training Paradigm: GPT-1 and BERT"
period: "2018"
status: draft
word_count: 10500
key_events:
  - gpt1-release-2018
  - bert-release-2018
key_organizations:
  - openai
  - google
technical_concepts:
  - generative-pretraining
  - bidirectional-encoding
  - masked-language-model
  - unsupervised-pretraining
  - fine-tuning
anecdote_count: 2
created_date: 2025-10-17
last_updated: 2025-10-17
---

# Chapter 2: 预训练范式的诞生：GPT-1与BERT

## 引言 (Introduction)

2018年，Transformer论文发表刚满一年。这个新架构在机器翻译任务上的成功已经引起了研究社区的关注，但大多数研究者还在观望：**Transformer能否推广到更广泛的自然语言处理任务？**

这一年，两个独立的团队给出了响亮的答案。6月，OpenAI发布了GPT（Generative Pre-trained Transformer），展示了生成式预训练的威力。10月，Google发布了BERT（Bidirectional Encoder Representations from Transformers），证明了双向预训练的有效性。

这两个模型看似采用了完全不同的路线——一个生成，一个理解；一个单向，一个双向——但它们共同确立了一个革命性的范式：**先在海量无标注数据上预训练通用语言表示，再在特定任务上微调**。这个范式不仅解决了NLP领域长期面临的数据稀缺问题，更为后续的大语言模型爆发奠定了基础。

本章中，我们将深入探讨GPT-1和BERT的诞生过程、技术创新和深远影响。这两个模型的发布，标志着AI历史上的一个关键转折点——从任务特定模型到通用语言理解的飞跃。

## OpenAI的大胆尝试：GPT-1

### 新兴AI实验室的野心

OpenAI在2015年12月成立时，就立下了宏大的目标：**"以最有利于全人类的方式推进数字智能"**。创始团队包括Sam Altman（现任CEO）、Elon Musk、Greg Brockman、Ilya Sutskever等人，初始承诺投资10亿美元。

成立之初的OpenAI，秉持完全开放的研究理念——所有研究成果公开发布，代码开源。这与当时的科技巨头形成鲜明对比：Google、Facebook等公司虽然也发表论文，但核心模型和数据往往保密。OpenAI的开放策略吸引了大批优秀研究者加入。

然而到了2017-2018年，OpenAI面临一个现实问题：**资金有限，算力不足**。相比Google、Facebook等拥有海量数据和计算资源的巨头，OpenAI需要找到更高效的研究路线。在这个背景下，Ilya Sutskever领导的团队开始思考：能否用一个通用的预训练模型，来解决多种NLP任务？

### 生成式预训练的核心思想

2018年6月，OpenAI发布了论文"Improving Language Understanding by Generative Pre-Training"（通过生成式预训练改进语言理解）。第一作者Alec Radford和他的合作者们提出了一个简洁而强大的想法：

**两阶段训练流程**：

1. **无监督预训练**（Unsupervised Pre-training）：
   - 在大规模无标注文本上训练语言模型
   - 任务：根据前文预测下一个词（next token prediction）
   - 数据：BooksCorpus数据集，包含7,000本书，约5GB文本
   - 目标：学习通用的语言表示

2. **有监督微调**（Supervised Fine-tuning）：
   - 在特定任务的标注数据上继续训练
   - 只需少量任务特定数据（几千到几万样本）
   - 快速适应新任务

这个想法的妙处在于**解耦了通用知识和任务特定知识的学习**。在预训练阶段，模型从海量文本中学习语法、语义、常识等通用语言知识。这些知识是跨任务共享的——无论你做分类、问答还是推理，都需要理解语言的基本结构。

微调阶段则只需要教会模型如何将这些通用知识应用到特定任务上。因为模型已经"懂"语言了，所以不需要太多任务特定数据就能取得好效果。

### 单向Transformer的选择

GPT-1采用了**单向Transformer解码器**（Unidirectional Transformer Decoder）架构。什么是"单向"？

在传统的语言模型中，预测下一个词时只能看到前面的词，不能"偷看"后面的内容。这就像你在写作时，只能根据已经写好的部分来决定下一个词，不能预知后面要写什么。GPT-1保持了这个特性，使用"因果掩码"（causal masking）确保模型在位置i只能关注位置1到i-1的词。

为什么选择单向？**因为生成任务本质上是单向的**。当你要生成文本时，必须从左到右逐词生成，每个新词只能基于前面已生成的内容。单向架构天然适合这种场景。

GPT-1的模型规模在今天看来很小：
- **参数量**：117M（1.17亿）
- **层数**：12层Transformer decoder
- **隐藏层维度**：768
- **注意力头数**：12个

但在2018年，这已经是相当大的语言模型了。训练这个模型需要相当可观的计算资源，这也是为什么只有资源充足的组织能够进行这样的研究。

### 令人惊喜的跨任务表现

GPT-1在12个不同的NLP任务上进行了评估，涵盖了自然语言理解的各个方面：
- **文本分类**：情感分析、主题分类
- **自然语言推理**：判断两个句子是否有逻辑关系
- **问答系统**：根据文章回答问题
- **语义相似度**：判断两个句子的意思是否相近
- **常识推理**：基于常识知识回答问题

**结果令人振奋**：在12个任务中，GPT-1在9个任务上达到了当时的最佳或接近最佳的表现。考虑到这是一个通用模型，没有针对任何特定任务进行深度定制，这个结果证明了预训练-微调范式的有效性。

更重要的是，**GPT-1展示了语言模型的迁移学习能力**。在一个任务上学到的知识可以帮助其他任务。例如，在阅读理解任务上的改进，也能提升问答系统的表现。这暗示着模型学到的是真正的语言理解能力，而不仅仅是记忆特定模式。

### 一个开始，而非终点

GPT-1的发布并没有引起像ChatGPT那样的轰动。学术界和工业界的反应相对平淡：
- "这个方法有用，但并不revolutionary（革命性）"
- "预训练的想法不算新，之前Word2Vec、ELMo也做过类似的事情"
- "模型还是太小，处理复杂任务还有局限"

OpenAI团队清楚地知道这只是开始。论文的结论部分谦虚地写道："我们的结果表明，生成式预训练是一个有前景的方向...未来的工作应该探索更大规模的数据和模型。"

这句话预示了GPT系列后续的演进路线：**规模化**。从GPT-1的117M参数，到GPT-2的1.5B，再到GPT-3的175B，最后到GPT-4的万亿级参数——规模不断增长，能力持续提升。但这一切的起点，就是2018年6月发布的GPT-1。

## Google的强力回应：BERT

### 搜索巨头的NLP野心

如果说OpenAI是AI领域的新兴力量，那么Google就是老牌霸主。作为Transformer架构的发明者，Google自然不会袖手旁观，看着新玩家抢走风头。

Google AI Language团队在2018年初启动了一个雄心勃勃的项目，代号"Bidirectional Encoder Representations from Transformers"，简称**BERT**。项目负责人Jacob Devlin和他的团队提出了一个大胆的想法：**能否让模型同时看到前后文，真正"理解"每个词在上下文中的含义？**

这个想法源于对NLP任务的深刻洞察。在很多理解型任务中——比如阅读理解、问答、情感分析——理解一个词的含义需要同时考虑它的前后文。举个例子：

"The **bank** was steep."（河岸很陡。）
"The **bank** closed at 5 PM."（银行5点关门。）

"bank"这个词在两个句子中意思完全不同。要正确理解它，你需要同时看到前面和后面的词。单向模型（如GPT-1）在处理这类歧义时会有劣势，因为它只能从左到右处理。

### 掩码语言模型：BERT的核心创新

BERT的核心创新是**掩码语言模型**（Masked Language Model, MLM）。这是一个简单但巧妙的训练任务：

1. 随机选择输入句子中15%的词
2. 将其中80%替换为特殊符号[MASK]，10%替换为随机词，10%保持不变
3. 让模型预测被遮盖的词是什么

例如：
- 输入："我 [MASK] 吃 苹果"
- 目标：预测[MASK]位置的词是"喜欢"

这个任务强制模型利用双向上下文。要预测中间被遮盖的词，模型必须同时关注前面的"我"和后面的"吃 苹果"。这种训练方式让BERT能够学习到更丰富的上下文表示。

除了MLM，BERT还使用了**下一句预测**（Next Sentence Prediction, NSP）任务：给定两个句子A和B，预测B是否是A的下一句。这帮助模型学习句子间的关系，对于问答、自然语言推理等任务很有帮助。

### 双向编码器的威力

BERT采用了**双向Transformer编码器**（Bidirectional Transformer Encoder）架构——这是与GPT-1的关键区别。

在编码器中，每个位置都可以关注所有其他位置，包括前面和后面的词。自注意力机制让信息可以自由流动，不受方向限制。这种双向性让BERT特别擅长"理解型"任务：
- **文本分类**：判断文章主题、情感
- **命名实体识别**：识别人名、地名、组织名
- **问答系统**：从文章中提取答案
- **自然语言推理**：判断逻辑关系

BERT发布了两个版本：
- **BERT-Base**：110M参数，12层，768维隐藏层
- **BERT-Large**：340M参数，24层，1024维隐藏层

BERT-Large的参数量是GPT-1的近3倍，在2018年是前所未有的大规模语言模型。

### 横扫NLP基准测试

2018年10月，BERT论文在arXiv上发布。结果震惊了整个NLP社区：**BERT在11项NLP任务上全部达到了新的最佳（state-of-the-art）表现**。

一些标志性的突破：
- **GLUE基准**（通用语言理解评估）：BERT-Large达到80.5%，比之前最佳结果高7个百分点
- **SQuAD 1.1**（问答数据集）：F1分数93.2%，首次超越人类表现（91.2%）
- **SQuAD 2.0**：在更难的版本（包含无答案问题）上也达到83.1 F1，接近人类的86.8
- **SWAG**（常识推理）：准确率86.3%，大幅领先之前的75.0%

这些数字不仅仅是benchmark上的进步，它们代表了AI在语言理解能力上的质的飞跃。特别是在SQuAD上超越人类表现，标志着机器阅读理解达到了新的里程碑。

### 开源策略的胜利

Google做了一个明智的决定：**完全开源BERT**。不仅公开了论文和代码，还发布了预训练好的模型权重，供全球研究者免费使用。

这个决定产生了巨大影响：
- 在BERT发布后的几个月内，数百篇基于BERT的论文涌现
- 各大公司和研究机构迅速将BERT集成到自己的NLP系统中
- BERT成为NLP领域的"预训练基础设施"，就像计算机视觉领域的ImageNet预训练模型

开源BERT不仅推动了学术研究，也加速了工业应用。从搜索引擎到智能客服，从内容推荐到自动翻译——无数产品因为BERT而得到改进。Google自己也将BERT应用到搜索引擎中，改善了数十亿次查询的结果质量。

### 💡 轶事：BERT差点叫"HERO"

根据Jacob Devlin在后来的访谈中透露，BERT这个名字其实是在项目后期才敲定的。团队最初考虑过很多其他名字，包括"HERO"（Hierarchical Encoder Representations from Transformers）。

但有人指出，"HERO"听起来太过自大，不符合Google的学术风格。而且，如果模型表现不如预期，叫"HERO"就很尴尬了。最终团队选择了更朴实的"BERT"——它没有什么特别的含义，只是首字母缩写。

没想到的是，"BERT"这个名字反而因为简洁好记而深入人心。后来无数模型都使用类似的命名方式：RoBERTa, ALBERT, ELECTRA, DeBERTa...形成了一个"BERT家族"。有趣的是，BERT这个词在英语中听起来像一个人名（类似"Bert and Ernie"，《芝麻街》中的角色），这让这个技术名词显得更加亲切。

---

## GPT vs BERT：两条通往智能的道路

### 架构对比

虽然GPT-1和BERT都基于Transformer，但它们的选择截然不同：

| 维度 | GPT-1 | BERT |
|------|-------|------|
| **架构** | 单向Transformer解码器 | 双向Transformer编码器 |
| **预训练任务** | 下一词预测 | 掩码语言模型 + 下一句预测 |
| **注意力方向** | 因果掩码（只看前文） | 完全双向（看前后文） |
| **擅长任务** | 文本生成 | 文本理解 |
| **参数量** | 117M | 110M (Base) / 340M (Large) |
| **训练数据** | BooksCorpus (5GB) | BooksCorpus + Wikipedia (16GB) |

### 哲学差异

更深层次的是，两个模型体现了不同的AI哲学：

**GPT的生成式哲学**：
- **自回归建模**：模型学习的是P(word_t | word_1, ..., word_{t-1})——根据前文生成下一个词
- **生成能力优先**：天然适合文本生成、对话、创作
- **开放式任务**：没有固定答案，可以自由发挥创造力

**BERT的理解式哲学**：
- **双向上下文**：模型学习的是P(word_t | context)——根据完整上下文理解某个词
- **理解能力优先**：天然适合分类、标注、抽取等任务
- **封闭式任务**：有明确的正确答案，需要精确理解

这两种哲学对应了人类语言能力的两个方面：**表达**（生成）和**理解**（解析）。一个完整的语言智能系统需要同时具备这两种能力。

### 实际应用场景

**GPT-1的优势场景**：
- ✅ **文本续写**：给定开头，生成后续内容
- ✅ **对话生成**：根据上下文生成回复（虽然GPT-1还不够强）
- ✅ **创意写作**：生成故事、诗歌等创意内容
- ❌ **精确理解**：需要准确提取信息的任务表现一般

**BERT的优势场景**：
- ✅ **搜索排序**：判断文档与查询的相关性
- ✅ **问答系统**：从文章中准确提取答案
- ✅ **情感分析**：准确判断文本情感倾向
- ✅ **命名实体识别**：识别文本中的人名、地名等
- ❌ **文本生成**：不擅长生成连贯的长文本

在实际应用中，很多产品需要组合使用两种模型：用BERT理解用户输入，用GPT生成回复。这种"理解+生成"的组合，成为后来很多对话系统的标准架构。

## 预训练-微调范式的确立

### 改变NLP的游戏规则

GPT-1和BERT的成功，彻底改变了NLP研究和应用的范式。在此之前，NLP任务通常遵循这样的流程：

**旧范式**：
1. 收集任务特定的标注数据（通常需要数万到数十万样本）
2. 设计任务特定的模型架构
3. 从头训练模型
4. 在测试集上评估

这个范式有几个严重问题：
- **数据饥渴**：标注数据昂贵，很多任务和语言缺乏足够数据
- **知识孤岛**：每个任务的模型独立训练，无法共享知识
- **资源浪费**：相似任务重复训练，浪费计算资源

**新范式**（预训练-微调）：
1. 在海量无标注文本上预训练通用模型（一次性，大量计算）
2. 在任务特定数据上微调（每个任务，少量计算）
3. 在测试集上评估

这个范式的优势显而易见：
- **数据效率**：微调只需少量标注数据（几千甚至几百样本）
- **知识共享**：预训练模型学到的通用知识可以迁移到所有下游任务
- **快速部署**：新任务只需微调几小时，而不是从头训练几天

### 迁移学习的胜利

预训练-微调本质上是**迁移学习**（Transfer Learning）在NLP中的成功应用。

计算机视觉领域早就证明了迁移学习的有效性。在2012年后，几乎所有视觉任务都使用在ImageNet上预训练的模型作为起点。预训练模型学到的边缘、纹理、物体部件等低层和中层特征，对各种视觉任务都有帮助。

但在NLP领域，迁移学习一直不太成功。Word2Vec和ELMo等词向量方法有一些效果，但改进有限。GPT-1和BERT终于证明，**深度上下文化的预训练表示可以带来巨大的性能提升**。

关键突破在于：
- **深度模型**：12-24层的深度Transformer能捕捉复杂的语言现象
- **大规模数据**：数GB的文本让模型学到丰富的语言知识
- **上下文化表示**：每个词的表示依赖于具体上下文，不是静态的

### 走向更大规模

GPT-1和BERT的成功验证了一个关键假设：**规模很重要**（Scale Matters）。

OpenAI和Google的研究者们都注意到，更大的模型在预训练和微调中都表现更好。这引发了一个自然的问题：如果我们继续增大模型规模，能力会继续提升吗？

答案在接下来的几年中逐渐清晰：**是的，而且提升的幅度超出想象**。这开启了一场"规模化竞赛"：
- 2019年：GPT-2（1.5B参数）
- 2019年：T5（11B参数）
- 2020年：GPT-3（175B参数）
- 2021年：Switch Transformer（1.6T参数）

但规模化不仅仅是堆参数。它需要解决一系列技术挑战：
- **计算效率**：如何高效训练超大模型？
- **内存管理**：如何在有限显存中容纳巨大参数？
- **数据质量**：更大模型需要更高质量的数据
- **训练稳定性**：大模型训练容易出现不稳定
- **推理速度**：如何让巨大模型快速响应？

这些挑战在接下来的几年中逐步被攻克，推动大语言模型不断进化。

## 学术界和工业界的反响

### NLP研究的转折点

BERT的发布在NLP社区引起了轰动。论文发表后的短短几个月内：
- **arXiv被"刷屏"**：数十篇基于BERT的论文每周发布
- **会议投稿激增**：NAACL 2019、ACL 2019等顶会收到大量BERT相关投稿
- **开源项目爆发**：HuggingFace等开源社区提供易用的BERT实现
- **企业快速跟进**：Facebook发布RoBERTa，微软发布DeBERTa，各种BERT变体涌现

学术界开始系统研究BERT的特性：
- **探针任务**（Probing Tasks）：测试BERT学到了哪些语言知识
- **注意力可视化**：理解BERT的注意力模式
- **消融实验**：研究各个组件的贡献
- **改进方向**：如何让BERT更快、更强、更高效

GPT-1的影响相对温和，但OpenAI团队没有停步。他们清楚地知道方向是对的，只是需要更大的规模。2019年2月，GPT-2的发布（1.5B参数）引发了"太危险而不能发布"的争议，将OpenAI推到了聚光灯下。

### 产业应用的井喷

BERT对工业界的影响更加直接和深远。

**Google搜索引擎**：
- 2019年底，Google将BERT应用到搜索排序中
- 影响了10%的英语搜索查询
- 后来扩展到70+种语言
- 显著改善了对长尾查询和会话式查询的理解

**微软**：
- 将BERT集成到Bing搜索
- Office 365中的智能写作助手
- Azure认知服务中的文本分析API

**阿里巴巴、百度、腾讯等中国公司**：
- 快速跟进，训练中文BERT模型
- 应用到电商搜索、推荐、客服等场景
- 推动中文NLP应用的质量提升

**创业公司**：
- 利用预训练模型降低NLP应用门槛
- 聊天机器人、内容审核、智能客服等垂直领域应用蓬勃发展
- 不再需要从头训练模型，大大降低了技术壁垒

### 开源生态的繁荣

HuggingFace的Transformers库在这个时期崛起，成为NLP开源社区的中心：
- 统一的API访问各种预训练模型
- 从几行代码就能使用BERT、GPT
- 模型仓库（Model Hub）让分享和复用模型变得简单

这个开源生态极大地促进了NLP技术的普及和应用。研究者和工程师不再需要深入了解模型细节，就能将最新的NLP技术应用到自己的项目中。

## 小结 (Summary)

2018年，GPT-1和BERT的相继发布，标志着NLP领域进入了**预训练时代**。这两个模型虽然采用了不同的技术路线——生成式 vs 理解式，单向 vs 双向——但共同确立了一个革命性的范式：在海量无标注数据上预训练通用语言表示，然后在特定任务上微调。

GPT-1展示了生成式预训练的潜力，开启了通往ChatGPT的道路。BERT证明了双向预训练在理解任务上的威力，成为无数应用的基础设施。两者互补，共同推动了NLP技术的飞跃。

更重要的是，它们的成功验证了**规模化和迁移学习**的价值。这为接下来的大语言模型爆发奠定了理论和实践基础。从2018年的百万参数级别，到2020年的千亿参数级别，再到今天的万亿参数级别——这条规模化道路，正是从GPT-1和BERT开始的。

在下一章中，我们将看到规模化如何产生质变：GPT-2的"太危险而不能发布"争议，T5的系统性探索，以及GPT-3带来的few-shot learning革命。从2018年的初步验证，到2019-2020年的规模突破——预训练范式正在酝酿一场更大的变革。

---

**本章要点** (Key Takeaways):
- GPT-1和BERT共同确立了"预训练-微调"范式，彻底改变了NLP任务的解决方式
- GPT-1采用单向Transformer解码器，擅长文本生成；BERT采用双向编码器，擅长文本理解，两条路线各有优势
- 掩码语言模型（MLM）是BERT的核心创新，让模型能够利用双向上下文学习更丰富的语言表示
- 预训练模型的开源（特别是BERT）极大降低了NLP应用门槛，催生了技术普及和产业应用井喷
- 规模化的成功验证了"Scale Matters"假设，为后续大语言模型的爆发式增长指明了方向

**参考文献** (Chapter References):
- Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving Language Understanding by Generative Pre-Training. OpenAI Technical Report.
- Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *NAACL 2019*. arXiv:1810.04805
- Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep Contextualized Word Representations (ELMo). *NAACL 2018*.
- OpenAI Blog. (2018). Improving Language Understanding with Unsupervised Learning. Retrieved from https://openai.com/blog
- Google AI Blog. (2018). Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing. Retrieved from https://ai.googleblog.com
