# 大语言模型编年史：从Transformer到ChatGPT的AI革命

[![License](https://img.shields.io/badge/License-All%20Rights%20Reserved-red.svg)](LICENSE)
[![Language](https://img.shields.io/badge/语言-简体中文-blue.svg)]()
[![Last Updated](https://img.shields.io/badge/最后更新-2025年10月-green.svg)]()

> 一部全面记录大语言模型发展历史的编年史，从2017年Transformer论文到2025年的ChatGPT时代，涵盖技术创新、公司竞争和行业变革的完整故事。

---

## 📖 关于本书

### 背景

2022年11月30日，ChatGPT横空出世，两个月内用户突破1亿，成为史上增长最快的消费级应用。这个现象级产品让"大语言模型"（Large Language Model, LLM）从实验室走向大众视野，也标志着人工智能发展进入了新纪元。

但ChatGPT的成功绝非偶然。从2017年Google发表"Attention is All You Need"论文提出Transformer架构，到ChatGPT引发全球AI竞赛，这短短六年间究竟发生了什么？

### 写作目的

本书试图以编年史的方式，完整记录从Transformer诞生到2025年10月的大语言模型发展历程，连接每一个关键节点，解释每一次技术突破的意义，呈现一幅完整的LLM进化图景。

**核心目标：**
- 📚 **系统性记录**：建立完整的LLM发展时间线和事件脉络
- 🔍 **深度解析**：解释技术突破的本质和历史意义
- 🌍 **全球视角**：平衡呈现中西方AI发展的完整图景
- 🎯 **可信度**：基于可验证的事实和引用来源

### 覆盖范围

本书按时间顺序记录了**2017年6月至2025年10月**期间的50多个重要事件和里程碑：

#### 核心内容章节

| 章节 | 时间跨度 | 主要内容 |
|------|---------|---------|
| **第1章：基础奠定** | 2017-2018 | Transformer架构诞生、GPT-1、BERT的早期应用 |
| **第2章：GPT时代** | 2019-2020 | GPT-2"太危险"争议、GPT-3震撼登场、Google的回应 |
| **第3章：对齐革命** | 2021-2022 | RLHF技术、InstructGPT、ChatGPT的诞生 |
| **第4章：ChatGPT革命** | 2022末-2023初 | ChatGPT现象级爆发、全球AI觉醒 |
| **第5章：全球竞赛** | 2023 | GPT-4、Claude、Bard/Gemini、Meta开源策略 |
| **第6章：中国AI发展** | 2023-2024 | 百模大战、文心一言、通义千问、Kimi等国产模型 |
| **第7章：多模态时代** | 2024 | GPT-4o、Sora、Claude 3.5 Sonnet等重大突破 |
| **第8章：当前格局** | 2025 | 最新发展和竞争态势 |

#### 涉及的主要技术

- **架构创新**：Transformer、Self-Attention、MoE、长上下文
- **训练方法**：预训练、微调、RLHF、Constitutional AI
- **应用形态**：文本生成、代码生成、多模态、Agent
- **规模法则**：Scaling Laws、涌现能力、参数效率

#### 涉及的主要公司/机构

**西方：** OpenAI、Google/DeepMind、Meta、Anthropic、Microsoft、Cohere

**中国：** 百度、阿里、腾讯、字节跳动、智谱AI、月之暗面、MiniMax

**学术界：** Stanford、UC Berkeley、清华、北大等

---

## 👥 适合谁读

本书主要面向以下读者：

### ✅ 推荐阅读人群

- **技术从业者**：软件工程师、数据科学家、产品经理
- **AI研究者和学生**：需要系统了解LLM发展脉络的研究人员
- **科技爱好者**：对人工智能发展充满好奇的深度学习者
- **行业观察者**：投资人、创业者、战略分析师

### 📋 阅读前提

- ✅ **不需要**机器学习专业背景（会用通俗语言解释所有核心概念）
- ✅ **不需要**数学推导能力（避免复杂公式，注重理解本质）
- ✅ **需要**基本的技术素养和阅读耐心
- ✅ **需要**对AI发展有真正的好奇心

---

## 🌟 本书特色

### 1. 完整的编年史视角
从2017年6月到2025年10月，按时间顺序记录50+重要事件，每个发展都说明前因后果，构建清晰的技术演进路径。

### 2. 全球化平衡视角
不同于只关注美国公司的叙述，本书平衡呈现西方和中国的AI发展，理解全球竞赛的真实动力。

### 3. 技术与人文并重
除了解释技术原理，也讲述背后的人物故事、争议决策、商业博弈。技术准确，但不枯燥。

### 4. 中文优先，术语对照
以中文为主要语言，技术概念中英文对照（如："自注意力机制 (Self-Attention)"），建立完整术语表。

### 5. 可验证的事实基础
所有重要声明提供引用来源，未证实信息明确标注，存在争议的呈现多方观点。

---

## 📚 如何阅读

### 推荐阅读路径

**线性阅读（推荐）：** 从头到尾顺序阅读，每章承接前章，概念首次出现时解释清楚。

**针对性阅读：**
- **技术创新重点** → 第1章（Transformer）、第3章（RLHF）
- **公司竞争重点** → 第5章（全球竞赛）、第6章（中国AI）
- **最新进展** → 第7章（2024突破）、第8章（2025现状）

### 配合使用的材料

- **术语表**（manuscript/99-backmatter/glossary.md）：查阅不熟悉的技术术语
- **参考文献**（manuscript/99-backmatter/references.md）：深入研究原始资料
- **索引**（manuscript/99-backmatter/index.md）：快速定位特定主题

---

## 📖 获取本书

本书提供以下格式：

- **📄 PDF** - 适合打印和屏幕阅读（`output/llm-history-chronicle.pdf`）
- **📱 EPUB** - 适合电子书阅读器（`output/llm-history-chronicle.epub`）
- **🌐 HTML** - 适合网页浏览（`output/llm-history-chronicle.html`）

### 本地构建

如果你想自己构建本书，需要以下工具：

**必需：**
- [Pandoc](https://pandoc.org/) ≥ 2.19
- XeLaTeX (PDF生成) - 推荐 [TeX Live](https://www.tug.org/texlive/)
- PowerShell（Windows）或 Bash（Linux/macOS）

**可选：**
- [7-Zip](https://www.7-zip.org/)（EPUB后处理，Windows推荐）

**构建命令：**

```powershell
# Windows PowerShell
.\scripts\build-all.ps1

# Linux/macOS Bash
bash scripts/build-all.sh
```

生成的文件将出现在 `output/` 目录。

---

## 🤝 贡献与反馈

### 欢迎您的参与

本书旨在成为LLM发展的可信参考资料。如果您：

- 🐛 **发现事实错误** - 请提供准确信息和可靠来源
- 📝 **有补充信息** - 特别是未被广泛报道的重要事件
- 💡 **提出改进建议** - 叙述结构、技术解释、术语使用等
- ❓ **有疑问讨论** - 对某些观点或解释的不同看法

**请通过以下方式反馈：**

1. 提交 [GitHub Issue](../../issues)
2. 发起 [Pull Request](../../pulls)（特别欢迎补充参考文献、修正错别字）
3. 参与 [Discussions](../../discussions)

### 贡献指南

- **事实准确性优先**：所有补充信息需提供可验证来源
- **保持中立客观**：避免主观臆断和未证实的推测
- **尊重版权**：引用内容需注明出处
- **语言规范**：遵循现有的术语表和写作风格

---

## ⚖️ 版权与许可

**© 2025 AI History Chronicler. All rights reserved.**

本书内容受版权保护。个人学习和研究使用欢迎，但**不允许**商业用途、转载、再分发或改编，除非获得明确授权。

引用本书内容请注明：
> 《大语言模型编年史：从Transformer到ChatGPT的AI革命》，AI History Chronicler，2025年

---

## 📋 局限性说明

### 时效性限制
AI领域瞬息万变。本书内容更新至**2025年10月**，之后的发展未能涵盖。某些预测性内容可能随技术进展而变化。

### 深度与广度的权衡
为在有限篇幅呈现完整编年史，部分话题点到为止。目标是"理解significance"而非"掌握implementation"。

### 信息来源的局限
某些公司（特别是OpenAI、Anthropic）对模型细节高度保密。我们基于公开信息和可靠爆料，但仍有许多内幕无法确认。书中明确区分"已证实"和"未证实"信息。

---

## 📞 联系方式

- **GitHub Repository**: [llm_history](https://github.com/yourusername/llm_history)
- **Issues**: 报告问题和建议
- **Discussions**: 深度讨论和交流

---

## 🙏 致谢

本书基于大量公开文献、学术论文、公司博客、新闻报道以及部分访谈资料编写而成。所有引用来源详见参考文献部分。

特别感谢所有为AI发展做出贡献的研究者、工程师、以及那些愿意分享知识和经验的行业从业者。

---

**最后更新：2025年10月**

现在，让我们回到2017年6月，从那篇改变一切的论文开始这段旅程。

*Happy Reading! 📖✨*
